[{"content":"tl;dr: Go\u0026rsquo;s Context.Value is controversial because of a lack of type-safety. I design a solution for that based on the new generics design draft.\nIf you are following what\u0026rsquo;s happening with Go, you are aware that recently an updated design draft for generics has dropped. What makes this particularly notable is that it comes with an actual prototype implementation of the draft, including a playground. This means for the first time, people get to actually try out how a Go with generics might feel, once they get in. It is a good opportunity to look at common Go code lacking type-safety and evaluate if and how generics can help address them.\nOne area I\u0026rsquo;d like to look at here is Context.Value. It is often criticized for not being explicit enough about the dependencies a function has and some people even go so far as to discourage its use altogether. On the other hand, I\u0026rsquo;m on record saying that it is too useful to ignore. Generics might be a way to bring together these viewpoints.\nWe want to be able to declare dependency on a functionality in context.Context via a function\u0026rsquo;s signature and make it impossible to call it without providing that functionality, while also preserving the ability to pass it through APIs that don\u0026rsquo;t know anything about it. As an example of such functionality, I will use logging. Let\u0026rsquo;s start by creating a fictional little library to do that (the names are not ideal, but let\u0026rsquo;s not worry about that):\npackage logctx  import (  \u0026#34;context\u0026#34;  \u0026#34;log\u0026#34; )  type LogContext interface {  // We embed a context.Context, to say that we are augmenting it with  // additional functionality.  context.Context   // Logf logs the given values in the given format.  Logf(format string, values ...interface{}) }  func WithLog(ctx context.Context, l *log.Logger) LogContext {  return logContext{ctx, l} }  // logContext is unexported, to ensure it can\u0026#39;t be modified. type logContext struct {  context.Context  l *log.Logger }  func (ctx logContext) Logf(format string, values ...interface{}) {  ctx.l.Printf(format, values...) } You might notice that we are not actually using Value() here. This is fundamental to the idea of getting compiler-checks - we need some compiler-known way to \u0026ldquo;tag\u0026rdquo; functionality and that can\u0026rsquo;t be Value. However, we provide the same functionality, by essentially adding an optional interface to context.Context.\nIf we want to use this, we could write\nfunc Foo(ctx logctx.LogContext, v int) {  ctx.Logf(\u0026#34;Foo(%v)\u0026#34;, v) }  func main() {  ctx := logctx.WithLog(context.Background(), log.New(os.Stderr, \u0026#34;\u0026#34;, log.LstdFlags))  Foo(ctx, 42) } However, this has a huge problem: What if we want more than one functionality (each not knowing about the other)? We might try the same trick, say\npackage tracectx  import (  \u0026#34;context\u0026#34;   \u0026#34;github.com/opentracing/opentracing-go\u0026#34; )  type TraceContext interface {  context.Context  Tracer() opentracing.Tracer }  func WithTracer(ctx context.Context, t opentracing.Tracer) TraceContext {  return traceContext{ctx, t} }  type traceContext struct {  context.Context  t opentracing.Tracer }  func (ctx traceContext) Tracer() opentracing.Tracer {  return ctx.t } But because a context.Context is embedded, only those methods explicitly mentioned in that interface are added to traceContext. The Logf method is erased. After all, that is the trouble with optional interfaces.\nThis is where generics come in. We can change our wrapper-types and -functions like this:\ntype LogContext(type parent context.Context) struct {  // the type-parameter is lower case, so the field is not exported.  parent  l *log.Logger }  func WithLog(type Parent context.Context) (ctx Parent, l *log.Logger) LogContext(Parent) {  return LogContext(parent){ctx, l} } By adding a type-parameter and embedding it, we actually get all methods of the parent context on LogContext. We are no longer erasing them. After giving the tracectx package the same treatment, we can use them like this:\n// FooContext encapsulates all the dependencies of Foo in a context.Context. type FooContext interface {  context.Context  Logf(format string, values ...interface{})  Tracer() opentracing.Tracer }  func Foo(ctx FooContext, v int) {  span := ctx.Tracer().StartSpan(\u0026#34;Foo\u0026#34;)  defer span.Finish()   ctx.Logf(\u0026#34;Foo(%v)\u0026#34;, v) }  func main() {  l := log.New(os.Stderr, \u0026#34;\u0026#34;, log.LstdFlags)  t := opentracing.GlobalTracer()  // ctx has type TraceContext(LogContext(context.Context)),  // which embeds a LogContext(context.Context),  // which embeds a context.Context  // So it has all the required methods  ctx := tracectx.WithTracer(logctx.WithLog(context.Background(), l), t)  Foo(ctx, 42) } Foo has now fully declared its dependencies on a logger and a tracectx, without requiring any type-assertions or runtime-checks. The logging- and tracing-libraries don\u0026rsquo;t know about each other and yet are able to wrap each other without loss of type-information. Constructing the context is not particularly ergonomic though. We require a long chained function call, because the values returned by the functions have no longer a unified type context.Context (so the ctx variable can\u0026rsquo;t be re-used).\nAnother thing to note is that we exported LogContext as a struct, instead of an interface. This is necessary, because we can\u0026rsquo;t embed type-parameters into interfaces, but we can embed them as struct-fields. So this is the only way we can express that the returned type has all the methods the parameter type has. The downside is that we are making this a concrete type, which isn\u0026rsquo;t always what we want1.\nWe have now succeeded in annotating context.Context with dependencies, but this alone is not super useful of course. We also need to be able to pass it through agnostic APIs (the fundamental problem Context.Value solves). However, this is easy enough to do.\nFirst, let\u0026rsquo;s change the context API to use the same form of generic wrappers. This isn\u0026rsquo;t backwards compatible, of course, but this entire blog post is a thought experiment, so we are ignoring that. I don\u0026rsquo;t provide the full code here, for brevity\u0026rsquo;s sake, but the basic API would change into this:\npackage context  // CancelContext is the generic version of the currently unexported cancelCtx. type CancelContext(type parent context.Context) struct {  parent  // other fields }  func WithCancel(type Parent context.Context) (ctx Parent) (ctx CancelContext(Parent), cancel CancelFunc) {  // ... } This change is necessary to enable WithCancel to also preserve methods of the parent context. We can now use this in an API that passes through a parametric context. For example, say we want to have an errgroup package, that passes the context through to the argument to (*Group).Go, instead of returning it from WithContext:\n// Derived from the current errgroup code.  // A Group is a collection of goroutines working on subtasks that are part of the same overall task. // // A zero Group is invalid (as opposed to the original errgroup). type Group(type Context context.Context) struct {  ctx Context  cancel func()   wg sync.WaitGroup   errOnce sync.Once  err error }  func WithContext(type C context.Context) (ctx C) *Group(C) {  ctx, cancel := context.WithCancel(ctx)  return \u0026amp;Group(C){ctx: ctx, cancel: cancel} }  func (g *Group(Context)) Wait() error {  g.wg.Wait()  return g.err }  func (g *Group(Context)) Go(f func(Context) error) {  g.wg.Add(1)   go func() {  defer g.wg.Done()   if err := f(g.ctx); err != nil {  g.errOnce.Do(func() {  g.err = err  })  }  }() } Note that the code here has barely changed. It can be used as\nfunc Foo(ctx FooContext) error {  span := ctx.Tracer().StartSpan(\u0026#34;Foo\u0026#34;)  defer span.Finish()  ctx.Logf(\u0026#34;Foo was called\u0026#34;) }  func main() {  var ctx FooContext = newFooContext()  eg := errgroup.WithContext(ctx)  for i := 0; i \u0026lt; 20; i++ {  eg.Go(Foo)  }  if err := eg.Wait(); err != nil {  log.Fatal(err)  } } After playing around with this for a couple of days, I feel pretty confident that these patterns make it possible to get a fully type-safe version of context.Context, while preserving the ability to have APIs that pass it through untouched or augmented.\nA completely different question, of course, is whether all of this is a good idea. Personally, I am on the fence about it. It is definitely valuable, to have a type-safe version of context.Context. And I think it is impressive how small the impact of it is on the users of APIs written this way. The type-argument can almost always be inferred and writing code to make use of this is very natural - you just declare a suitable context-interface and take it as an argument. You can also freely pass it to functions taking a pure context.Context unimpeded.\nOn the other hand, I am not completely convinced the cost is worth it. As soon as you do non-trivial things with a context, it becomes a pretty \u0026ldquo;infectious\u0026rdquo; change. For example, I played around with a mock gRPC API to allow interceptors to take a parametric context and it requires almost all types and functions involved to take a type-parameter. And this doesn\u0026rsquo;t even touch on the fact that gRPC itself might want to add annotations to the context, which adds even more types. I am not sure if the additional machinery is really worth the benefit of some type-safety - especially as it\u0026rsquo;s not always super intuitive and easily understandable. And even more so, if it needs to be combined with other type-parameters, to achieve other goals.\nI think this is an example of what I tend to dislike about generics and powerful type-systems in general. They tempt you to write a lot of extra machinery and types in a way that isn\u0026rsquo;t necessarily semantically meaningful, but only used to encode some invariant in a way the compiler understands.\n I don\u0026rsquo;t actually think there is much of a performance problem with context.Value in practice, but if there is, this could solve that.\n  One upside however, is that this could actually address the other criticism of context.Value: Its performance. If we consequently embed the parent-context as values in struct fields, the final context will be a flat struct. The interface-table of all the extra methods we add will point at the concrete implementations. There\u0026rsquo;s no longer any need for a linear search to find a context value.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://blog.merovius.de/posts/2020-07-20-parametric-context/","summary":"Go\u0026rsquo;s Context.Value is controversial because of a lack of type-safety. I design a solution for that based on the new generics design draft.","title":"Parametric context"},{"content":"tl;dr: I provide a very high-level overview of what Go-the-language means vs. Go-the-ecosystem vs. Go-an-implementation. I also try to provide specific references to what documentation is most useful for what purpose. See the bottom-most section for that.\nWhen we talk about \u0026ldquo;Go\u0026rdquo;, depending on context, we can mean very different things. This is my attempt at providing a very high-level overview of the language and ecosystem and to link to the relevant documentation about how each part fits together (it\u0026rsquo;s also a bit of a hodgepodge though, addressing individual random questions I encountered recently). So, let\u0026rsquo;s dive in:\nThe Go programming language The bottom turtle is Go, the programming language. It defines the format and meaning of source code and the authoritative source is the Go language specification. If something doesn\u0026rsquo;t conform to the spec, it\u0026rsquo;s not \u0026ldquo;Go\u0026rdquo;. And conversely, if something isn\u0026rsquo;t mentioned in the spec it\u0026rsquo;s not part of the language. The language spec is maintained by the Go team and versioned, with a new release roughly every six months. At the time I wrote this post, the newest release was version 1.12.\nThe domain of the spec are\n The grammar of the language Types, values and their semantics What identifiers are predeclared and what their meaning is How Go programs get executed The special package unsafe (though not all of its semantics)  The spec alone should enable you to write a compiler for Go. And indeed, there are many different compilers.\nA Go compiler and runtime The language spec is a text document, which is not useful in and of itself. For that you need software that actually implements these semantics. This is done by a compiler (which analyzes and checks the source code and transforms it into an executable format) and a runtime (which provides the necessary environment to actually run the code). There are many such combinations and they all differ a bit more or a bit less. Examples are\n gc, a compiler and runtime written in pure Go (with some assembly) by the Go team themselves and versioned and released together with the language. Unlike other such tools, gc doesn\u0026rsquo;t strictly separate the compiler, assembler and linker - they end up sharing a lot of code and some of the classical responsibilities move or are shared between them. As such, it\u0026rsquo;s in general not possible to e.g. link packages compiled by different versions of gc. gccgo and libgo, a frontend for gcc and a runtime. It\u0026rsquo;s written in C and maintained by the Go team. It lives in the gcc organization though and is released according to the gcc release schedule and thus often lags a bit behind the \u0026ldquo;latest\u0026rdquo; version of the Go spec. llgo, a frontend for LLVM. I don\u0026rsquo;t know much else about it. gopherjs, compiling Go code into javascript and using a javascript VM plus some custom code as a runtime. Long-term, it\u0026rsquo;ll probably be made obsolete by gc gaining native support for WebAssembly. tinygo, an incomplete implementation targeting small code size. Runs on either bare-metal micro-controllers or WebAssembly VMs, with a custom runtime. Due to its limitations it doesn\u0026rsquo;t technically implement Go - notably, it doesn\u0026rsquo;t include a garbage collector, concurrency or reflection.  There are more, but this gives you an overview over the variety of implementations. Each of these made potentially different choices for how to implement the language and have their own idiosyncrasies. Examples (some of them a bit exotic, to illustrate) where they might differ are:\n Size of int/uint - the language allows them to be either 32 or 64 bit wide. How fundamental functionalities of the runtime, like allocation, garbage collection or concurrency are implemented. The order of ranging over a map isn\u0026rsquo;t defined in the language - gc famously explicitly randomizes it, gopherjs uses (last time I checked) whatever the javascript implementation you are running on uses. How much extra space append allocates if it needs to - not however, when it allocates extra space. How conversions between unsafe.Pointer and uintptr happen. gc, in particular, comes with its own set of rules regarding when these conversions are valid and when they aren\u0026rsquo;t. In general, the unsafe package is virtual and implemented in the compiler.  In general, relying on details not mentioned in the spec (in particular the ones mentioned here) makes your program compile with different compilers, but not work as expected. So you should avoid it if possible.\nIf you install Go via a \u0026ldquo;normal\u0026rdquo; way (by downloading it from the website, or installing it via a package manager), you\u0026rsquo;ll get gc and the official runtime by the Go team. And if the context doesn\u0026rsquo;t imply otherwise, when we talk about how \u0026ldquo;Go does things\u0026rdquo;, we usually refer to gc. It\u0026rsquo;s the main implementation.\nThe standard library The standard library is a set of packages that come with Go and can be relied upon to immediately build useful applications with. It too is maintained by the Go team and versioned and released together with the language and compiler. In general the standard library of one implementation will only work with the compiler it comes with. The reason is that most (but not all) of the runtime is part of the standard library (mainly in the packages runtime, reflect, syscall). As the compiler needs to generate code compatible with the used runtime, both need to come from the same version. The API of the standard library is stable and won\u0026rsquo;t change in incompatible ways, so a Go program written against a given version of the standard library will continue to work as expected with future versions of the compiler.\nSome implementations use their own version of some or all of the standard library - in particular, the runtime, reflect, unsafe and syscall packages are completely implementation-defined. As an example, I believe that AppEngine Standard used to re-define parts of the standard library for security and safety. In general, implementations try to make that transparent to the user.\nThere is also a separate set of repositories, colloquially referred to as x or \u0026ldquo;the subrepositories\u0026rdquo;. They contain packages which are developed and maintained by the Go team with all the same processes, but are not on the same release schedule as the language and have less strict compatibility guarantees (and commitment to maintainership) than Go itself. The packages in there are either experimental (for potential future inclusion in the standard library), not widely useful enough to be included in the standard library or, in rare cases, a way for people on the Go team to work on code using the same review processes they are used to.\nAgain, when referring to \u0026ldquo;the standard library\u0026rdquo; devoid of extra context, we mean the officially maintained and distributed one, hosted on golang.org.\nThe build tool To make the language user-friendly, you need a build tool. The primary role of this tool is to find the package you want to compile, find all of its dependencies, and execute the compiler and linker with the arguments necessary to build them. Go (the language) has support for packages, which combine multiple source files into one unit of compilation. And it defines how to import and use other packages. But importantly, it doesn\u0026rsquo;t define how import paths map to source files or how they are laid out on disk. As such, each build tool comes with its own ideas for this. It\u0026rsquo;s possible to use a generic build tool (like Make) for this purpose, but there are a bunch of Go-specific ones:\n The go tool1 is the build tool officially maintained by the Go team. It is versioned and released with the language (and gc and the standard library). It expects a directory called GOROOT (from an environment variable, with a compiled default) to contain the compiler, the standard library and various other tools. And it expects all source code in a single directory called GOPATH (from an environment variable, defaulting to $HOME/go or equivalent). Specifically, package a/b is expected to have its source at $GOPATH/src/a/b/c.go etc. And $GOPATH/src/a/b is expected to only contain source files of one package. It also has a mechanism to download a package and its dependencies recursively from an arbitrary server, in a fully decentralized scheme, though it does not support versioning or verification of downloads. The go tool also contains extra tooling for testing Go code, reading documentation (golang.org is served by the Go tool), file bugs, run various tools… gopherjs comes with its own build tool, that largely mimics the Go tool. gomobile is a build tool specifically to build Go code for mobile operating systems. dep, gb, glide,… are community-developed build-tools and dependency managers, each with their own approach to file layout (some are compatible with the go tool, some aren\u0026rsquo;t) and dependency declarations. bazel is the open source version of Google\u0026rsquo;s own build system. While it\u0026rsquo;s not actually Go-specific, I\u0026rsquo;m mentioning it explicitly due to common claims that idiosyncrasies of the go tool are intended to serve Google\u0026rsquo;s own use cases, in conflict with the needs of the community. However, the go tool (and many public tools) can\u0026rsquo;t be used at Google, because bazel uses an incompatible file layout.  The build tool is what most users directly interface with and as such, it\u0026rsquo;s what largely determines aspects of the Go ecosystem and how packages can be combined and thus how different Go programmers interact. As above, the go tool is what\u0026rsquo;s implicitly referred to (unless other context is specified) and thus its design decisions significantly influence public opinion about \u0026ldquo;Go\u0026rdquo;. While there are alternative tools and they have wide adoption for use cases like company-internal code, the open source community in general expects code to conform to the expectations of the go tool, which (among other things) means:\n Be available as source code. The go tool has little support for binary distribution of packages, and what little it has is going to be removed soon. Be documented according to the godoc format. Have tests that can be run via go test. Be fully compilable by a go build (together with the next one, this is usually called being \u0026ldquo;go-gettable\u0026rdquo;). In particular, to use go generate if generating source-code or metaprogramming is required and commit the generated artifacts. Namespace import paths with a domain-name as the first component and have that domain-name either be a well-known code hoster or have a webserver running on it, so that go get works and can find the source code of dependencies. Have one package per directory and use build constraints for conditional compilation.  The documentation of the go tool is very comprehensive and probably a good starting point to learn how Go implements various ecosystem aspects.\nTools Go\u0026rsquo;s standard library includes several packages to interact with Go source code and the x/tools subrepo contains even more. As a result (and due to a strong desire to keep the canonical Go distribution lean), Go has developed a strong culture of developing third-party tools. In general, these tools need to know where to find source code, and might need access to type information. The go/build package implements the conventions used by the Go tool, and can thus also serve as documentation for parts of its build process. The downside is that tools built on top of it sometimes don\u0026rsquo;t work with code relying on other build tools. That\u0026rsquo;s why there is a new package in development which integrates nicely with other build tools.\nBy its nature the list of Go tools is long and everybody has their own preferences. But broadly, they contain:\n Tools developed by the Go team and released as part of the distribution. This includes tools for automatically formatting source code, coverage testing, runtime tracing and profiling, a static analyzer for common mistakes and a mostly obsolete tool to migrate code to new Go versions. These are generally accesed via go tool \u0026lt;cmd\u0026gt;. Tools developed by the Go team and maintained out-of-tree. This includes tools to write blog posts and presentations, easily do large refactors, automatically find and fix import paths and a language server. Third-party tools - too many to count. There are many lists of these; here is one.  In Summary I wanted to end this with a short list of references for beginners who feel lost. So this is where you should go, if you:\n Want to start learning Go. Want to understand how a specific language construct works. Want to nitpick what is or is not valid Go and why. Want documentation about what the go tool does Also available via go help. It sometimes references other topics, that you can also see on the web, but not nicely. Want to write code that adheres to community standards. Want to test your code. Want to find new packages or look at documentation of public packages.  There are many more useful supplementary documents, but this should serve as a good start. Please let me know on Twitter if you are a beginner and there\u0026rsquo;s an area of Go you are missing from this overview (I might follow this up with more specific topics), or a specific reference you found helpful. You can also drop me a note if you\u0026rsquo;re a more experienced Gopher and think I missed something important (but keep in mind that I intentionally left out most references, so as to keep the ways forward crisp and clear :) ).\n   The Go team is currently rolling out support for modules, which is a unit of code distribution above packages, including support for versioning and more infrastructure to solve some issues with the \u0026ldquo;traditional\u0026rdquo; go tool. With that, basically everything in that paragraph becomes obsolete. However, for now the module support exists but is opt-in. And as the point of this article is to provide an overview of the separation of concerns, which doesn\u0026rsquo;t actually change, I felt it was better to stay within ye olden days - for now.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://blog.merovius.de/posts/2019-06-12-birdseye-go/","summary":"I provide a very high-level overview of what Go-the-language means vs. Go-the-ecosystem vs. Go-an-implementation. I also try to provide specific references to what documentation is most useful for what purpose.","title":"A bird's eye view of Go"},{"content":"tl;dr: Roughtime can be (ab)used for Trusted Timestamping. I wrote a simple tool as a PoC\nRecently, Cloudflare announced that they are now running a roughtime server. Roughtime is a cryptographically secured time-synchronization protocol - think NTP with signatures. For an actual description of how it works, I recommend reading the Cloudflare blog post. But at it\u0026rsquo;s very core (oversimplification ahead), the user chooses an arbitrary (usually randomly generated) nonce and the server signs it, plus the current time.\nOne thing roughtime adds on top of this, is the ability to build a chain of requests. This is achieved by taking a hash of a response, combining it with a randomly generated \u0026ldquo;blind\u0026rdquo; and using the combined hash as a nonce to the next request. The intended use-case of this is that if a server provides the wrong time or otherwise misbehaves, you can obtain cryptographic proof of that fact by getting a timestamped signature of its response from a different server. By storing the initial nonce, generated blinds and responses, the entire chain can be validated later.\nWhen I saw Cloudflares announcement, my first thought was that it should be possible to use a roughtime server as a Time Stamping Authority. The goal is, to obtain a cryptographic proof, that you owned a particular document at the current point in time - for example to ensure you can proof original authorship without publishing the document itself.\nThe simplest way to achieve this using roughtime is to use the SHA512 hash of the file as an initial nonce. That way, the roughtime server signs that hash together with the current time with their private key. By using the roughtime chain protocol, you can get that proof corroborated by multiple servers.\nYou can also think of extending this, to get stronger properties. Using the hash of the file as a nonce only proves that the file existed before that specific point in time. It also doesn\u0026rsquo;t actually prove that you had the file, but only the hash. This can be remediated though. If we run a regular roughtime request, the resulting response is unpredictable (to us) and signs the current time. Thus, if we use a hash of that response as a prefix \u0026ldquo;salt\u0026rdquo; of the file itself, the resulting hash proofs that we knew the file after that chain ran. We can then use that hash as a nonce for another roughtime chain and get a proof that we had the file at a specific point (or rather, a small interval) in time. Furthermore, we can opt to use the file-hash not as the nonce itself, but as a blind. The advantage is, that the blind is never transmitted over the network, so the actual proof is only available to us (if we use it as a nonce, an eavesdropper could intercept the proof). I illustrated these options in a recent talk I gave on the subject.\nThese ideas are mostly academic. I\u0026rsquo;m not sure how useful these properties are in practice. Nevertheless, the idea intriguiged me enough to implement it in a simple tool. It\u0026rsquo;s in a pretty rough, proof-of-concept like shape and I don\u0026rsquo;t know if I will ever progress it from there. It also comes with a client implementation of the roughtime protocol in Go - initially I was not aware that there already was a Go implementation, but that also is not go-gettable. Either way, it was fun to implement it myself :)\n","permalink":"https://blog.merovius.de/posts/2018-10-10-using-roughtime-as-a-cryptographic-notary/","summary":"Roughtime can be (ab)used for Trusted Timestamping. I wrote a simple tool as a PoC","title":"Using roughtime as a \"cryptographic notary\""},{"content":"tl;dr: I describe a way to simplify the generics design. The ideas are not particularly novel and have been expressed to various degrees by other people as well. I hope to provide a more complete view of the design though.\nRecently a Problem Overview and Draft Design for generics in Go have dropped. Since then, predictably, there has been a bunch of chatter on the intertubez about it. This is a summary of my thoughts, so far, on the subject - after a bunch of discussions on Twitter and Reddit.\nNote: The design is called \u0026ldquo;Contracts\u0026rdquo;, but I will refer to it as \u0026ldquo;the design doc\u0026rdquo; here. When I say \u0026ldquo;contracts\u0026rdquo;, I will refer to the specific part of the design to express constraints.\nContracts vs. Interfaces First, there is a common observation of overlap between generics and interfaces. To untangle that, we can say that when we use \u0026ldquo;generics\u0026rdquo;, what we mean is constrained parametric polymorphism. Go already allows polymorphism by using interfaces. This desgn doc adds two things: One, a way to add type-parameters to functions and types. And two, a syntax to constrain those type-parameters to a subset that allows specific operations, via contracts.\nThe latter is where the overlap lies: Interfaces already allow you to constrain arguments to types that allow certain operations. In a way, what contracts add to this, is that those operations can not only be method calls, but also allow (and constrain) builtin operators and functions to be used and to allow or disallow certain composite types (though that mainly affects map).\nContracts allow that by the way they are specified: You write a function body (including arguments, whose notational type becomes the type-variable of the contract) containing all the statements/expressions you wish to be able to do. When instantiating a generic type/function with a given set of type-arguments, the compiler will try to substitute the corresponding type-variable in the contract body and allow the instantiation, if that body type-checks.\nThe cost of contracts After talking a bit through some examples, I feel that contracts optimize for the wrong thing. The analogy I came up with is vocabulary vs. grammar.\nThe contracts design is appealing to a good degree, because it uses familiar syntax: You don\u0026rsquo;t have to learn any new syntax or language to express your contract. Just write natural Go code and have that express your constraints for you. I call this the \u0026ldquo;grammar\u0026rdquo; of constraints: The structure that you use to input them.\nOn the other hand, for the user of Go, the relevant question is what constraints are possible to express and how to express them. They might be interested in deduplicating values in their algorithm, which requires equality-operations. Or they might want to do comparisons (e.g. Max), which requires \u0026gt;. I call this the vocabulary: What is the correct way to express the set of constraints that my algorithm needs?\nThe issue now, is that while the grammar of constraints might be obvious, it is not always clear what the actual semantic constraints that generates are. A simple example is map-keys. The design doc uses the contract\ncontract comparable (t T) { t == t } to specify types that are valid map-keyes. But to a beginner, it is not immediately obvious, what comparisons have to do with maps. An alternative would be\ncontract mapkey (t T) { var _ map[t]bool } But which is better? Similarly, these two contracts\ncontract mult (t T) { t = t * t } contract add (t T) { t = t + t } seem very similar, but they are, in theory at least, fundamentally different. Not only because add allows string, while mult doesn\u0026rsquo;t. But also, because technically any type that supports * also supports - and /. And then there\u0026rsquo;s\ncontract div (t T) { t = t % t } which creates another completely different set of types and allowed operators.\nA third example is\ncontract stringlike (t T) { append([]byte(nil), t...) } This allows any type with underlying type string or []byte, but nothing else. And again, technically that would imply allowing index-operations and len. But does the compiler understand that?\nLastly, it\u0026rsquo;s not really clear how len, cap, make or range would work. For example, all these contracts are superficially valid:\ncontract rangeable (t T) { for x := range t { fmt.Println(x) } } contract lengthed (t T) { var _ int = len(t) } contract capped (t T) { var _ int = cap(t) } contract makeable (t T) { t = make(T) } contract makeable2 (t T) { t = make(T, 0) } But in all these cases, they allow some subset of channel, map, slice and array types, with vastly different interpretations of these operations, depending on the kind of type used - to the degree, that code using them would usually be nonsensical. Disallowing these, however, opens questions about the claim of familiar Go syntax, as we now have to make decisions what sort of expressions and statements we do or don\u0026rsquo;t allow in a contract.\nThis is why I say contracts optimize for grammar, instead of vocabulary. The programmer is interested in the vocabulary - what does the contract actually mean and what contract should they use? But the vocabulary is obscured by the grammar - because we use Go syntax, to understand a given contract we need to know a bunch of things about what the compiler is and is not able to infer from it.\nThis is why I don\u0026rsquo;t really buy the argument of not wanting to learn a bunch of new syntax or new identifiers for constraints: You still have to learn that vocabulary, but you express it in an obscure and unnatural grammar. I hope to show that we can introduce the power of generics while also using familiar grammar and with minimal addition of vocabulary.\nScrapping contracts Now, I\u0026rsquo;m not the first person to suggest this, but I think we should consider scrapping contracts from the design. We can still retain type-parameters and we can still have constraints, but we express them via interfaces instead. I should point out, that - for now - I\u0026rsquo;m intentionally optimizing for simplicity of the design, at the cost of some boilerplate and some loss of power. I will later try and provide some alternatives to compensate for that in part. But there is still likely going to remain a net cost in expressiveness. Personally, I think that tradeoff is worth exploring.\nThe new design would retain type-parameters and most of their syntax. The difference is that type-parameters are a full argument list. The type of an argument has to be an interface type. It can be ellided, in which case it defaults to the type of the following type-parameter. The last type-parameter defaults to interface{}. As a bonus, this allows providing multiple sets of constraints on one declaration:\nfunc Map(type A, B) (s []A, f func(A) B) []B { var out []B for _, a := range s { out = f(a) } return out } func Stringify(type A fmt.Stringer) (s []A) []string { // Because of the signature of fmt.Stringer.String, we can infer all the // type-arguments here. Note that A does not *have* to be passed boxed in an // interface. A.String is still a valid method-expression for any fmt.Stringer. return Map(s, A.String) } We still want to be able to express multiple, interdependent parameters, which we can, via parametric interfaces:\ntype Graph(type Node, Edge) interface { Nodes(Edge) []Node Edges(Node) []Edge } func ShortestPath(type Node, Edge) (g Graph(Node, Edge), from, to Node) []Edge { // … } // Undirected Graph as an adjacency list. This could be further parameterized, // to allow for user-defined paylooads. type AdjacencyList [][]int func (g AdjacencyList) Nodes(edge [2]int) []int { return edge[:] } func (g AdjacencyList) Edges(node int) [][2]int { var out [][2]int for _, v := range g[node] { out = append(out, [2]int{node, v} if v != node { out = append(out, [2]int{v, node}) } } return out } func main() { g := AdjacencyList{…} // Types could be infered here, as the names of methods are unique, so we can // look at the methods Nodes and Edges of AdjacencyList to infer the // type-arguments. path := ShortestPath(g, 0, len(g)-1) fmt.Println(path) } The last example is relevant to the difference in power between contracts and interfaces: Usage of operators. We can still express the concept, but this is where the increased boilerplate comes in:\nfunc Max(type T)(a, b T, less func(T, T) bool) T { if less(a, b) { return b } return a } func main() { fmt.Println(Max(a, b int, func(a, b int) { return a \u0026lt; b })) } I will try to show some ways to get rid of that boilerplate later. For now, let\u0026rsquo;s just treat it as a necessary evil of this idea. Though it should be mentioned, that while this is more cumbersome, it\u0026rsquo;s still just as typesafe as contracts (as opposed to, say, a reflect-based generic Max).\nSo, scrapping contracts leaves us with more boilerplate, but just the same set of concepts we can express - though we do have to pass in any builtin operations we want to perform as extra functions (or express them in an interface). In exchange, we get\n Only one way to specify constraints. A simpler spec (we don\u0026rsquo;t need to add a new concept, contracts, to the language) and a saved (pseudo-)keyword. A simpler compiler: We don\u0026rsquo;t need to add a solver to deduce constraints from a given contract. The constraint-checker already exists. Still a well-known, though less powerfull, language to express constraints, with interfaces. Simple syntax (same as normal arglists) for having multiple sets of constraints in one declaration. Trivially good error messages. Types passed in need only be checked for consistency and interface satisfaction - the latter is already implemented, including good error messages.  Getting rid of boilerplate I see two main ways to get rid of boilerplate: Adding methods to builtin types, or what I call pseudo-interfaces.\nMethods on builtin types An obvious idea is to not use operators in generic code, but instead use method-call syntax. That is, we\u0026rsquo;d do something akin to\nfunc Max(type T Ordered) (a, b T) T { if a.Less(b) { return b } return a } To actually reduce the boilerplate, we\u0026rsquo;d predefine methods for all the operators on the builtin types. That would allow us to call Max with int, for example.\nUnfortunately, I can see a bunch of roadblocks to make this work. Methods are not promoted to derived types, so you couldn\u0026rsquo;t use Max with e.g. time.Duration, which has underlying type int64, but is not the same type. We\u0026rsquo;d probably want those methods to be \u0026ldquo;special\u0026rdquo; in that they automatically get promoted to any type whose underlying type is predeclared. That introduces compatibility issues of clashing Method/Field names.\nAt the end, to express that Less has to take the same argument as the receiver type, Ordered might look something like this:\ntype Ordered(T) interface {  Less(T) bool }  func Max(type T Ordered(T)) (a, b T) T {  if a.Less(b) {  return b  }  return a }  // In the universe block:  // Implements Ordered(int). func (a int) Less(b int) bool {  retun a \u0026lt; b } Though it\u0026rsquo;s not clear, whether a parameter like T Ordered(T) should be allowed. And this would technically allow to implement Ordered(int) on a custom type. While that probably won\u0026rsquo;t be very useful (the majority of usecases will require T Ordered(T)), it\u0026rsquo;s not excluded.\nPseudo-interfaces Unfortunately I didn\u0026rsquo;t have a lot of time the last couple of days, so I got beat to the punch on this. Matt Sherman described the idea first and called the concept \u0026ldquo;typeclasses\u0026rdquo;. I will stick with pseudo-interface, because it fits better in the general concept of this description.\nThe idea is to introduce a set of types into the language that can be used like interfaces (including embedding), but instead of providing methods, provide operators. There is a limited set of base types that need to be provided:\npseudo-interface | Allowed operators -----------------+------------------- comparable | ==, != ordered | \u0026lt;, \u0026lt;= \u0026gt; \u0026gt;= boolean | ||, \u0026amp;\u0026amp;, ! bitwise | ^, %, \u0026amp;, \u0026amp;^, \u0026lt;\u0026lt;, \u0026gt;\u0026gt; arith | +, -, *, / concat | + complex | real(z), imag(z) nilable | v == nil and a set of derived pseudo-interfaces:\npseudo-interface | definition -----------------+----------------------------------------------------- num | interface { comparable; ordered; arith } integral | interface { num; bitwise } stringy | interface { comparable; ordered; concat; len() int } iface | interface { comparable; nilable } The pseudo-interfaces would be declared in the universe block, as predeclared identifiers. This makes them backwards-compatible (as opposed to methods on builtin types), because any existing identifier would just shadow these (akin to how you can have a variable with name string).\nBitshift-operators currently are restricted when used with constants overflowing the width of an integral type. For generic code, this restriction would be lifted (as the size is not statically known) and instead the behavior is equivalent to if the right operand is an uint variable with the given value.\nThis would allow us to write\nfunc Max(type T ordered) (a, b T) T { if a \u0026lt; b { return b } return a } Notably, the list of pseudo-interfaces doesn\u0026rsquo;t include anything related to channel-, slice- or map-operations (or other composite types). The idea is to instead use a type literal directly:\ntype Keys(type K, V) (m map[K]V) []K { var out []K for k := range m { out = append(out, k) } return out } As every type supporting, e.g. map operations, need to have underlying type map[K]V, it\u0026rsquo;s thus assignable to that type and can be passed to Keys as is. That is, this is completely legal:\nfunc main() { type MyMap map[string]int var m = MyMap{ \u0026#34;foo\u0026#34;: 23, \u0026#34;bar\u0026#34;: 42, } fmt.Println(Keys(m)) } This also solves another problem with contracts: The ambiguity of len, cap and range. As the actual kind of the value is not only known during compilation of the generic function, but even obvious from the code, there is no question about the intended semantics.\nShould Go ever grow operator overloading via operator methods, the pseudo-interfaces could be changed into actual interfaces, containing the necessary methods. Of course, that implies that operator overloading would retain the properties of existing operators, e.g. that having == implies having !=, or having - implying having +. Personally, I consider that a good thing - it limits the abuse of operator overloading for nonsensical operations (say, \u0026lt;\u0026lt; for writing to an io.Writer).\nI\u0026rsquo;m not trying to advocate for operator overloading, but think it\u0026rsquo;s worth mentioning that this design leaves the door open to that.\nBut performance A possible criticism of either of these approaches is, that operators have better performance than dynamic dispatch to a method. I believe (vigorous handwaving ahead) that this is no different in the existing contracts proposal. If generic code is compiled generically, it still needs to employ some means of dynamic dispatch for operators. If, on the other hand, it\u0026rsquo;s compiled instantiated, then the compiler would also be able to devirtualize the interfaces - and then inline the method definition.\nConclusion I\u0026rsquo;ve previously said that I\u0026rsquo;m \u0026ldquo;meh\u0026rdquo; on the design doc, which is the strongest form of endorsement a generics proposal could ever get from me. After some discussion, I\u0026rsquo;m more and more convinced that while contracts seem conceptually simple, they create a plethora of implementation- and usage questions. I\u0026rsquo;m not sure, the supposed advantage of contracts, of a well-known syntax, holds up to scrutiny when it comes to mapping that to the actually derived constraints or writing contracts. There are also many open questions in regards to contracts, a bunch of them related to the ambiguity of Go-expressions. As a result, I\u0026rsquo;m starting to feel more negative towards them - they look like an elegant idea, but in practice, they have a lot of weird corners.\nThis design is similar (AIUI) to the type functions proposal, so I assume there are good reasons the Go team does not want this. The difference is mainly the absence of operator methods in favor of pseudo-interfaces or explicit method calls. This design also handwaves a couple of important implementation questions - the justification for that is that these questions (e.g. type inference and code generation) should be able to be taken from the design doc with minimal changes. It\u0026rsquo;s entirely possible that I am overlooking something, though.\n","permalink":"https://blog.merovius.de/posts/2018-09-05-scrapping_contracts/","summary":"I describe a way to simplify the generics design. The ideas are not particularly novel and have been expressed to various degrees by other people as well. I hope to provide a more complete view of the design though.","title":"Scrapping contracts"},{"content":"tl;dr: I explain what co-, contra- and invariance are and what the implications for Go\u0026rsquo;s type system would be. In particular, why it\u0026rsquo;s impossible to have variance in slices.\nA question that comes up relatively often with Go newcomers is \u0026ldquo;why can\u0026rsquo;t I pass e.g. an []int to a func([]interface{})\u0026rdquo;? In this post I want to explore this question and its implications for Go. But the concept of variance (which this is about) is also useful in other languages.\nVariance describes what happens to subtype relationships, when they are used in composite types. In this context, \u0026ldquo;A is a subtype of B\u0026rdquo; means that a value of type A can always be used, where a value of type B is required. Go doesn\u0026rsquo;t have explicit subtype relationships - the closest it has is assignability which mostly determines whether types can be used interchangeably. Probably the most important case of this is given by interfaces: If a type T (whether its a concrete type, or itself an interface) implements an interface I, then T can be viewed as a subtype of I. In that sense, *bytes.Buffer is a subtype of io.ReadWriter, which is a subtype of io.Reader. And every type is a subtype of interface{}.\nThe easiest way to understand what variance means, is to look at function types. Let\u0026rsquo;s assume, we have a type and a subtype - for example, let\u0026rsquo;s look at *bytes.Buffer as a subtype of io.Reader. Say, we have a func() *bytes.Buffer. We could also use this like a func() io.Reader - we just reinterpret the return value as an io.Reader. The reverse is not true: We can\u0026rsquo;t treat a func() io.Reader as a func() *bytes.Buffer, because not every io.Reader is a *bytes.Buffer. So, function return values could preserve the direction of subtyping relationships: If A is a subtype of B, func() A could be a subtype of func() B. This is called covariance.\nfunc F() io.Reader { \treturn new(bytes.Buffer) }  func G() *bytes.Buffer { \treturn new(bytes.Buffer) }  func Use(f func() io.Reader) { \tuseReader(f()) }  func main() { \tUse(F) // Works  \tUse(G) // Doesn\u0026#39;t work right now; but *could* be made equivalent to… \tUse(func() io.Reader { return G() }) } On the other hand, say we have a func(*bytes.Buffer). Now we can\u0026rsquo;t use that as a func(io.Reader): You can\u0026rsquo;t call it with an io.Reader. But we can do the reverse. If we have a *bytes.Buffer, we can call a func(io.Reader) with it. Thus, function arguments reverse the subtype relationship: If A is a subtype of B, then func(B) could be a subtype of func(A). This is called contravariance.\nfunc F(r io.Reader) { \tuseReader(r) }  func G(r *bytes.Buffer) { \tuseReader(r) }  func Use(f func(*bytes.Buffer)) { \tb := new(bytes.Buffer) \tf(b) }  func main() { \tUse(F) // Doesn\u0026#39;t work right now; but *could* be made equivalent to… \tUse(func(r *bytes.Buffer) { F(r) })  \tUse(G) // Works } So, func is contravariant for arguments and covariant for return values. Of course, we can combine the two: If A and C are subtypes of B and D respectively, we can make func(B) C a subtype of func(A) D, by converting like this:\n// *os.PathError implements error  func F(r io.Reader) *os.PathError { \t// ... }  func Use(f func(*bytes.Buffer) error) { \tb := new(bytes.Buffer) \terr := f(b) \tuseError(err) }  func main() { \tUse(F) // Could be made to be equivalent to \tUse(func(r *bytes.Buffer) error { return F(r) }) } However, func(A) C and func(B) D are incompatible. Neither can be a subtype of the other:\nfunc F(r *bytes.Buffer) *os.PathError { \t// ... }  func UseF(f func(io.Reader) error) { \tb := strings.NewReader(\u0026#34;foobar\u0026#34;) \terr := f(b) \tuseError(err) }  func G(r io.Reader) error { \t// ... }  func UseG(f func(*bytes.Buffer) *os.PathErorr) { \tb := new(bytes.Buffer) \terr := f() \tusePathError(err) }  func main() { \tUseF(F) // Can\u0026#39;t work, because: \tUseF(func(r io.Reader) error { \treturn F(r) // type-error: io.Reader is not *bytes.Buffer \t})  \tUseG(G) // Can\u0026#39;t work, because: \tUseG(func(r *bytes.Buffer) *os.PathError { \treturn G(r) // type-error: error is not *os.PathError \t}) } So in this case, there just is not relationship between the composite types. This is called invariance.\n Now, we can get back to our opening question: Why can\u0026rsquo;t you use []int as []interface{}? This really is the question \u0026ldquo;Why are slice-types invariant\u0026rdquo;?. The questioner assumes that because int is a subtype of interface{}, we should also make []int a subtype of []interface{}. However, we can now see a simple problem with that. Slices support (among other things) two fundamental operations, that we can roughly translate into function calls:\nas := make([]A, 10) a := as[0] // func Get(as []A, i int) A as[1] = a // func Set(as []A, i int, a A) This shows a clear problem: The type A appears both as an argument and as a return type. So it appears both covariantly and contravariantly. So while with functions there is a relatively clear-cut answer to how variance might work, it just doesn\u0026rsquo;t make a lot of sense for slices. Reading from it would require covariance but writing to it would require contravariance. In other words: If you\u0026rsquo;d make []int a subtype of []interface{} you\u0026rsquo;d need to explain how this code would work:\nfunc G() { \tv := []int{1,2,3} \tF(v) \tfmt.Println(v) }  func F(v []interface{}) { \t// string is a subtype of interface{}, so this should be valid \tv[0] = \u0026#34;Oops\u0026#34; } Channels give another interesting perspective here. The bidirectional channel type has the same issue as slices: Receiving requires covariance, whereas sending requires contravariance. But you can restrict the directionality of a channel and only allow send- or receive-operations respectively. So while chan A and chan B would not be related, we could make \u0026lt;-chan A a subtype of \u0026lt;-chan B. And chan\u0026lt;- B a subtype of chan\u0026lt;- A.\nIn that sense, read-only types have the potential to at least theoretically allow variance for slices. While []int still wouldn\u0026rsquo;t be a subtype of []interface{}, we could make ro []int a subtype of ro []interface{} (borrowing the syntax from the proposal).\n Lastly, I want to emphasize that all of these are just the theoretical issues with adding variance to Go\u0026rsquo;s type system. I consider them harder, but even if we could solve them we would still run into practical issues. The most pressing of which is that subtypes have different memory representations:\nvar ( \t// super pseudo-code to illustrate \tx *bytes.Buffer // unsafe.Pointer \ty io.ReadWriter // struct{ itable *itab; value unsafe.Pointer } \t// where itable has two entries \tz io.Reader\t// struct{ itable *itab; value unsafe.Pointer } \t// where itable has one entry ) So even though you might think that all interfaces have the same memory representation, they actually don\u0026rsquo;t, because the method tables have a different assumed layout. So in code like this\nfunc Do(f func() io.Reader) { \tr := f() \tr.Read(buf) }  func F() io.Reader { \treturn new(bytes.Buffer) }  func G() io.ReadWriter { \treturn new(bytes.Buffer) }  func H() *bytes.Buffer { \treturn new(bytes.Buffer) }  func main() { \t// All of F, G, H should be subtypes of func() io.Reader \tDo(F) \tDo(G) \tDo(H) } there still needs to be a place where the return value of H is wrapped into an io.Reader and there needs to be a place where the itable of the return value of G is transformed into the correct format expected for an io.Reader. This isn\u0026rsquo;t a huge problem for func: The compiler can generate the appropriate wrappers at the call site in main. There is a performance overhead, but only code that actually uses this form of subtyping needs to pay it. However, it becomes significant problem for slices.\nFor slices, we must either a) convert the []int into an []interface{} when passing it, meaning an allocation and complete copy. Or b) delay the conversion between int and interface{} until the access, which would mean that every slice access now has to go through an indirect function call - just in case anyone would ever pass us a subtype of what we are expecting. Both options seem prohibitively expensive for Go\u0026rsquo;s goals.\n","permalink":"https://blog.merovius.de/posts/2018-06-03-why-doesnt-go-have-variance-in/","summary":"I explain what co-, contra- and invariance are and what the implications for Go\u0026rsquo;s type system would be. In particular, why it\u0026rsquo;s impossible to have variance in slices.","title":"Why doesn't Go have variance in its type system?"},{"content":"I\u0026rsquo;ve recently taken a liking to persistent datastructures. These are datastructures where instead of mutating data in-place, you are creating a new version of the datastructures, that shares most of its state with the previous version. Not all datastructures can be implemented efficiently like this, but those that do get a couple of immediate benefits - keeping old versions around allows you to get cheap snapshotting and copying. It is trivial to pass a copy to a different thread and you don\u0026rsquo;t have to worry about concurrent writes, as neither actually mutates any shared state.\nPersistent datastructures are popular in functional programming languages, but I also found the idea a useful tool to model datastructures in Go. Go\u0026rsquo;s interfaces provide a nice way to model them and make them easy to reason about. In this post, I will try to illustrate this with a couple of examples.\nThere are four key ideas I\u0026rsquo;d like you to walk away with:\n Modeling datastructures as persistent (if possible) makes them easier to reason about. When you want to use sum types, try to think of the common properties you are trying to abstract over instead - put those in an interface. Separate out the required from the provided interface. Make the former an interface type, provide the latter as functions or a wrapper. Doing these allows you to add more efficient implementations later, when you discover they are necessary.  Linked lists This is more of an illustrative example, to demonstrate the techniques, than actually useful. But one of the simplest datastructures existing are linked lists: A list of nodes, where each node has a value and possibly a next node (unless we are at the end of the List). In functional languages, you\u0026rsquo;d use a sum type to express this:\ntype List a = Node a (List a) -- either it\u0026#39;s a node with a value and the rest of the list  | End -- or it\u0026#39;s the end of the list Go infamously does not have sum types, but we can use interfaces to instead. The classical way would be something like\ntype List interface {  // We use an unexported marker-method. As nothing outside the current package  // can implement this unexported method, we get control over all  // implementations of List and can thus de-facto close the set of possible  // types.  list() }  type Node struct {  Value int  Next List }  func (Node) list() {}  type End struct {}  func (End) list() {}  func Value(l List) (v int, ok bool) {  switch l := l.(type) {  case Node:  return l.Value, true  case End:  return 0, false  default:  // This should never happen. Someone violated our sum-type assumption.  panic(fmt.Errorf(\u0026#34;unknown type %T\u0026#34;, l))  } } This works, but it is not really idiomatic Go code. It is error-prone and easy to misuse, leading to potential panics. But there is a different way to model this using interfaces, closer to how they are intended. Instead of expressing what a list is\n A list is either a value and a next element, or the end of the list\n we say what we want a list to be able to do:\n A list has a current element and may have a tail\n type List interface {  // Value returns the current value of the list  Value() int  // Next returns the tail of the list, or nil, if this is the last node.  Next() List }  type node struct {  value int  next List }  func (n node) Value() int {  return n.value }  func (n node) Next() List {  return n.next }  func New(v int) List {  return node{v, nil} }  func Prepend(l List, v int) List {  return node{v, l} } This is a far more elegant abstraction. The empty list is represented by the nil interface. We have only one implementation of that interface, for the nodes. We offer exported functions to create new lists - potentially from existing ones.\nNote that the methods actually have node as a receiver, not *node, as we often tend to do with structs. This fact makes this implementation a persistent linked list. None of the methods can modify the list. So after creation, the linked list will stay forever immutable. Even if you type-assert to get to the underlying data, that would only provide you with a copy of the data - the original would stay unmodified. The memory layout, however, is the same - the value gets put on the heap and you are only passing pointers to it around.\nThe beauty of this way to think about linked lists, is that it allows us to amend it after the fact. For example, say we notice that our program is slow, due to excessive cache-misses (as linked lists are not contiguous in memory). We can easily add a function, that packs a list:\ntype packed []int  func (p packed) Value() int {  return p[0] }  func (p packed) Next() List {  if len(p) == 0 {  return nil  }  return p[1:] }  func Pack(l List) List {  if l == nil {  return nil  }  var p packed  for ; l != nil; l = l.Next() {  p = append(p, l.Value())  }  return p } The cool thing about this is that we can mix and match the two: For example, we could prepend new elements and once the list gets too long, pack it and continue to prepend to the packed list. And since List is an interface, users can implement it themselves and use it with our existing implementation. So, for example, a user could build us a list that calculates fibonacci numbers:\ntype fib [2]int  func (l fib) Value() int {  return l[0] }  func (l fib) Next() List {  return fib{l[1], l[0]+l[1]} } and then use that with functions that take a List. Or they could have a lazily evaluated list:\ntype lazy struct {  o sync.Once  f func() (int, List)  v int  next List }  func (l *lazy) Value() int {  l.o.Do(func() { l.v, l.next = l.f() })  return l.v }  func (l *lazy) Next() List {  l.o.Do(func() { l.v, l.next = l.f() })  return l.next } Note that in this case the methods need to be on a pointer-receiver. This (technically) leaves the realm of persistent data-structures. While they motivated our interface-based abstraction and helped us come up with a safe implementation, we are not actually bound to them. If we later decide, that for performance reasons we want to add a mutable implementation, we can do so (of course, we still have to make sure that we maintain the safety of the original). And we can intermix the two, allowing us to only apply this optimization to part of our data structure.\nI find this a pretty helpful way to think about datastructures.\nAssociative lists Building on linked lists, we can build a map based on Association Lists. It\u0026rsquo;s a similar idea as before:\ntype Map interface {  Value(k interface{}) interface{}  Set(k, v interface{}) Map }  type empty struct{}  func (empty) Value(_ interface{}) interface{} {  return nil }  func (empty) Set(k, v interface{}) Map {  return pair{k, v, empty{}} }  func Make() Map {  return empty{} }  type pair struct {  k, v interface{}  parent Map }  func (p pair) Value(k interface{}) interface{} {  if k == p.k {  return p.v  }  return p.parent.Value(k) }  func (p pair) Set(k, v interface{}) Map {  return pair{k, v, p} } This time, we don\u0026rsquo;t represent an empty map as nil, but add a separate implementation of the interface for an empty map. That makes the implementation of Value cleaner, as it doesn\u0026rsquo;t have to check the parent map for nil \u0026ndash; but it requires users to call Make.\nThere is a problem with our Map, though: We cannot iterate over it. The interface does not give us access to any parent maps. We could use type-assertion, but that would preclude users from implementing their own. What if we added a method to the interface to support iteration?\ntype Map interface {  Value(k interface{}) interface{}   // Iterate calls f with all key-value pairs in the map.  Iterate(f func(k, v interface{})) }  func (empty) Iterate(func(k, v interface{})) { }  func (p pair) Iterate(f func(k, v interface{})) {  f(p.k, p.v)  p.parent.Iterate(f) } Unfortunately, this still doesn\u0026rsquo;t really work though: If we write multiple times to the same key, Iterate as implemented would call f with all key-value-pairs. This is likely not what we want.\nThe heart of the issue here, is the difference between the required interface and the provided interface. We can also see that with Set. Both of the implementations of that method look essentially the same and neither actually depends on the used type. We could instead provide Set as a function:\nfunc Set(m Map, k, v interface{}) Map {  return pair{k,v,m} } The lesson is, that some operations need support from the implementation, while other operations can be implemented without it. The provided interface is the set of operations we provide to the user, whereas the required interface is the set of operations that we rely on. We can split the two and get something like this:\n// Interface is the set of operations required to implement a persistent map. type Interface interface {  Value(k interface{}) interface{}  Iterate(func(k, v interface{})) }  type Map struct {  Interface }  func (m Map) Iterate(f func(k, v interface{})) {  seen := make(map[interface{}]bool)  m.Interface.Iterate(func(k, v interface{}) {  if !seen[k] {  f(k, v)  }  }) }  func (m Map) Set(k, v interface{}) Map {  return Map{pair{k, v, m.Interface}} } Using this, we could again implement a packed variant of Map:\ntype packed map[interface{}]interface{}  func (p packed) Value(k interface{}) interface{} {  return p[k] }  func (p packed) Iterate(f func(k, v interface{})) {  for k, v := range p {  f(k, v)  } }  func Pack(m Map) Map {  p := make(packed)  m.Iterate(func(k,v interface{}) {  p[k] = v  })  return m } Ropes A Rope is a data structure to store a string in a way that is efficiently editable. They are often used in editors, as it is too slow to copy the complete content on every insert operation. Editors also benefit from implementing them as persistent data structures, as that makes it very easy to implement multi-level undo: Just have a stack (or ringbuffer) of Ropes, representing the states the file was in after each edit. Given that they all share most of their structure, this is very efficient. Implementing ropes is what really bought me into the patterns I\u0026rsquo;m presenting here. Let\u0026rsquo;s see, how we could represent them.\n A Rope is a binary tree with strings as leafs. The represented string is what you get when you do a depth-first traversal and concatenate all the leafs. Every node in the tree also has a weight, which corresponds to the length of the string for leafs and the length of the left subtree for inner nodes. This allows easy recursive lookup of the ith character: If i is less than the weight of a node, we look into the left subtree, otherwise into the right. Let\u0026rsquo;s represent this:\ntype Base interface {  Index(i int) byte  Length() int }  type leaf string  func (l leaf) Index(i int) byte {  return l[i] }  func (l leaf) Length() int {  return len(l) }  type node struct {  left, right Base }  func (n node) Index(i int) byte {  if w := n.left.Length(); i \u0026gt;= w {  // The string represented by the right child starts at position w,  // so we subtract it when recursing to the right  return n.right.Index(i-w)  }  return n.left.Index(i) }  func (n node) Length() int {  return n.left.Length() + n.right.Length() }  type Rope struct {  Base }  func New(s string) Rope {  return Rope{leaf(s)} }  func (r Rope) Append(r2 Rope) Rope {  return Rope{node{r.Base, r2.Base}} } Note, how we did not actually add a Weight-method to our interface: Given that it\u0026rsquo;s only used by the traversal on inner nodes, we can just directly calculate it from its definition as the length of the left child tree. In practice, we might want to pre-calculate Length on creation, though, as it currently is a costly recursive operation.\nThe next operation we\u0026rsquo;d have to support, is splitting a Rope at an index. We can\u0026rsquo;t implement that with our current interface though, we need to add it:\ntype Base interface {  Index(i int) byte  Length() int  Split(i int) (left, right Base) }  func (l leaf) Split(i int) (Base, Base) {  return l[:i], l[i:] }  func (n node) Split(i int) (Base, Base) {  if w := n.left.Length(); i \u0026gt;= w {  left, right := n.right.Split(i-w)  return node{n.left, left}, right  }  left, right := n.left.Split(i)  return left, node{n.right, right} }  func (r Rope) Split(i int) (Rope, Rope) {  // Note that we return the wrapping struct, as opposed to Base.  // This is so users work with the provided interface, not the required one.  left, right := r.Split(i)  return Rope{left}, Rope{right} } I think this code is remarkably readable and easy to understand - and that is mostly due to the fact that we are reusing subtrees whenever we can. What\u0026rsquo;s more, given these operations we can implement the remaining three from the wikipedia article easily:\nfunc (r Rope) Insert(r2 Rope, i int) Rope {  left, right := r.Split(i)  return left.Append(r2).Append(right) }  func (r Rope) Delete(i, j int) Rope {  left, right := r.Split(j)  left, _ = left.Split(i)  return left.Append(right) }  func (r Rope) Slice(i, j int) Rope {  r, _ = r.Split(j)  _, r = r.Split(i)  return r } This provides us with a fully functioning Rope implementation. It doesn\u0026rsquo;t support everything we\u0026rsquo;d need to write an editor, but it\u0026rsquo;s a good start that was quick to write. It is also reasonably simple to extend with more functionality. For example, you could imagine having an implementation that can rebalance itself, when operations start taking too long. Or adding traversal, or random-access unicode support that is still backed by compact UTF-8. And I found it reasonably simple (though it required some usage of unsafe) to write an implementation of Base that used an mmaped file (thus you\u0026rsquo;d only need to keep the actual edits in RAM, the rest would be read directly from disk with the OS managing caching for you).\nClosing remarks None of these ideas are revolutionary (especially to functional programmers). But I find that considering if a datastructure I need can be implemented as a persistent/immutable one helps me to come up with clear abstractions that work well. And I also believe that Go\u0026rsquo;s interfaces provide a good way to express these abstractions - because they allow you to start with a simple, immutable implementation and then compose it with mutable ones - if and only if there are clear efficiency benefits. Lastly, I think there is an interesting idea here of how to substitute sum-types by interfaces - not in a direct manner, but instead by thinking about the common behavior you want to provide over the sum.\nI hope you find that this inspires you to think differently about these problems too.\n","permalink":"https://blog.merovius.de/posts/2018-02-25-persistent_datastructures_with_go/","summary":"I present a couple of patterns that help modelling persistent datastructures in Go. I also apply them to three examples.","title":"Persistent datastructures with Go"},{"content":"tl;dr: Error handling shouldn\u0026rsquo;t be about how to best propagate an error value, but how to make it destroy it (or make it irrelevant). To encourage myself to do that, I started removing errors from function returns wherever I found it at all feasible\nError handling in Go is a contentious and often criticized issue. There is no shortage on articles criticizing the approach taken, no shortage on articles giving advice on how to deal with it (or defending it) and also no shortage on proposals on how to improve it.\nDuring these discussion, I always feel there is something missing. The proposals for improvement usually deal with syntactical issues, how to avoid boilerplate. Then there is the other school of thought - where it\u0026rsquo;s not about syntax, but about how to best pass errors around. Dave Chaney wrote an often quoted blog post on the subject, where he lists all the ways error information can be mapped into the Go type system, why he considers them flawed and what he suggests instead. This school of thought regularly comes up with helper packages, to make wrapping or annotating errors easier. pkg/errors is very popular (and is grown out of the approach of above blog post) but upspin\u0026rsquo;s incarnation also gathered some attention.\nI am dissatisfied with both schools of thought. Overall, neither seems to explicitly address, what to me is the underlying question: What is error handling? In this post, I\u0026rsquo;m trying to describe how I interpret the term and why, to me, the existing approaches and discussions mostly miss the mark. Note, that I don\u0026rsquo;t claim this understanding to be universal - just how I would put into words my understanding of the topic.\n Let\u0026rsquo;s start with a maybe weird question: Why is the entry point into the program func main() and not func main() error? Personally, I start most of my programs writing\nfunc main() {  if err := run(); err != nil {  log.Fatal(err)  } }  func run() error {  // … } This allows me to use defer, pass on errors and all that good stuff. So, why doesn\u0026rsquo;t the language just do that for me?\nWe can find part of the answer in this old golang-nuts thread. It is about return codes, instead of an error, but the principle is the same. And the best answer - in my opinion - is this:\n I think the returned status is OS-specific, and so Go the language should not define its type (Maybe some OS can only report 8-bit result while some other OS support arbitrary string as program status, there is considerable differences between that; there might even be environment that don\u0026rsquo;t support returning status code or the concept of status code simply doesn\u0026rsquo;t exist)\nI imagine some Plan 9 users might be disagree with the signature of os.Exit().\n So, in essence: Not all implementations would necessarily be able to assign a reasonable meaning to a return code (or error) from main. For example, an embedded device likely couldn\u0026rsquo;t really do anything with it. It thus seems preferable to not couple the language to this decision which only really makes semantic sense on a limited subset of implementations. Instead, we provide mechanisms in the standard library to exit the program or take any other reasonable action and then let the developer decide, under what circumstances they want to exit the program and with what code. Being coupled to a decision in the standard library is better than being coupled in the language itself. And a developer who targets a platform where an exit code doesn\u0026rsquo;t make sense, can take a different action instead.\nOf course, this leaves the programmer with a problem: What to do with errors? We could write it to stderr, but fmt.Fprintf also returns an error, so what to do with that one? Above I used log.Fatal, which does not return an error. What happens if the underlying io.Writer fails to write, though? What does log do with the resulting error? The answer is, of course: It ignores any errors.\nThe point is, that passing on the error is not a solution. Eventually every program will return to main (or os.Exit or panic) and the buck stops there. It needs to get handled and the signature of main enforces that the only way to do that is via side-effects - and if they fail, you just have to deal with that one too.\n Let\u0026rsquo;s continue with a similar question, that has a similar answer, that occasionally comes up: Why doesn\u0026rsquo;t ServeHTTP return an error? Sooner or later, people face the question of what to do with errors in their HTTP Handlers. For example, what if you are writing out a JSON object and Marshal fails? In fact, a lot of HTTP frameworks out there will define their own handler-type, which differs from http.Handler in exactly that way. But if everyone wants to return an error from their handler, why doesn\u0026rsquo;t the interface just add that error return itself? Was that just an oversight?\nI\u0026rsquo;m strongly arguing that no, this was not an oversight, but the correct design decision. Because the HTTP Server package can not handle any errors. An HTTP server is supposed to stay running, every request demands a response. If ServeHTTP would return an error, the server would have to do something with it, but what to do is highly application-specific. You might respond that it should serve a 500 error code, but in 99% of cases, that is the wrong thing to do. Instead you should serve a more specific error code, so the client knows (for example) whether to retry or if the response is cacheable. http.Server could also just ignore the error and instead drop the request on the floor, but that\u0026rsquo;s even worse. Or it could propagate it up the stack. But as we determined, eventually it would have to reach main and the buck stops there. You probably don\u0026rsquo;t want your server to come down, every time a request contains an invalid parameter.\nSo, given that a) every request needs an answer and b) the right answer is highly application-specific, the translation from errors into status codes has to happen in application code. And just like main enforces you to handle any errors via side-effects by not allowing you to return an error, so does http force you to handle any errors via writing a response by not allowing you to return an error.1\nSo, what are you supposed to do, when json.Marshal fails? Well, that depends on our application. Increment a metric. Log the error. panic. Write out a 500. Ignore it and write a 200. Commit to the uncomfortable knowledge, that sometimes, you can\u0026rsquo;t just pass the decision on what to do with an error to someone else.\n These two examples distill, I think, pretty well, what I view as error handling: An error is handled, when you destroy the error value. In that parlance, log.Error handles any errors of the underlying writer by not returning them. Every program needs to handle any error in some way, because main can\u0026rsquo;t return anything and the values need to go somewhere. Any HTTP handler needs to actually handle errors, by translating them into HTTP responses.\nAnd in that parlance, packages like pkg/errors have little, really, to do with error handling - instead, they provides you with a strategy for the case where you are not handling your errors. In the same vein, proposals that address the repetitive checking of errors via extra syntax do not really simplify their handling at all - they just move it around a bit. I would term that error propagation, instead - no doubt important, but keep in mind, that an error that was handled, doesn\u0026rsquo;t need to be propagated at all. So to me, a good approach to error handling would be characterized by mostly obviating the need for convenient error propagation mechanisms.\nAnd to me, at least, it seems that we talk too little about how to handle errors, in the end.\n Does Go encourage explicit error handling? This is the phrasing very often used to justify the repetitive nature, but I tend to disagree. Compare, for example, Go\u0026rsquo;s approach to checked exceptions in Java: There, errors are propagated via exceptions. Every exception that could be thrown (theoretically) must be annotated in the method signature. Any exception that you handle, has to be mentioned in a try-catch-statement. And the compiler will refuse to compile a program which does not explicitly mention how exceptions are handled. This, to me, seems like the pinnacle of explicit error handling. Rust, too, requires this - it introduces a ? operator to signify propagating an error, but that, still, is an explicit annotation. And apart from that, you can\u0026rsquo;t use the return value of a function that might propagate an error, without explicitly handling that error first.\nIn Go, on the other hand, it is not only perfectly acceptable to ignore errors when it makes sense (for example, I will always ignore errors created from writing to a *bytes.Buffer), it is actually often the only sensible thing to do. It is fundamentally not only okay, but 99% of times correct to just completely ignore the error returned by fmt.Println. And while it makes sense to check the error returned from json.Marshal in your HTTP handler against *json.MarshalError (to panic/log/complain loudly, because your code is buggy), any other errors should 99% of the time just be ignored. And that\u0026rsquo;s fine.\nI believe that to say Go encourages explicit error handling, it would need some mechanism of checked exceptions, Result types, or a requirement to pass an errcheck like analysis in the compiler.\nI think it would be closer to say, that Go encourages local error handling. That is, the code that handles an error, is close to the code that produced it. Exceptions encourages the two to be separated: There are usually several or many lines of code in a single try-block, all of which share one catch-block and it is hard to tell which of the lines produced it. And very often, the actual error location is several stack frames deep. You could contrast this with Go, where the error return is immediately obvious from the code and if you have a line of error handling, it is usually immediately attached to the function call that produced it.\nHowever, that still seems to come short, in my view. After all, there is nothing to force you to do that. And in fact, one of the most often cited articles about Go error handling is often interpreted to encourage exactly that. Plus, a lot of people end up writing return err far too often, simply propagating the error to be handled elsewhere. And the proliferation of error-wrapping libraries happens in the same vein: What their proponents phrase as \u0026ldquo;adding context to the error value\u0026rdquo;, I interpret as \u0026ldquo;adding back some of the information as a crutch, that you removed when passing the error to non-local handling code\u0026rdquo;. Sadly, far too often, the error then ends up not being handled at all, as everyone just takes advantage of that crutch. This leaves the end-user with an error message that is essentially a poorly formatted, non-contiguous stacktrace.\nPersonally, I\u0026rsquo;d characterize Go\u0026rsquo;s approach like this: In Go, error handling is simply first-class code. By forcing you to use exactly the same control-flow mechanisms and treat errors like any other data, Go encourages you to code your error handling. Often that means a bunch of control flow to catch and recover from any errors where they occur. But that\u0026rsquo;s not \u0026ldquo;clutter\u0026rdquo;, just as it is not \u0026ldquo;clutter\u0026rdquo; to write if n \u0026lt; 1 { return 1 } when writing a Fibonacci function (to choose a trivial example). It is just code. And yes, sometimes that code might also store the error away or propagate it out-of-band to reduce repetition where it makes sense - like in above blog post. But focussing on the \u0026ldquo;happy path\u0026rdquo; is a bit of a distraction: Your users will definitely be more happy about those parts of the control flow that make the errors disappear or transform them into clean, actionable advise on how to solve the problem.\nSo, in my reading, the title of the Go blog post puts the emphasis in slightly the wrong place - and often, people take the wrong message from it, in my opinion. Not \u0026ldquo;errors are values\u0026rdquo;, but \u0026ldquo;error handling is code\u0026rdquo;.\n So, what would be my advise for handling errors? To be honest, I don\u0026rsquo;t know yet - and I\u0026rsquo;m probably in no place to lecture anyone anyway.\nPersonally, I\u0026rsquo;ve been trying for the last couple of months to take a page out of http.Handlers playbook and try, as much as possible, to completely avoid returning an error. Instead of thinking \u0026ldquo;I should return an error here, in case I ever do any operation that fails\u0026rdquo;, I instead think \u0026ldquo;is there any way at all I can get away with not returning an error here?\u0026rdquo;. It doesn\u0026rsquo;t always work and sometimes you do have to pass errors around or wrap them. But I am forcing myself to think very hard about handling my errors and it encourages a programming-style of isolating failing components. The constraint of not being able to return an error tends to make you creative in how to handle it.\n I think this particular pattern is fine, though, personally, I don\u0026rsquo;t really see the point. Anything that builds an appError needs to provide the complete response anyway, so you might as well just write it out directly. YMMV.\n  You might be tempted to suggest, that you could define an HTTPError, containing the necessary info. Indeed, that\u0026rsquo;s what the official Go blog does, so it can\u0026rsquo;t be bad? And indeed, that is what they do, but note that they do not actually return an error in the end - they return an appError, which contains the necessary information. Exactly because they don\u0026rsquo;t know how to deal with general errors. So they translate any errors into a domain specific type that carries the response. So, that is not the same as returning an error.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://blog.merovius.de/posts/2018-01-21-what_even_is_error_handling/","summary":"I philosophize about error handling, what it actually means and how to characterize Go\u0026rsquo;s approach to it.","title":"What even is error handling?"},{"content":"tl;dr: I come up with a couple of useless, but entertaining ways to generate entropy without relying on any packages.\nThis post is inspired by a comment on reddit, saying\n […]given the constraints of no imports and the function signature:\nfunc F(map[string]string) map[string]string { ... }\nF must use a deterministic algorithm, since it is a deterministic algorithm it can be represented in a finite state machine.\n Now, the point of this comment was to talk about how to then compile such a function into a deterministic finite state machine, but it got me thinking about a somewhat different question. If we disallow any imports and assume a standard (gc) Go implementation - how many ways can we find to create a non-deterministic function?\nSo, the challenge I set to myself was: Write a function func() string that a) can not refer to any qualified identifier (i.e. no imports) and b) is non-deterministic, that is, produces different outputs on each run. To start me off, I did add a couple of helpers, to accumulate entropy, generate random numbers from it and to format strings as hex, without any imports:\ntype rand uint32  func (r *rand) mix(v uint32) { \t*r = ((*r \u0026lt;\u0026lt; 5) + *r) + rand(v) }  func (r *rand) rand() uint32 { \tmx := rand(int32(*r)\u0026gt;\u0026gt;31) \u0026amp; 0xa8888eef \t*r = *r\u0026lt;\u0026lt;1 ^ mx \treturn uint32(*r) }  func hex(v uint32) string { \tvar b []byte \tfor v != 0 { \tif x := byte(v \u0026amp; 0xf); x \u0026lt; 10 { \tb = append(b, \u0026#39;0\u0026#39;+x) \t} else { \tb = append(b, \u0026#39;a\u0026#39;+x-10) \t} \tv \u0026gt;\u0026gt;= 4 \t} \treturn string(b) } Obviously, these could be inlined, but separating them allows us to reuse them for our different functions. Then I set about the actual task at hand.\nMethod 1: Map iteration In Go, the iteration order of maps is not specified:\n The iteration order over maps is not specified and is not guaranteed to be the same from one iteration to the next.\n But gc, the canonical Go implementation, actively randomizes the map iteration order to prevent programs from depending on it. We can use this, to receive some of entropy from the runtime, by creating a map and iterating over it:\nfunc MapIteration() string {  var r rand   m := make(map[uint32]bool)  for i := uint32(0); i \u0026lt; 100; i++ {  m[i] = true  }  for i := 0; i \u0026lt; 1000; i++ {  for k := range m {  r.mix(k)  break // the rest of the loop is deterministic  }  }  return hex(r.rand()) } We first create a map with a bunch of keys. We then iterate over it a bunch of times; each map iteration gives us a different start index, which we mix into our entropy pool.\nMethod 2: Select Go actually defines a way in which the runtime is giving us access to entropy directly:\n If one or more of the communications can proceed, a single one that can proceed is chosen via a uniform pseudo-random selection.\n So the spec guarantees that if we have multiple possible communications in a select, the case has to be chosen non-deterministically. We can, again, extract that non-determinism:\nfunc Select() string { \tvar r rand  \tch := make(chan bool) \tclose(ch) \tfor i := 0; i \u0026lt; 1000; i++ { \tselect { \tcase \u0026lt;-ch: \tr.mix(1) \tcase \u0026lt;-ch: \tr.mix(2) \t} \t} \treturn hex(r.rand()) } We create a channel and immediately close it. We then create a select-statement with two cases and depending on which was taken, we mix a different value into our entropy pool. The channel is closed, to guarantee that communication can always proceed. This way, we extract one bit of entropy per iteration.\nNote, that there is no racing or concurrency involved here: This is simple, single-threaded Go code. The randomness comes directly from the runtime. Thus, this should work in any compliant Go implementation. The playground, however, is not compliant with the spec in this regard, strictly speaking. It is deliberately deterministic.\nMethod 3: Race condition This method exploits the fact, that on a multi-core machine at least, the Go scheduler is non-deterministic. So, if we let two goroutines race to write a value to a channel, we can extract some entropy from which one wins this race:\nfunc RaceCondition() string { \tvar r rand  \tfor i := 0; i \u0026lt; 1000; i++ { \tch := make(chan uint32, 2) \tstart := make(chan bool) \tgo func() { \t\u0026lt;-start \tch \u0026lt;- 1 \t}() \tgo func() { \t\u0026lt;-start \tch \u0026lt;- 2 \t}() \tclose(start) \tr.mix(\u0026lt;-ch) \t}  \treturn hex(r.rand()) } The start channel is there to make sure that both goroutines become runnable concurrently. Otherwise, the first goroutine would be relatively likely to write the value before the second is even spawned.\nMethod 4: Allocation/data races Another thought I had, was to try to extract some entropy from the allocator or GC. The basic idea is, that the address of an allocated value might be non-deterministic - in particular, if we allocate a lot. We can then try use that as entropy.\nHowever, I could not make this work very well, for the simple reason that Go does not allow you to actually do anything with pointers - except dereferencing and comparing them for equality. So while you might get non-deterministic values, those values can\u0026rsquo;t be used to actually generate random numbers.\nI thought I might be able to somehow get a string or integer representation of some pointer without any imports. One way I considered was inducing a runtime-panic and recovering that, in the hope that the error string would contain a stacktrace or offending values. However, none of the error strings created by the runtime actually seem to contain any values that could be used here.\nI also tried a workaround to interpret the pointer as an integer, by exploiting race conditions to do unsafe operations:\nfunc DataRace() string { \tvar r rand  \tvar data *uintptr \tvar addr *uintptr  \tvar i, j, k interface{} \ti = (*uintptr)(nil) \tj = \u0026amp;data  \tdone := false \tgo func() { \tfor !done { \tk = i \tk = j \t} \t}() \tfor { \tif p, ok := k.(*uintptr); ok \u0026amp;\u0026amp; p != nil { \taddr = p \tdone = true \tbreak \t} \t}  \tdata = new(uintptr) \tr.mix(uint32(*addr)) \treturn hex(r.rand()) } It turns out, however, that at least this particular instance of a data race has been fixed since Russ Cox wrote that blog post. In Go 1.9, this code just loops endlessly. I tried it in Go 1.5, though, and it works there - but we don\u0026rsquo;t get a whole lot of entropy (addresses are not that random). With other methods, we could re-run the code to collect more entropy, but in this case, I believe the escape analysis gets into our way by stack-allocating the pointer, so it will be the same one on each run.\nI like this method, because it uses several obscure steps to work, but on the other hand, it\u0026rsquo;s the least reliable and it requires an old Go version.\nYour Methods? These are all the methods I could think of; but I\u0026rsquo;m sure I missed a couple. If you can think of any, feel free to let me know on Twitter, reddit or hackernews :) I also posted the code in a gist, so you can download and run it yourself, but keep in mind, that the last method busy-loops in newer Go versions.\n","permalink":"https://blog.merovius.de/posts/2018-01-15-generating_entropy_without_imports_in_go/","summary":"I come up with a couple of useless, but entertaining ways to generate entropy without relying on any packages.","title":"Generating entropy without imports in Go"},{"content":"tl;dr: I explain the mathematical background of a joke-explanation of monads. Contains lots of math and a hasty introduction to category theory.\nThere is a running gag in the programming community, that newcomers will often be confused by the concept of monads (which is how sequential computations are modeled in purely functional languages) and getting the explanation \u0026ldquo;it is simple, really: Monads are just monoids in the category of endofunctors\u0026rdquo;. This is not meant as an actual explanation, but rather to poke a bit of fun at the habit of functional programmers to give quite abstract and theoretical explanations at times, that are not all that helpful.\nHowever, given my background in mathematics, I decided that I wanted to actually approach Haskell from this point of view: I am interested in how it uses math to model programming and also to, after several years of doing mostly engineering focused programming work, flex my math muscles again\n as there is quite a bit of interesting math behind these concepts.  The quote is from a pretty popular book about category theory and is, in full:\n All told, a monad in \\(X\\) is just a monoid in the category of endofunctors of \\(X\\), with product \\(\\times\\) replaced by composition of endofunctors and unit set by the identity endofunctor.\n This, of course, is an explanation of the mathematical concept of monads, not meant for programmers. Most explanations of the quote that I found either assumed quite a bit of knowledge in Haskell or took a lot of liberties with the mathematical concepts (and relied a lot on \u0026ldquo;squinting\u0026rdquo;) or both. This write up is my attempt, to walk through all the concepts needed to explain monads as a mathematical concept and how it relates to Haskell - with as little squinting as possible.\nOf course, there are a couple of disclaimers, I should start with:\n This is not the best way to understand what monads are, if you are actually interested in using them to program. In fact, it is literally the worst way. I would recommend this intro, which takes a much more practical approach. This is not the best way to understand how category theory works, if you are actually interested in learning mathematics. In fact, it is literally the worst way. I would recommend the book the quote is from, it\u0026rsquo;s quite good (but assumes a math audience). I haven\u0026rsquo;t done mathematics in years. I also don\u0026rsquo;t know much Haskell either. So I might be getting a bunch of stuff wrong to varying degrees. I\u0026rsquo;m sure I will hear all about it :) Even if I would understand everything correctly, there are still a lot of details, mostly of technical nature, I had to omit, to keep this \u0026ldquo;short\u0026rdquo;. Not that it is short.  Originally, I intended this to be the ultimate explanation, which would teach Haskellers category theory, mathematicians Haskell and people who know neither both. Unsurprisingly, this is not what this is, at all. It ended up mostly a write up to assure myself that I understood the path myself. If anything, you can treat this as a kind of \u0026ldquo;reading companion\u0026rdquo;: If you want to understand this topic of the intersection between category theory and functional programming, this post can lead you through the correct terms to search for and give you a good idea what to focus on, in the respective Wikipedia articles.\nWith all that out of the way, let\u0026rsquo;s begin.\nCategories In mathematics, a category is (roughly) a collection of objects and a collection of arrows between them. There is not a lot of meaning behind these, but it will probably help you to think of objects as sets and arrows as mappings. Every arrow goes from an object (the domain) to an object (the codomain) and we write an arrow as \\(f:X\\to Y\\), where \\(f\\) is the name of the arrow, \\(X\\) is the domain and \\(Y\\) is the codomain. Just like with mappings, there can be many arrows between any given pair of objects - or there may be none.\nWe do need some restrictions: First, we require a specific identity arrow \\(\\mathrm{id}:X\\to X\\) attached to every object \\(X\\), which has \\(X\\) as both domain and codomain. Secondly, we require (some) arrows to be composable. That is if we have two arrows \\(f:X\\to Y,g:Y\\to Z\\) - so, whenever the domain of \\(g\\) is the codomain of \\(f\\) - there should also be a composed arrow1 \\(g\\circ f: X\\to Z\\), that shares the domain with \\(f\\) and the codomain with \\(g\\).\nFurthermore, the identity arrows must act as a unit for composition, that is, for every arrow \\(f\\) we require \\(\\mathrm{id}\\circ f = f = f \\circ\\mathrm{id}\\). We also require composition to be associative, that is \\((f\\circ g)\\circ h = f\\circ(g\\circ h)\\) (whenever all compositions exist)2.\nWhen we talk about a category, we often draw diagrams like this:\n \\[ \\require{AMScd} \\begin{CD} X @{f} Y \\\\ @V{g}VV @VV{p}V \\\\ Z @{q} W \\\\ \\end{CD} \\]  They show some of the objects and arrows from the category in a compact way. This particular diagram indicates that there are four objects and four arrows involved, with obvious domains and codomains. We only draw a subset of the objects and arrows, that is interesting for the point we are trying to make - for example, above diagram could also contain, of course, identity arrows and compositions \\(p\\circ f\\) and \\(q\\circ g\\)), but we didn\u0026rsquo;t draw them. In a square like this, we can take two paths from \\(X\\) to \\(W\\). If these paths are identical (that is, \\(p\\circ f = q\\circ g\\), we say that the square commutes. A commutative diagram is a diagram, in which any square commutes, that is, it does not matter which path we take from any object to another. Most of the time, when we draw a diagram, we intend it to be commutative.\nSo, to summarize, to define a mathematical category, we need to:\n Specify what our objects are Specify what our arrows are, where each arrow starts and ends at a certain object This collection of arrows need to include an arrow \\(\\mathrm{id}_X\\) for every object \\(X\\), which starts and ends at \\(X\\) And we need to be able to glue together arrows \\(f:X\\to Y\\) and \\(g:Y\\to Z\\) to an arrow \\(g\\circ f: X\\to Z\\)  In Haskell, we work on the category Hask, which consists of:\n The objects are types: Int is an object, String is an object but also Int | String, String -\u0026gt; Int and any other complicated type you can think of. The arrows are functions: f :: a -\u0026gt; b is a function taking an a as an input and returning a b and is represented by an arrow f, which has a as its domain and b as its codomain. So, for example, length :: String -\u0026gt; Int would start at the type String and end at Int. Haskell has a function id :: a -\u0026gt; a which gives us the identity arrow for any type a. We can compose functions with the operator (.) :: (b -\u0026gt; c) -\u0026gt; (a -\u0026gt; b) -\u0026gt; (a -\u0026gt; c). Note, that this follows the swapped notation of \\(\\circ\\), where the input type of the left function is the output type of the right function.  In general, category theory is concerned with the relationship between categories, whereas in functional programming, we usually only deal with this one category. This turns out to be both a blessing and a curse: It means that our object of study is much simpler, but it also means, that it is sometimes hard to see how to apply the general concepts to the limited environment of functional programming.\nMonoids Understanding categories puts us in the position to understand monoids. A monoid is the generalized structure underlying concepts like the natural numbers: We can add two natural numbers, but we can\u0026rsquo;t (in general) subtract them, as there are no negative numbers. We also have the number \\(0\\), which, when added to any number, does nothing - it acts as a unit for addition. And we also observe, that addition is associative, that is, when doing a bunch of additions, the order we do them in doesn\u0026rsquo;t matter.\nThe same properties also apply to other constructs. For example, if we take all maps from a given set to itself, they can be composed and that composition is associative and there is a unit element (the identity map).\nThis provides us with the following elements to define a monoid:\n A set \\(M\\) An operation \\(\\star\\colon M\\times M\\to M\\), which \u0026ldquo;adds\u0026rdquo; together two elements to make a new one We need a special unit element \\(u\\in M\\), which acts neutrally when added to any other element, that is \\(m\\star u=m=u\\star m\\) The operation needs to be associative, that is we always require \\(m\\star(n\\star k)=(m\\star n)\\star k\\)  There is another way to frame this, which is closer in line with category theory. If we take \\(1 := \\{0\\}\\) to be a 1-element set, we can see that the elements of \\(M\\) are in a one-to-one correspondence to functions \\(1\\to M\\): Every such function chooses an element of \\(M\\) (the image of \\(0\\)) and every element \\(m\\in M\\) fixes such a function, by using \\(f(0) := m\\). Thus, instead of saying \u0026ldquo;we need a special element of \\(M\\)\u0026rdquo;, we can also choose a special function \\(\\eta: 1\\to M\\). And instead of talking about an \u0026ldquo;operation\u0026rdquo;, we can talk about a function \\(\\mu: M\\times M\\to M\\). Which means, we can define a monoid via a commutative diagram like so:\n \\[ \\begin{CD} 1 \\\\ @V{\\eta}VV \\\\ M \\\\ \\end{CD} \\hspace{1em} \\begin{CD} M\\times M \\\\ @V{\\mu}VV \\\\ M \\\\ \\end{CD} \\hspace{1em} \\begin{CD} M\\times 1 @{\\mathrm{id}\\times\\eta} M\\times M @{\\mathrm{id}} M @{\\mu\\times\\mathrm{id}} M\\times M \\\\ @V{\\mathrm{id}\\times\\mu}VV @V{\\mu}VV \\\\ M\\times M @{\\mu} M \\\\ \\end{CD} \\]  \\(\\pi_1\\) and \\(\\pi_2\\) here, are the functions that project to the first or second component of a cross product respectively (that is \\(\\pi_1(a, b) := a, \\pi_2(a, b) := b\\)) and e.g. \\(\\mathrm{id}\\times\\eta\\) is the map that applies \\(\\mathrm{id}\\) to the first component of a cross-product and \\(\\eta\\) to the second: \\(\\mathrm{id}\\times\\eta(m, 0) = (m, \\eta(0))\\).\nThere are four sub-diagrams here:\n The first diagram just says, that we need an arrow \\(\\eta:1\\to M\\). This chooses a unit element for us. Likewise, the second diagram just says, that we need an arrow \\(\\mu:M\\times M\\to M\\). This is the operation. The third diagram tells us that the chosen by \\(\\eta\\) should be a unit for \\(\\mu\\). The commutativity of the left square tells us, that it should be right-neutral, that is \\[ \\forall m\\in M: m = \\pi_1(m, 0) = \\mu(\\mathrm{id}\\times\\eta(m, 0)) = \\mu(m, \\eta(0)) \\] and the commutativity of the right square tells us, that it should be left-neutral, that is \\[ \\forall m\\in M: m = \\pi_2(0,m) = \\mu(\\eta\\times\\mathrm{id}(0, m)) = \\mu(\\eta(0), m) \\]  Thus, the first diagram is saying that the element chosen by \\(\\eta\\) should act like a unit. For example, the left square says\n\\[\\pi_1(m,0) = \\mu((\\mathrm{id}\\times\\eta)(m,0)) = \\mu(m,\\eta(0))\\]\nNow, writing \\(\\mu(m,n) = m\\star n\\) and \\(\\eta(0) = u\\), this is equivalent to saying \\(m = u\\star m\\).\nThe second diagram is saying that \\(\\mu\\) should be associative: The top arrow combines the first two elements, the left arrow combines the second two. The right and bottom arrows then combine the result with the remaining element respectively, so commutativity of that square means the familiar \\(m\\star (n\\star k) = (m\\star n)\\star k\\).\nHaskell has the concept of a monoid too. While it\u0026rsquo;s not really relevant to the discussion, it might be enlightening to see, how it\u0026rsquo;s modeled. A monoid in Haskell is a type-class with two (required) methods:\nclass Monoid a where  mempty :: a  mappend :: a -\u0026gt; a -\u0026gt; a Now, this gives us the operation (mappend) and the unit (a), but where are the requirements of associativity and the unit acting neutrally? The Haskell type system is unable to codify these requirements, so they are instead given as a \u0026ldquo;law\u0026rdquo;, that is, any implementation of a monoid is supposed to have these properties, to be manually checked by the programmer:\n mappend mempty x = x (the unit is left-neutral) mappend x mempty = x (the unit is right-neutral) mappend x (mappend y z) = mappend (mappend x y) z (the operation is associative)  Functors I mentioned that category theory investigates the relationship between categories - but so far, everything we\u0026rsquo;ve seen only happens inside a single category. Functors are, how we relate categories to each other. Given two categories \\(\\mathcal{B}\\) and \\(\\mathcal{C}\\), a functor \\(F:\\mathcal{B}\\to \\mathcal{C}\\) assigns to every object \\(X\\) of \\(\\mathcal{B}\\), an object \\(F(X)\\) of \\(\\mathcal{C}\\). It also assigns to every arrow \\(f:X\\to Y\\) in \\(\\mathcal{B}\\) a corresponding arrow \\(F(f): F(X)\\to F(Y)\\) in \\(\\mathcal{C}\\)3. So, a functor transfers arrows from one category to another, preserving domain and codomain. To actually preserve the structure, we also need it to preserve the extra requirements of a category, identities and composition. So we need, in total:\n An object map, \\(F:O_\\mathcal{B} \\to O_\\mathcal{C}\\) An arrow map, \\(F:A_\\mathcal{B}\\to A_\\mathcal{C}\\), which preserves start and end object, that is the image of an arrow \\(X\\to Y\\) starts at \\(F(X)\\) and ends at \\(F(Y)\\) The arrow map has to preserve identities, that is \\(F(\\mathrm{id}_X) = \\mathrm{id}_{F(X)}\\) The arrow map has to preserve composition, that is \\(F(g\\circ f) = F(g)\\circ F(f)\\).  A trivial example of a functor is the identity functor (which we will call \\(I\\)), which assigns each object to itself and each arrow to itself - that is, it doesn\u0026rsquo;t change the category at all.\nA simple example is the construction of the free monoid, which maps from the category of sets to the category of monoids. The Free monoid \\(S^*\\) on a set \\(S\\) is the set of all finite length strings of elements of \\(S\\), with concatenation as the operation and the empty string as the unit. Our object map then assigns to each set \\(S\\) its free monoid \\(S^*\\). And our arrow map assigns to each function \\(f:S\\to T\\) the function \\(f^*:S^*\\to T^*\\), that applies \\(f\\) to each element of the input string.\nThere is an interesting side note here: Mathematicians love to abstract. Categories arose from the observation, that in many branches of mathematics we are researching some class of objects with some associated structure and those maps between them, that preserve this structure. It turns out that category theory is a branch of mathematics that is researching the objects of categories, with some associated structure (identity arrows and composition) and maps (functors) between them, that preserve that structure. So it seems obvious that we should be able to view categories as objects of a category, with functors as arrows. Functors can be composed (in the obvious way) and every category has an identity functor, that just maps every object and arrow to itself.\nNow, in Haskell, Functors are again a type class:\nclass Functor f where  fmap :: (a -\u0026gt; b) -\u0026gt; (f a -\u0026gt; f b) This looks like our arrow map: It assigns to each function g :: a -\u0026gt; b a function fmap g :: f a -\u0026gt; f b. The object map is implicit: When we write f a, we are referring to a new type, that depends on a - so we \u0026ldquo;map\u0026rdquo; a to f a 4.\nAgain, there are additional requirements the type system of Haskell can not capture. So we provide them as laws the programmer has to check manually:\n fmap id == id (preserves identities) fmap (f . g) == fmap f . fmap g (preserves composition)  There is one thing to note here: As mentioned, in Haskell we only really deal with one category, the category of types. That means that a functor always maps from the category of types to itself. In mathematics, we call such a functor, that maps a category to itself, an endofunctor. So we can tell, that in Haskell, every functor is automatically an endofunctor.\nNatural transformations We now understand categories and we understand functors. We also understand, that we can look at something like the category of categories. But the definition of a monad given to us talks about the category of endofunctors. So we seem to have to step up yet another level in the abstraction hierarchy and somehow build this category. As objects, we\u0026rsquo;d like to have endofunctors - and arrows will be natural transformations, which take one functor to another, while preserving its internal structure (the mapping of arrows). If that sounds complicated and abstract, that\u0026rsquo;s because it is.\nWe need two functors \\(F,G:\\mathcal{B}\\to \\mathcal{C}\\) of the same \u0026ldquo;kind\u0026rdquo; (that is, mapping to and from the same categories). A natural transformation \\(\\eta:F\\dot\\to G\\) assigns an arrow5 \\(\\eta_X: F(X)\\to G(X)\\) (called a component of \\(\\eta\\)) to every object in \\(\\mathcal{B}\\). So a component \\(\\eta_X\\) describes, how we can translate the action of \\(F\\) on \\(X\\) into the action of \\(G\\) on \\(X\\) - i.e. how to translate their object maps. We also have to talk about the translation of the arrow maps. For that, we observe that for any arrow \\(f:X\\to Y\\) in \\(\\mathcal{B}\\), we get four new arrows in \\(\\mathcal{C}\\):\n \\[ \\begin{CD} X \\\\ @V{f}VV \\\\ Y \\\\ \\end{CD} \\hspace{1em} \\begin{CD} F(X) @{\\eta_X} G(X) \\\\ @V{F(f)}VV @VV{G(f)}V \\\\ F(Y) @{\\eta_Y} G(Y) \\\\ \\end{CD} \\]  For a natural transformation, we require the resulting square to commute.\nSo, to recap: To create a natural transformation, we need\n Two functors \\(F,G:\\mathcal{B}\\to\\mathcal{C}\\) For every object \\(X\\) in \\(\\mathcal{B}\\), an arrow \\(\\eta_X: F(X)\\to G(X)\\) The components need to be compatible with the arrow maps of the functors: \\(\\eta_Y\\circ F(f) = G(f)\\circ \\eta_X\\).  In Haskell, we can define a natural transformation like so:\nclass (Functor f, Functor g) =\u0026gt; Transformation f g where  eta :: f a -\u0026gt; g a f and g are functors and a natural transformation from f to g provides a map f a -\u0026gt; g a for every type a. Again, the requirement of compatibility with the actions of the functors is not expressible as a type signature, but we can require it as a law:\n eta (fmap fn a) = fmap fn (eta a)  Monads This, finally, puts us in the position to define monads. Let\u0026rsquo;s look at our quote above:\n All told, a monad in \\(X\\) is just a monoid in the category of endofunctors of \\(X\\), with product \\(\\times\\) replaced by composition of endofunctors and unit set by the identity endofunctor.\n It should be clear, how we can compose endofunctors. But it is important, that this is a different view of these things than if we\u0026rsquo;d look at the category of categories - there, objects are categories and functors are arrows, while here, objects are functors and arrows are natural transformations. That shows, how composition of functors can take the role of the cross-product of sets: In a set-category, the cross product makes a new set out of two other set. In the category of endofunctors, composition makes a new endofunctor out of two other endofunctors.\nWhen we defined monoids diagrammatically, we also needed a cross product of mappings, that is, given a map \\(f:X_1\\to Y_1\\) and a map \\(g:X_2\\to Y_2\\), we needed the map \\(f\\times g: X_1\\times X_2\\to Y_1\\times Y_2\\), which operated on the individual constituents. If we want to replace the cross product with composition of endofunctors, we need an equivalent for natural transformations. That is, given two natural transformations \\(\\eta:F\\to G\\) and \\(\\epsilon:J\\to K\\), we want to construct a natural transformation \\(\\eta\\epsilon:J\\circ F\\to K\\circ G\\). This diagram illustrates how we get there (working on components):\n \\[ \\begin{CD} F(X) @{\\eta_X} G(X) @. \\\\ @V{J}VV @VV{J}V @. \\\\ J(F(X)) @{J(\\eta_X)} J(G(X)) @{\\epsilon_{G(X)}} K(G(X)) \\\\ \\end{CD} \\]  As we can see, we can build an arrow \\(\\epsilon_{G(X)}\\circ J(\\eta_X): J(F(X)) \\to K(G(X))\\), which we can use as the components of our natural transformation \\(\\eta\\epsilon:J\\circ F\\to K\\circ G\\). This construction is called the horizontal composition of natural transformations. We should verify that this is indeed a natural transformation - for now, let\u0026rsquo;s just accept that it follows from the naturality of \\(\\eta\\) and \\(\\epsilon\\).\nLastly, there is an obvious natural transformation taking a functor to itself; each component being just the identity arrow. We call that natural transformation \\(\\iota\\), staying with the convention of using Greek letters for natural transformations.\nWith this, we can redraw the diagram we used to define monoids above, the replacements indicated by the quotes:\n \\[ \\begin{CD} I \\\\ @V{\\eta}VV \\\\ M \\\\ \\end{CD} \\hspace{1em} \\begin{CD} M\\circ M \\\\ @V{\\mu}VV \\\\ M \\\\ \\end{CD} \\hspace{1em} \\begin{CD} M\\circ I @{\\iota\\ \\eta} M\\circ M @{\\iota} M @{\\mu\\ \\iota} M\\circ M \\\\ @V{\\iota\\ \\mu}VV @V{\\mu}VV \\\\ M\\circ M @{\\mu} M \\\\ \\end{CD} \\]  The vertical arrows in the middle diagram now simply apply the composition of functors, using that the identity functor is a unit.\nThese diagrams encode these conditions on our natural transformations6:\n \\(\\mu\\circ\\eta\\iota = \\mu = \\iota\\eta\\circ\\mu\\), that is \\(\\eta\\) serves as a unit \\(\\mu\\circ\\mu\\iota = \\mu\\circ\\iota\\mu\\), that is \\(\\mu\\) is associative  To recap, a monad, in category theory, is\n An endofunctor \\(M\\) A natural transformation \\(\\eta: I\\to M\\), which serves as an identity for horizontal composition. A natural transformation \\(\\mu: M\\circ M\\to M\\), which is associative in respect to horizontal composition.  Now, let\u0026rsquo;s see, how this maps to Haskell monads.\nFirst, what is the identity functor in Haskell? As we pointed out above, the object function of functors is implicit, when we write f a instead of a. As such, the identity functor is simply a - i.e. we map any type to itself. fmap of that functor would thus also just be the identity fmap :: (a -\u0026gt; a) -\u0026gt; (a -\u0026gt; a).\nSo, what would our natural transformation \\(\\eta\\) look like? As we said, a natural transformation between two functors is just a map f a -\u0026gt; g a. So (if we call our endofunctor m) the identity transformation of our monoid is eta :: a -\u0026gt; m a mapping the identity functor to m. We also need our monoidal operation, which should map m applied twice to m: mu :: m (m a) -\u0026gt; m a.\nNow, Haskellers write return instead of eta and write join instead of mu, giving us the type class7\nclass (Functor m) =\u0026gt; Monad where  return :: a -\u0026gt; m a  join :: m (m a) -\u0026gt; m a As a last note, it is worth pointing out that you usually won\u0026rsquo;t implement join, but instead a different function, called \u0026ldquo;monadic bind\u0026rdquo;:\n(\u0026gt;\u0026gt;=) :: m a -\u0026gt; (a -\u0026gt; m b) -\u0026gt; m b The reason is, that this more closely maps to what monads are actually used for in functional programming. But we can move between join and \u0026gt;\u0026gt;= via\n(\u0026gt;\u0026gt;=) :: m a -\u0026gt; (a -\u0026gt; m b) -\u0026gt; m b v \u0026gt;\u0026gt;= f = join ((fmap f) v)  join :: m (m a) -\u0026gt; m a join v = v \u0026gt;\u0026gt;= id Conclusion This certainly was a bit of a long ride. It took me much longer than anticipated both to understand all the steps necessary and to write them down. I hope you found it helpful and I hope I didn\u0026rsquo;t make too many, too glaring mistakes. If so (either), feel free to let me know on Twitter, reddit or Hacker News - but please remember to be kind :)\nI want to thank Tim Adler and mxf+ for proof-reading this absurdly long post and for making many helpful suggestions for improvements\n The fact that we are not doing that is a completely justified criticism, that is due to a historical accident - we write function application from right to left, that is we write \\(f(x)\\), for applying \\(f\\) to \\(x\\). Accordingly, we write \\(g(f(x))\\), when applying \\(g\\) to the result of applying \\(f\\) to \\(x\\). And we chose to have the composite-notation be consistent with that, instead of the arrow-notation.\nI chose to just eat the unfortunate confusion, as it turns out Haskell is doing exactly the same thing, so swapping things around would just increase the confusion.\nSorry.\nWhat was that? Oh, you thought Mathematicians where precise? Ha!\nThis is one of the things that was tripping me up for a while: I was trying to figure out, how I would map types to other types in Haskell or even talk about the object map. But the most useful answer is \u0026ldquo;you don\u0026rsquo;t\u0026rdquo;.\n   It is often confusing to people, that the way the arrows point in the notation and the order they are written seems to contradict each other: When writing \\(f:X\\to Y\\) and \\(g:Y\\to Z\\) you might reasonably expect their composite to work like \\(f\\circ g: X\\to Z\\), that is, you glue together the arrows in the order you are writing them.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Keep in mind that this is a different notion from the ones for monoids, which we come to a bit later: While the formulas seem the same and the identities look like a unit, the difference is that only certain arrows can be composed, not all. And that there are many identity arrows, not just one. However, if we would have only one object, it would have to be the domain and codomain of every arrow and there would be exactly one identity arrow. In that case, the notions would be the same and indeed, \u0026ldquo;a category with exactly one object\u0026rdquo; is yet another way to define monoids.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It is customary, to use the same name for the object and arrow map, even though that may seem confusing. A slight justification of that would be, that the object map is already given by the arrow map anyway: If \\(F\\) is the arrow map, we can define the object map as \\(X\\mapsto \\mathrm{dom}(F(\\mathrm{id}_X))\\). So, given that they are always occurring together and you can make one from the other, we tend to just drop the distinction and save some symbols.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n It is important to note, that this is not really a function. Functions operate on values of a given type. But here, we are operating on types and Haskell has no concept of a \u0026ldquo;type of types\u0026rdquo; built in that a function could operate on. There are constructs operating on types to construct new types, like data, type, newtype or even deriving. But they are special syntactical constructs that exist outside of the realm of functions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n An important note here, is that the \\(\\eta_X\\) are arrows. Where the object map of a functor is just a general association which could look anything we like, the components of a natural transformation need to preserve the internal structure of the category we are working in.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n You will often see these conditions written differently, namely written e.g. \\(\\mu M\\) instead of \\(\\mu\\iota\\). You can treat that as a notational shorthand, it really means the same thing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There is a technicality here, that Haskell also has an intermediate between functor and monad called \u0026ldquo;applicative\u0026rdquo;. As I understand it, this does not have a clear category theoretical analogue. I\u0026rsquo;m not sure why it exits, but I believe it has been added into the hierarchy after the fact.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://blog.merovius.de/posts/2018-01-08-monads-are-just-monoids/","summary":"I explain the mathematical background of a joke-explanation of monads. Contains lots of math and a hasty introduction to category theory","title":"Monads are just monoids in the category of endofunctors"},{"content":"I\u0026rsquo;m going to try to make an argument for being vegan, but, to be clear, it is not very likely to convince you to change your eating habits. It is not designed to - it is only supposed to change the way you think about it. I mention all of that, so you are aware that I don\u0026rsquo;t care what your conclusions are here. If you are reading this, you should do so out of a genuine interest of my motives and for the purpose of self-reflection - not to pick a fight with that vegan dude and really show him he\u0026rsquo;s wrong. I will not debate the content of this article with you. So, with that out of the way, here is a thought experiment:\n Say, we would live in a Star Trek like post-scarcity society. Energy is all but abundant and we figured out replicator-technology, that can instantly create anything we like out of it. You get offer the choice between two meals, one is a delicious steak dinner (or whatever), made in a traditional manner. The second is the same thing, but from a replicator. Both are indistinguishable, they taste the same, they have the same nutritional and chemical composition, they cost the same. They only differ in how they\u0026rsquo;re made.\n You might be trying to rules-lawyer this. You might be trying to make up an argument, for why the replicator-steak would have to be worse. Or that the cow would already be dead, so it wouldn\u0026rsquo;t matter. But that is obviously not the point of this thought experiment (and remember, you don\u0026rsquo;t have to convince anyone of being right, here). The point is, that I strongly believe that the vast majority of people would agree, that all things being equal, choosing the meal that no animal suffered for is the correct choice. And if you truly believe that it isn\u0026rsquo;t, if you can honestly say to yourself that it doesn\u0026rsquo;t matter: You won\u0026rsquo;t gain anything from the rest of this article. You are relieved and might now just as well stop reading.\nThe point I am trying to make, is that you probably already know all the reasons you should be vegan. It\u0026rsquo;s very likely that you already have an intuitive understanding of all the reasons in the \u0026ldquo;pro veganism\u0026rdquo; column of your pro/contra list. And it really shouldn\u0026rsquo;t be necessary to convince you it\u0026rsquo;s a good idea, in general.\nWhy then, do so few people actually choose to be vegan, if they are fully aware of all the reasons to do so? The obvious answer is: Because not all things are being equal. There is a \u0026ldquo;contra veganism\u0026rdquo; column and it\u0026rsquo;s filled with many good reasons not to. What reasons those are, is deeply individual. It might be due to health. Due to an appeal to nature. Convenience. Money. Availability. Taste. Or maybe just priorities: Other things seem more important and deserving of your energy. And that\u0026rsquo;s okay. We all have to make hundreds of decisions every day and weigh these kinds of questions. And sometimes we do things that we shouldn\u0026rsquo;t and we usually have good reasons to. And sometimes we compromise and don\u0026rsquo;t go all the way, but just do the best we feel able to and that\u0026rsquo;s fine too. Nobody has to be perfect all the time.\nThe reason, I\u0026rsquo;m writing this article anyway, is that there is a fundamental difference between the two questions \u0026ldquo;Why are you vegan?\u0026rdquo; and \u0026ldquo;Why are you not not vegan?\u0026rdquo;. When you ask me why I am vegan, you are making the conversation inherently about my values and you will usually end up attacking them - not because you disagree with them (you likely are not), but just because that\u0026rsquo;s the natural progression of this question. And to be absolutely clear: I don\u0026rsquo;t owe you a justification for my value system. I\u0026rsquo;m sorry if that sounds harsh, but the topic is mostly really annoying to me (as hard as that may be to believe at this point).\nA more sensible question, though, is to ask how to best mitigate the contra-column. If we agree that, fundamentally, the points in the pro-column are valid and shared reasons, we can proceed into the much more productive conversation about how much weight the downsides really have and how you might be able to reduce at least some of their impact. And, again to be clear: The outcome of that might very well be, that your reasons are completely valid, rational and that, applied to your personal situation, veganism wouldn\u0026rsquo;t be a good choice. (And to be also clear: I might not be in the mood to have this conversation either. But it\u0026rsquo;s much preferable).\nSo, what I wish people to take away from this is\n Stop asking why you should be vegan (or why I am), you more than likely already know. If you are really interested in making an informed choice, bring up your concerns instead, but also accept if I don\u0026rsquo;t want to talk about them at that particular time - it\u0026rsquo;s a lot of emotional labor, to give the same explanations repeatedly. It might not seem like a big deal to me, to ask these questions, but I\u0026rsquo;ve repeated most of my answers literally hundreds of times at this point and might prefer to just enjoy my food. Stop treating veganism as a preference and start treating it as a moral choice. There is a qualitative difference between someone who does not like Italian food and a vegan. This is interesting when choosing what or where to eat as a group: This is hard enough as it is and I at least usually try very hard to accommodate everyone and not be a burden. And I absolutely do not expect to be treated like I\u0026rsquo;m better for that. But if it actually would come down to a choice between a vegetarian restaurant or a steakhouse, just because you really like meat: Yes, I do absolutely expect us to avoid the steakhouse. (To be clear: In my experience, it rarely actually comes down to only those two choices. And there are good reasons to avoid vegetarian restaurants that are not based on preference which should be given ample weight too - e.g. someone I know has Coeliac disease, fructose intolerance and lactose intolerance and tends to have a very hard time eating non-meat things. In my experience, though, people who have needs and not just preferences tend to ironically be more open to compromise anyway, so it is less often a problem with them). Maybe think about your reasons for not being vegan and evaluate them seriously. To be clear, this is a stretch-goal and not the actual point of this article.  And if you want to, you can watch someone who does eat meat say essentially the same things here:\n  Thanks for reading, don\u0026rsquo;t @ me. ;)\n Reasons I\u0026rsquo;m not not vegan Now, the main point of this post is dedicated to the general question of \u0026ldquo;how I think about the topic and how I believe you should think about it too\u0026rdquo;. But I also want it to serve as a reference of my personal, specific thoughts driving my decision - so if you don\u0026rsquo;t know me well or are not interested in my personal reasons, this would be a good place to close the tab and do something else.\nI\u0026rsquo;m still writing this, because I hope this can be the last thing I ever have to write about this (yeah, lol). Because again, I don\u0026rsquo;t actually like discussing it, as unbelievable as that may seem. So here is, as a reference, why I am vegan (and I might add to/change this list over time, when my viewpoints evolve). Why, after ten-ish years of thinking \u0026ldquo;I should be vegan, but…\u0026rdquo;, I decided to make the switch - or at least give it a serious try. So, this is a list of reasons I gave to myself to justify not going vegan and why they stopped being convincing to me. Your mileage may vary.\nMeat/Cheese/Eggs/Bailey\u0026rsquo;s is awesome and I can\u0026rsquo;t imagine giving it up. For most of my life I didn\u0026rsquo;t think about vegetarianism or veganism at all. Eating meat was the default, so that\u0026rsquo;s what I did. When I did start to think about it, I convinced myself that I couldn\u0026rsquo;t give up meat, because most of my favorite foods where meat-based. However, a lot of people in my peer-group around that time (university) where vegetarian or vegan, so I naturally got into contact with a lot of good food that wasn\u0026rsquo;t naturally meat-based. So I started eating less and less meat - and the less meat I ate, the more I realized I didn\u0026rsquo;t really miss it that much, given how much good alternatives there are. Eventually I decided to become a \u0026ldquo;flexitarian\u0026rdquo;, which very quickly (in ~1-2 months) became \u0026ldquo;vegetarian\u0026rdquo;, when I realized that it didn\u0026rsquo;t actually bother me to not eat meat at all - and making that commitment meant less decisions, so it made things easier. With Cheese/Eggs/Bailey\u0026rsquo;s, I basically went through exactly the same transition, six or seven years later: \u0026ldquo;I can\u0026rsquo;t imagine giving them up\u0026rdquo; - \u0026ldquo;Actually, there are really good alternatives\u0026rdquo; - \u0026ldquo;Let\u0026rsquo;s just try reducing it and see how far I get\u0026rdquo; - \u0026ldquo;Meh, might as well just commit completely\u0026rdquo;.\nSo, to me, giving up animal products just turned out much easier, than expected, when I actually tried. And I\u0026rsquo;m not saying I don\u0026rsquo;t miss them, every once in a while, I will still look longingly at a steak or think fondly of my scrambled eggs. But for the most part, the alternatives are just as great (or at times better), so it isn\u0026rsquo;t as big a sacrifice as expected.\nBeing vegetarian/vegan is unhealthy. There is a bunch of research about this and for a while (especially before actually looking into the details) the health implications of veganism (vegetarianism not so much) did concern me. But, it turns out, this topic is pretty complicated. Nutrition research is very hard - and that manifests in the fact that for most of it, the statistical significance is usually low and the effect sizes usually small. Now, I\u0026rsquo;m not denying, that there are health downsides to a vegan diet. But even with the general mess that nutritional research is, it doesn\u0026rsquo;t seem very controversial that if you are concerned for your health, there are much more important factors to consider. If weighed against the health benefits of sleeping more, doing more sports, not sitting all day, stop recreational drug use, taking extensive vacations… (neither of which I seem to be willing to do, even though they would be easy enough), the relatively minor health effects of eating animal products (contrasted with a somewhat balanced vegan diet plus supplementation) just did not seem to be a relevant driving force for that decision and more of a rationalization.\nThat being said, from what I can gather so far, there is a general consensus that if a) you pay attention to having a somewhat balanced diet and b) are willing to supplement the nutrients you can\u0026rsquo;t actually get, the health impact of veganism is pretty low, if any. Personally, I am supplementing Vitamins B12 and D right now, which has very low quality of life impact - so I don\u0026rsquo;t consider that a significant downside. I also pay a little bit more attention to what I\u0026rsquo;m eating, which I consider a good thing.\nIf it turns out that I can not sustain a vegan diet, I will reconsider it, but for now, I don\u0026rsquo;t see any realistic danger of that happening.\nIt is cumbersome to know whether or not something is vegan. This is mostly true. As a vegetarian, this mostly revolved around looking for gelatin in the ingredients of sweets and asking in a restaurant whether \u0026ldquo;Lasagna\u0026rdquo; is made with meat or not. Being a vegan does involve a lot of scanning ingredients-lists of basically every product I buy. Though I\u0026rsquo;m positively surprised how many vendors are recently starting to choose to get their products certified - and not only brands you would expect to focus on that, but also, increasingly, all kinds of mainstream products.\nThat being said, there is an availability issue (especially around \u0026ldquo;may contain traces of…\u0026rdquo;, which is basically saying \u0026ldquo;if you are allergic, we can\u0026rsquo;t rule out cross-contamination of other things from the same factory\u0026rdquo;). I tend to be pragmatic about this: If I have the choice, I will buy the certifiably vegan option, otherwise I\u0026rsquo;m also fine with traces of animal products, personally. If I don\u0026rsquo;t know, I will go with my best guess and how I feel in the moment.\nThis is definitely the most true and heavy argument still on the contra-side for me, but being kind of pragmatic about it helps alleviate most of the pain.\nIt\u0026rsquo;s hypocritical to draw the line at X and not at Y. You can always be more rigorous and there are a lot of line-drawing questions that come up when thinking about vegetarianism/veganism. For the record, a lot of that is just plain nonsense, but there are some legitimate questions to be asked around whether or not insects count, for example, or certain shellfish, whether you would eat meat if it would be thrown away otherwise or would eat an egg, if the Hen laying it was living a happy, free life. In the end, the vast majority of things you can eat will involve some harm to the environment or animals and you won\u0026rsquo;t always know, so where do you draw the line?\nPersonally, I decided that definite harm is worse than potential harm and more harm is worse than less harm. \u0026ldquo;It is hypocritical to not eat meat/cheese/eggs but still kill a wasp entering your apartment\u0026rdquo; is about as convincing an argument to me as \u0026ldquo;it is hypocritical to eat meat/cheese/eggs but not also eat dogs/jellyfish/human\u0026rdquo;. The world isn\u0026rsquo;t black-and-white and it\u0026rsquo;s fine to choose a gray spot in the middle that makes you comfortable.\nEating out becomes a PITA. Yes. Going out and eating in a group is a PITA. Honestly, there are no two ways about it. I do have to actively make sure that a chosen food place has options for me and more often than not that does involve making special requests and/or making do with less great meals.\nIn general, this still works reasonably well. The cafeteria at work has great vegan options most of the time, Zurich has an amazing choice of great restaurants for vegans to offer, most other restaurants can accommodate too and even if not, I\u0026rsquo;m fine just eating a little thing and then later eat some more at home.\nThe main problem is working around the social issues associated with it - dealing with people who are unwilling to be accommodating, having to justify/discuss my choice or just exposing something about my person I might prefer to keep private. Basically, I wrote a whole thing about this.\nBut this is simply one of those downsides I chose to accept. Nobody said going vegan wouldn\u0026rsquo;t come with sacrifices.\nBeing vegan is expensive I am not sure this is true in general. I am relatively sure, that being vegetarian at least actually ended up saving me money as a student. But I can\u0026rsquo;t be completely certain, as the change also came with other changes in circumstances. My vegan diet is probably more expensive than my vegetarian one, mainly because it includes a lot more processed substitute products (\u0026ldquo;faux meat\u0026rdquo; and various plant milks, which are at least in Switzerland still significantly more expensive than the cow-based variants), but again, I didn\u0026rsquo;t actually run any numbers.\nI\u0026rsquo;m pretty sure it\u0026rsquo;s possible to have an affordable vegan diet, especially if limiting processed substitute products and not eating out so often. Luckily, this isn\u0026rsquo;t really a concern for me, right now, though. Food and Groceries is a relatively small proportion of my monthly expenses and as such, the impact this has on me is pretty limited either way.\nI convinced myself, that if I can afford spending money on all kinds of luxury items and electronic gadgets, I can probably afford spending a little more on food.\n","permalink":"https://blog.merovius.de/posts/2018-01-02-my-case-for-veganism/","summary":"I make a case for veganism and why you probably already agree with all the reasons you should be vegan. And how I wish people would reframe the topic, to make it less contentious, emotional and, frankly, annoying.","title":"My case for veganism"},{"content":"You get an E-Mail. It\u0026rsquo;s an invite to a team dinner. As you have only recently joined this team, it\u0026rsquo;s going to be your first. How exciting! You look forward to get to know your new coworkers better and socialize outside the office. They are nice and friendly people and you are sure it\u0026rsquo;s going to be a great evening. However, you also have a distinct feeling of anxiety and dread in you. Because you know, the first dinner with new people also means you are going to have the conversation again. You know it will come up, whether you want to or not. Because you had it a thousand times - because you are an Omnivore.\nYou quickly google the place your manager suggested. \u0026ldquo;Green\u0026rsquo;s Organic Foodery\u0026rdquo;. They don\u0026rsquo;t have a menu on their site, but the name alone makes clear, that meat probably isn\u0026rsquo;t their specialty, exactly. You consider asking for a change of restaurant, but quickly decide that you don\u0026rsquo;t want to get a reputation as a killjoy who forces their habits on everyone just yet. You figure they are going to have something with meat on their menu. And if not, you can always just grab a burger on your way home or throw a steak into a pan when you get back. You copy the event to your calendar and continue working.\nAt six, everyone gathers to go to dinner together. It\u0026rsquo;s not far, so you decide to just walk. On the way you get to talk to some of your team mates. You talk about Skiing, your home countries, your previous companies. You are having fun - they seem to be easy-going people, you are getting along well. It\u0026rsquo;s going to be an enjoyable night.\nYou arrive at the restaurant and get seated. The waiter arrives with the menu and picks up your orders for drinks. When they leave, everyone starts flipping through the menu. “You\u0026rsquo;ve got to try their Tofu stir-fry. It\u0026rsquo;s amazing”, Kevin tells you. You nod and smile and turn your attention to the booklet in your hand. You quickly take in the symbols decorating some of the items. “G” - safe to assume, these are the gluten-free options. There\u0026rsquo;s also an “O” on a bunch of them. Also familiar, but ambiguous - could be either “Omnivores\u0026quot; or “Ovo-lacto” (containing at least dairy products or eggs), you\u0026rsquo;ve seen both usages. There is no legend to help disambiguate and quickly flipping through the rest of the menu, you find no other symbols. Ovo-lacto, then. You are going to have to guess from the names and short descriptions alone, whether they also contain any meat. They have lasagna, marked with an “O”. Of course that\u0026rsquo;s probably just the cheese but they might make it with actual minced beef.\nThe waiter returns and takes your orders. “The lasagna - what kind is it?”, you ask. You are trying to avoid the O-word as long as you can. “It\u0026rsquo;s Lasagna alla Bolognese, house style”. Uh-oh. House style? “Is it made from cattle, or based on gluten proteins?” (you don\u0026rsquo;t care how awkward you have to phrase your question, you are not saying the magic trigger words!) “Uhm… I\u0026rsquo;m not sure. I can ask in the kitchen, if you\u0026rsquo;d like?” “That would be great, thanks”. They leave. Jen, from across the table, smiles at you - are you imagining it, or does it look slightly awkward? You know the next question. “Are you an Omnivore?” “I eat meat, yes”, you say, smiling politely. Frick. Just one meal, is all you wanted. But it had to come up at some point anyway, so fine. “Wow. I\u0026rsquo;m Ovo-lacto myself. But I couldn\u0026rsquo;t always eat meat, I think. Is it hard?” You notice that your respective seat neighbors have started to listen too. Ovo-lactos aren\u0026rsquo;t a rarity anymore (Onmivores a bit more so, but not that much), but the topic still seems interesting enough to catch attention. You\u0026rsquo;ve seen it before. What feels like a hundred thousand times. In fact, you have said exactly the same thing Jen just said, just a year or so ago. Before you just decided to go Omnivore.\n“Not really”, you start. “I found it not much harder than when I went Ovo-lacto. You have to get used to it, of course, and pay a little more attention, but usually you find something. Just like when you start eating cheese and eggs.” At that moment, the waiter returns. “I spoke to the chef and we can make the lasagna with beef, if you like”. “Yes please”, you hand the menu back to them with a smile. “I considered going Ovo-lacto”, Mike continues the conversation from the seat next to Jen, “but for now I just try to have some milk every once in a while. Like, in my coffee or cereal. It\u0026rsquo;s not that I don\u0026rsquo;t like it, there are really great dairy products. For example, this place in the city center makes this amazing yogurt. But having it every day just seems very hard“. “Sure”, you simply say. You know they mean well and you don\u0026rsquo;t want to offend them by not being interested; but you also heard these exact literal words at least two dozen times. And always with ample evidence in the room that it\u0026rsquo;s not actually that hard.\n“I don\u0026rsquo;t really see the point”, Roy interjects from the end of the table. You make a mental check-mark. “Shouldn\u0026rsquo;t people just eat what they want? I don\u0026rsquo;t get why suddenly we all have to like meat”. It doesn\u0026rsquo;t matter that no one suggested that. “I mean, I do think it\u0026rsquo;s cool if people eat meat”, someone whose name you don\u0026rsquo;t remember adds, “I sometimes think I should eat more eggs myself. But it\u0026rsquo;s just so annoying that you get these Omnivores who try to lecture you about how unhealthy it is to not eat meat or that humans are naturally predisposed to digest meat. I mean, you seem really relaxed about it”, they quickly add as assurance in your direction, “but you can\u0026rsquo;t deny that there are also these Omni-nazis”. You sip on your water, mentally counting backwards from 3. “You know how you find an Omnivore at a party?”, Kevin asks jokingly from your right. “Don\u0026rsquo;t worry, they will tell you”, Rick delivers the punchline for him. How original.\n”I totally get Omnivores. If they want to eat meat, that\u0026rsquo;s great. What I don\u0026rsquo;t understand is this weird trend of fake-salad. Like, people get a salad, but then they put french dressing on it, or bacon bits. I mean, if you want a salad, why not just simply have a salad?”. You know the stupidly obvious answer of course and you haven\u0026rsquo;t talked in a while, so you decide to steer the conversation into a more pleasant direction. “It\u0026rsquo;s simple, really. You like salad, right?” “Yeah, of course“ “So, if you like salad, but decide that you also want to eat dairy or meat - doesn\u0026rsquo;t it make sense to get as close to a pure salad as you can? While still staying with your conviction to eat meat? It\u0026rsquo;s a tradeoff, sure, but isn\u0026rsquo;t it better than no salad at all?” There\u0026rsquo;s a brief pause. You can tell that they haven\u0026rsquo;t considered that before. No one has. Which you find baffling. Every single time. “Hm. I guess I haven\u0026rsquo;t thought about it like that before. From that point of view it does kind of make sense. Anyway, I still prefer the real deal”. “That\u0026rsquo;s fine”, you say, smiling “I will continue to eat my salad with french dressing”.\nYour food arrives and the conversation continues for a bit, with the usual follow-up questions - do you eat pork too, or just beef? What about dogs? Would you consider eating monkey meat? Or Human? You explain that you don\u0026rsquo;t worry about the exact line, that you are not dogmatic about it and usually just decide based on convenience and what seems obvious (and in luckily, these questions don\u0026rsquo;t usually need an answer in practice anyway). Someone brings up how some of what\u0026rsquo;s labeled as milk actually is made from almonds, because it\u0026rsquo;s cheaper, so you can\u0026rsquo;t even be sure you actually get dairy. But slowly, person by person, the topic shifts back to work, hobbies and family. “How\u0026rsquo;s the lasagna?”, Jen asks. “Great”, you reply with a smile, because it is.\nOn your way home, you take stock. Overall, the evening went pretty well. You got along great with most of your coworkers and had long, fun conversations. The food ended up delicious, even if you wish they had just properly labeled their menu. You probably are going to have to nudge your team on future occasions, so you go out to Omnivore-friendlier places. But you are also pretty sure they are open to it. Who knows, you might even get them to go to a steak house at some point. You know you are inevitably going to have the conversation again, at some point - whether it will come up at another meal with your team or with a new person, who you eat with for the first time. This time, at least, it went reasonably well.\n This post is a work of fiction. ;) Names, characters, places and incidents either are products of the author\u0026rsquo;s imagination or are used fictitiously. Any resemblance to actual events or locales or persons, living or dead, is entirely coincidental.\nAlso, if we had \u0026ldquo;the conversation\u0026rdquo; before, you should know I still love you and don\u0026rsquo;t judge you :) It\u0026rsquo;s just that I had it a thousand times :)\n","permalink":"https://blog.merovius.de/posts/2017-10-20-a-day-in-the-life-of-an-omnivore/","summary":"I take a toung-in-cheek look at how I imagine the life of a meat eater to look. At least based on my own experience.","title":"A day in the life of an Omnivore"},{"content":"I often get into discussions with people, where the matter of strictness and expressiveness of a static type system comes up. The most common one, by far, is Go\u0026rsquo;s lack of generics and the resulting necessity to use interface{} in container types (the container-subpackages are obvious cases, but also context). When I express my view, that the lack of static type-safety for containers isn\u0026rsquo;t a problem, I am treated with condescending reactions ranging from disbelief to patronizing.\nI also often take the other side of the argument. This happens commonly, when talking to proponents of dynamically typed languages. In particular I got into debates of whether Python would be suitable for a certain use-case. When the lack of static type-safety is brought up, the proponents of Python defend it by pointing out that it now features optional type hints. Which they say make it possible, to reap the benefits of static typing even in a conventionally dynamically typed language.\nThis is an attempt to write my thoughts on both of these (though they are not in any way novel or creative) down more thoroughly. Discussions usually don\u0026rsquo;t provide the space for that. They are also often charged and parties are more interested in “winning the argument”, than finding consensus.\n I don\u0026rsquo;t think it\u0026rsquo;s particularly controversial, that static typing in general has advantages, even though actual data about those seems to be surprisingly hard to come by. I certainly believe that, it is why I use Go in the first place. There is a difference of opinion though, in how large and important those benefits are and how much of the behavior of a program must be statically checked to reap those benefits.\nTo understand this, we should first make explicit what the benefits of static type checking are. The most commonly mentioned one is to catch bugs as early in the development process as possible. If a piece of code I write already contains a rigorous proof of correctness in the form of types, just writing it down and compiling it gives me assurance that it will work as intended in all circumstances. At the other end of the spectrum, in a fully dynamic language I will need to write tests exercising all of my code to find bugs. Running tests takes time. Writing good tests that actually cover all intended behavior is hard. And as it\u0026rsquo;s in general impossible to cover all possible execution paths, there will always be the possibility of a rare edge-case that we didn\u0026rsquo;t think of testing to trigger a bug in production.\nSo, we can think of static typing as increasing the proportion of bug-free lines of code deployed to production. This is of course a simplification. In practice, we would still catch a lot of the bugs via more rigorous testing, QA, canarying and other practices. To a degree we can still subsume these in this simplification though. If we catch a buggy line of code in QA or the canary phase, we are going to roll it back. So in a sense, the proportion of code we wrote that makes it as bug-free into production will still go down. Thus:\nThis is usually the understanding, that the “more static typing is always better” argument is based on. Checking more behavior at compile time means less bugs in production means more satisfied customers and less being woken up at night by your pager. Everybody\u0026rsquo;s happy.\nWhy then is it, that we don\u0026rsquo;t all code in Idris, Agda or a similarly strict language? Sure, the graph above is suggestively drawn to taper off, but it\u0026rsquo;s still monotonically increasing. You\u0026rsquo;d think that this implies more is better. The answer, of course, is that static typing has a cost and that there is no free lunch.\nThe costs of static typing again come in many forms. It requires more upfront investment in thinking about the correct types. It increases compile times and thus the change-compile-test-repeat cycle. It makes for a steeper learning curve. And more often than we like to admit, the error messages a compiler will give us will decline in usefulness as the power of a type system increases. Again, we can oversimplify and subsume these effects in saying that it reduces our speed:\nThis is what we mean when we talk about dynamically typed languages being good for rapid prototyping. In the end, however, what we are usually interested in, is what I\u0026rsquo;d like to call velocity: The speed with which we can deploy new features to our users. We can model that as the speed with which we can roll out bug-free code. Graphically, that is expressed as the product of the previous two graphs:\nIn practice, the product of these two functions will have a maximum, a sweet spot of maximum velocity. Designing a type system for a programming language is, at least in part, about finding that sweet spot1.\nNow if we are to accept all of this, that opens up a different question: If we are indeed searching for that sweet spot, how do we explain the vast differences in strength of type systems that we use in practice? The answer of course is simple (and I\u0026rsquo;m sure many of you have already typed it up in an angry response). The curves I drew above are completely made up. Given how hard it is to do empirical research in this space and to actually quantify the measures I used here, it stands to reason that their shape is very much up for interpretation.\nA Python developer might very reasonably believe that optional type-annotations are more than enough to achieve most if not all the advantages of static typing. While a Haskell developer might be much better adapted to static typing and not be slowed down by it as much (or even at all). As a result, the perceived sweet spot can vary widely:\nWhat\u0026rsquo;s more, the importance of these factors might vary a lot too. If you are writing avionics code or are programming the control unit for a space craft, you probably want to be pretty darn sure that the code you are deploying is correct. On the other hand, if you are a Silicon Valley startup in your growth-phase, user acquisition will be of a very high priority and you get users by deploying features quicker than your competitors. We can model that, by weighing the factors differently:\nYour use case will determine the sweet spot you are looking for and thus the language you will choose. But a language is also designed with a set of use cases in mind and will set its own sweet spot according to that.\nI think when we talk about how strict a type system should be, we need to acknowledge these subjective factors. And it is fine to believe that your perception of one of those curves or how they should be weighted is closer to a hypothetical objective reality than another persons. But you should make that belief explicit and provide a justification of why your perception is more realistic. Don\u0026rsquo;t just assume that other people view them the same way and then be confused that they do not come to the same conclusions as you.\n Back to Go\u0026rsquo;s type system. In my opinion, Go manages to hit a good sweet spot (that is, its design agrees with my personal preferences on this). To me it seems that Go reaps probably upwards of 90% of the benefits you can get from static typing while still being not too impeding. And while I definitely agree static typing is beneficial, the marginal benefit of making user-defined containers type-safe simply seems pretty low (even if it\u0026rsquo;s positive). In the end, it would probably be less than 1% of Go code that would get this additional type-checking and it is probably pretty obvious code. And meanwhile, I perceive generics as a language feature pretty costly. So I find it hard to justify a large perceived cost with a small perceived benefit.\nNow, that is not to say I\u0026rsquo;m not open to be convinced. Just that simply saying “but more type-safety!” is only looking at one side of the equation and isn\u0026rsquo;t enough. You need to acknowledge that there is no free lunch and that this is a tradeoff. You need to accept that your perceptions of how big the benefit of adding static typing is, how much it costs and how important it is are all subjective. If you want to convince me that my perception of their benefit is wrong, the best way would be to provide specific instances of bugs or production crashes caused by a type-assertion on an interface{} taken out of a container. Or a refactoring you couldn\u0026rsquo;t make because of the lack of type-safety with a specific container. Ideally, this takes the form of an experience report, which I consider an excellent way to talk about engineered tradeoffs.\nOf course you can continue to roll your eyes whenever someone questions your perception of the value-curve of static typing. Or pretend that when I say the marginal benefit of type-safe containers is small, I am implying that the total benefit of static typing is small. It\u0026rsquo;s an effective debate-tactic, if your goal is to shut up your opposition. But not if your goal is to convince them and build consensus.\n   There is a generous and broad exception for research languages here. If the point of your design is to explore the possibility space of type-systems, matters of practicality can of course often be ignored.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://blog.merovius.de/posts/2017-09-12-diminishing-returns-of-static-typing/","summary":"When talking about static type systems, we often tend to focus on one side of the equation. I\u0026rsquo;m trying to make explicit how I view the question as a tradeoff and why I neither agree with “more is always better”, nor with “a little is enough”.","title":"Diminishing returns of static typing"},{"content":"tl;dr: \u0026ldquo;Some marbles, apparently, have a gender. And they seem to be overwhelmingly male.\u0026rdquo;\nA couple of days ago The MarbleLympics 2017 popped into my twitter stream. In case you are unaware (I certainly was): It\u0026rsquo;s a series of videos where a bunch of marbles participate in a made-up Olympics. They are split into teams that then participate in a series of \u0026ldquo;competitions\u0026rdquo; in a variety of different events. The whole event is professionally filmed, cut and overlaid both with fake noises from spectators and a well-made, engaging sports commentary. It is really fun to watch. I don\u0026rsquo;t know why, but I find it way more captivating than watching actual sports. I thoroughly recommend it.\nAround event 8 (high jump) though, I suddenly noticed that the commentator would occasionally not only personify but actually gender marbles. For most of the commentary he just refers to the teams as a whole with a generic \u0026ldquo;they\u0026rdquo;. But every once in a while - and especially often during the high-jump event - he would use a singular gendered pronoun. Also, that only really occurred to me when he referred to one of the marbles as \u0026ldquo;she\u0026rdquo;.\nThis instantly became one of those things that after noticing it, I couldn\u0026rsquo;t unnotice it. It\u0026rsquo;s not so much that it matters. But from then on, I couldn\u0026rsquo;t stop listening up every time a singular pronoun was used.\nWell, you know where this is going. Fully aware of how much of a waste of my time this is, I sat down and counted. More specifically, I downloaded the closed captions of all the videos and grepped through them for pronouns. I did double-check all findings and here is what I found: By my count, 20 distinct marbles are referred to by singular pronouns (yes. I noted their names to filter duplicates. Also I kind of hoped to find a genderfluid marble to be honest). Here is an alphabetized list of gendered marbles:\n Aqua (Oceanics) - Male Argent (Quicksilvers) - Male Clementin (O\u0026rsquo;Rangers) - Male Cocoa (Chocolatiers) - Male (in two events) Cosmo (Team Galactic) - Male Imar (Primaries) - Male Jump (Jungle Jumpers) - Male Leap (Jungle Jumpers) - Male Mandarin (O\u0026rsquo;Rangers) - Male (in two events) Mary (Team Primary) - Female Mercurial (Quicksilvers) - Male Mimo (Team Momo) - Male Momo Momo (Team Momo) - Male (in three events) Pinky Winky (Pinkies) - Male Rapidly (Savage Speeders) - Male Taffy (Jawbreakers) - Male Wespy (Midnight Wisps) - Male Whizzy (Savage Speeders) - Male Yellah (Mellow Yellow) - Male Yellup (Mellow Yellow) - Male  As you can see, the overwhelming majority of gendered marbles are men. There is exactly one exception: Mary. From what I can tell, that\u0026rsquo;s because it\u0026rsquo;s the only name that has clear gender associations. All the other names probably could go either way. And marbles obviously have no gender. They are as non-gendered an object as you could imagine. And yet there seems to be a default assumption that athletic marbles would be men.\nObviously this doesn\u0026rsquo;t matter. Obviously you can\u0026rsquo;t discriminate marbles. You can\u0026rsquo;t misgender them or hurt their feelings. Obviously the commentator didn\u0026rsquo;t sit down and made a list of all the marbles and assigned 95% of them a male gender - it\u0026rsquo;s clearly just an ad-hoc subconscious assignment. And to be absolutely clear: I do not try to fault the makers of these videos at all. They did nothing wrong. It\u0026rsquo;s a ludicrous expectation for them to sit down and make sure that they assign balanced genders to their marbles.\nBut I do find it an interesting observation. I do think it reflects an implicit, unconscious bias in a striking way. I also think it illustrates nicely that gender bias in language isn\u0026rsquo;t exclusive to languages like German, where all nouns are gendered (take note, German friends). Of course none of this is news. This kind of unconscious gender bias in language is well-researched and documented. It\u0026rsquo;s just that once you know about it, you can\u0026rsquo;t stop noticing the evidence for it popping up everywhere. Even with marbles.\nAnd all of that being said: Yes, I am also aware that all of this is slightly ridiculous.\n PS: In case the team behind the MarbleLympics are reading this: Really, thank you for the videos :) They are great.\n","permalink":"https://blog.merovius.de/posts/2017-09-05-gendered-marbles/","summary":"Some marbles, apparently, have a gender. And they seem to be overwhelmingly male.","title":"Gendered Marbles"},{"content":"tl;dr: I think context.Value solves the important use case of writing stateless - and thus scalable - abstractions. I believe dynamic scoping could provide the same benefits while solving most of the criticism of the current implementation. I thus try to steer the discussion away from the concrete implementation and towards the underlying problem.\nThis blog post is relatively long. I encourage you to skip sections you find boring\nUpdate: I wrote a new post, detailing how the type-safety concerns of context.Value in light of the new design for generics. You can check it out here\n Lately this blogpost has been discussed in several Go forums. It brings up several good arguments against the context-package:\n It requires every intermediate functions to include a context.Context even if they themselves do not use it. This introduces clutter into APIs and requires extensive plumbing. Additionally, ctx context.Context \u0026ldquo;stutters\u0026rdquo;. context.Value is not statically type-safe, requiring type-assertions. It does not allow you to express critical dependencies on context-contents statically. It\u0026rsquo;s susceptible to name collisions due to requiring a global namespace. It\u0026rsquo;s a map implemented as a linked list and thus inefficient.  However, I don\u0026rsquo;t think the post is doing a good enough job to discuss the problems context was designed to solve. It explicitly focuses on cancellation. Context.Value is discarded by simply stating that\n […] designing your APIs without ctx.Value in mind at all makes it always possible to come up with alternatives.\n I think this is not doing this question justice. To have a reasoned argument about context.Value there need to be consideration for both sides involved. No matter what your opinion on the current API is: The fact that seasoned, intelligent engineers felt the need - after significant thought - for Context.Value should already imply that the question deserves more attention.\nI\u0026rsquo;m going to try to describe my view on what kind of problems the context package tries to address, what alternatives currently exist and why I find them insufficient and I\u0026rsquo;m trying to describe an alternative design for a future evolution of the language. It would solve the same problems while avoiding some of the learned downsides of the context package. It is not meant as a specific proposal for Go 2 (I consider that way premature at this point) but just to show that a balanced view can show up alternatives in the design space and make it easier to consider all options.\n The problem context sets out to solve is one of abstracting a problem into independently executing units handled by different parts of a system. And how to scope data to one of these units in this scenario. It\u0026rsquo;s hard to clearly define the abstraction I am talking about. So I\u0026rsquo;m instead going to give some examples.\n When you build a scalable web service you will probably have a stateless frontend server that does things like authentication, verification and parsing for you. This allows you to scale up the external interface effortlessly and thus also gracefully fall back if the load increases past what the backends can handle. By treating requests as independent from each other you can load-balance them freely between your frontends. Microservices split a large application into small individual pieces that each process individual requests, each potentially branching out into more requests to other services. The requests will usually be independent, making it easy to scale individual microservices up and down based on demand, to load-balance between instances and to solve problems in transparent proxies. Functions as a Service goes one step further: You write single stateless functions that transform data and the platform will make them scale and execute efficiently. Even CSP, the concurrency model built into Go, can be viewed through that lens. The programmer expresses her problem as individually executing \u0026ldquo;processes\u0026rdquo; and the runtime will execute them efficiently. Functional Programming as a paradigm calls this \u0026ldquo;purity\u0026rdquo;. The concept that a functions result may only depend on its input parameters means not much more than the absence of shared state and independent execution. The design of a Request Oriented Collector for Go plays exactly into the same assumptions and ideas.  The idea in all these cases is to increase scaling (whether distributed among machines, between threads or just in code) by reducing shared state while maintaining shared usage of resources.\nGo takes a measured approach to this. It doesn\u0026rsquo;t go as far as some functional programming languages to forbid or discourage mutable state. It allows sharing memory between threads and synchronizing with mutexes instead of relying purely on channels. But it also definitely tries to be a (if not the) language to write modern, scalable services in. As such, it needs to be a good language to write this kind of stateless services. It needs to be able to make requests the level of isolation instead of the process. At least to a degree.\n(Side note: This seems to play into the statement of the author of above article, who claims that context is mainly useful for server authors. I disagree though. The general abstraction happens on many levels. E.g. a click in a GUI counts just as much as a \u0026ldquo;request\u0026rdquo; for this abstraction as an HTTP request)\nThis brings with it the requirement of being able to store some data on a request-level. A simple example for this would be authentication in an RPC framework. Different requests will have different capabilities. If a request originates from an administrator it should have higher privileges than if it originates from an unauthenticated user. This is fundamentally request scoped data. Not process, service or application scoped. And the RPC framework should treat this data as opaque. It is application specific not only how that data looks en détail but also what kinds of data it requires.\nJust like an HTTP proxy or framework should not need to know about request parameters or headers it doesn\u0026rsquo;t consume, an RPC framework shouldn\u0026rsquo;t know about request scoped data the application needs.\n Let\u0026rsquo;s try to look at specific ways this problem is (or could be) solved without involving context. As an example, let\u0026rsquo;s look at the problem of writing an HTTP middleware. We want to be able to wrap an http.Handler (or a variation thereof) in a way that allows the wrapper to attach data to a request.\nTo get static type-safety we could try to add some type to our handlers. We could have a type containing all the data we want to keep request scoped and pass that through our handlers:\ntype Data struct { \tUsername string \tLog *log.Logger \t// … }  func HandleA(d Data, res http.ResponseWriter, req *http.Request) { \t// … \td.Username = \u0026#34;admin\u0026#34; \tHandleB(d, req, res) \t// … }  func HandleB(d Data, res http.ResponseWriter, req *http.Request) { \t// … } However, this would prevent us from writing reusable Middleware. Any such middleware would need to make it possible to wrap HandleA. But as it\u0026rsquo;s supposed to be reusable, it can\u0026rsquo;t know the type of the Data parameter. You could make the Data parameter an interface{} and require type-assertion. But that wouldn\u0026rsquo;t allow the middleware to inject its own data. You might think that interface type-assertions could solve this, but they have their own set of problems. In the end, this approach won\u0026rsquo;t bring you actual additional type safety.\nWe could store our state keyed by requests. For example, an authentication middleware could do\ntype Authenticator struct { \tmu sync.Mutex \tusers map[*http.Request]string \twrapped http.Handler }  func (a *Authenticator) ServeHTTP(res http.ResponseWriter, req *http.Request) { \t// … \ta.mu.Lock() \ta.users[req] = \u0026#34;admin\u0026#34; \ta.mu.Unlock() \tdefer func() { \ta.mu.Lock() \tdelete(a.users, req) \ta.mu.Unlock() \t}() \ta.wrapped.ServeHTTP(res, req) }  func (a *Authenticator) Username(req *http.Request) string { \ta.mu.Lock() \tdefer a.mu.Unlock() \treturn a.users[req] } This has some advantages over context:\n It is more type-safe. While we still can\u0026rsquo;t express a requirement on an authenticated user statically, we can express a requirement on an Authenticator It\u0026rsquo;s not susceptible to name-collisions anymore.  However, we bought this with shared mutable state and the associated lock contention. It can also break in subtle ways, if one of the intermediate handlers decides to create a new Request - as http.StripPrefix is going to do soon.\nLastly, we might consider to store this data in the *http.Request itself, for example by adding it as a stringified URL parameter. This too has several downsides, though. In fact it checks almost every single item from our list of downsides of context.Context. The exception is being a linked list. But even that advantage we buy with a lack of thread safety. If that request is passed to a handler in a different goroutine we get into trouble.\n(Side note: All of this also gives us a good idea of why the context package is implemented as a linked list. It allows all the data stored in it to be read-only and thus inherently thread-safe. There will never be any lock-contention around the shared state saved in a context.Context, because there will never be any need for locks)\nSo we see that it is really hard (if not impossible) to solve this problem of having data attached to requests in independently executing handlers while also doing significantly better than with context.Value. Whether you believe this a problem worth solving or not is debatable. But if you want to get this kind of scalable abstraction you will have to rely on something like context.Value.\n No matter whether you are now convinced of the usefulness of context.Value or still doubtful: The disadvantages can clearly not be ignored in either case. But we can try to find a way to improve on it. To eliminate some of the disadvantages while still keeping its useful attributes.\nOne way to do that (in Go 2) would be to introduce dynamically scoped variables. Semantically, each dynamically scoped variable represents a separate stack. Every time you change its value the new one is pushed to the stack. It is pop\u0026rsquo;ed off again after your function returns. For example:\n// Let\u0026#39;s make up syntax! Only a tiny bit, though. dyn x = 23  func Foo() { \tfmt.Println(\u0026#34;Foo:\u0026#34;, x) }  func Bar() { \tfmt.Println(\u0026#34;Bar:\u0026#34;, x) \tx = 42 \tfmt.Println(\u0026#34;Bar:\u0026#34;, x) \tBaz() \tfmt.Println(\u0026#34;Bar:\u0026#34;, x) }  func Baz() { \tfmt.Println(\u0026#34;Baz:\u0026#34;, x) \tx = 1337 \tfmt.Println(\u0026#34;Baz:\u0026#34;, x) }  func main() { \tfmt.Println(\u0026#34;main:\u0026#34;, x) \tFoo() \tBar() \tBaz() \tfmt.Println(\u0026#34;main:\u0026#34;, x) }  // Output: main: 23 Foo: 23 Bar: 23 Bar: 42 Baz: 42 Baz: 1337 Bar: 42 Baz: 23 Baz: 1337 main: 23 There are several notes about what I would imagine the semantics to be here.\n I would only allow dyn-declarations at package scope. Given that there is no way to refer to a local identifier of a different function, that seems logical. A newly spawned goroutine would inherit the dynamic values of its parent function. If we implement them (like context.Context) via linked lists, the shared data will be read-only. The head-pointer would need to be stored in some kind of goroutine-local storage. Thus, writes only ever modify this local storage (and the global heap), so wouldn\u0026rsquo;t need to be synchronized specifically. The dynamic scoping would be independent of the package the variable is declared in. That is, if foo.A modifies a dynamic bar.X, then that modification is visible to all subsequent callees of foo.A, whether they are in bar or not. Dynamically scoped variables would likely not be addressable. Otherwise we\u0026rsquo;d loose concurrency safety and the clear \u0026ldquo;down-stack\u0026rdquo; semantics of dynamic scoping. It would still be possible to declare dyn x *int though and thus get mutable state to pass on. The compiler would allocate the necessary storage for the stacks, initialized to their initializers and emit the necessary instructions to push and pop values on writes and returns. To account for panics and early returns, a mechanism like defer would be needed. There is some confusing overlap with package-scoped variables in this design. Most notably, from seeing foo.X = Y you wouldn\u0026rsquo;t be able to tell whether foo.X is dynamically scoped or not. Personally, I would address that by removing package-scoped variables from the language. They could still be emulated by declaring a dynamically-scoped pointer and never modifying it. Its pointee is then a shared variable. But most usages of package-scoped variables would probably just use dynamically scoped variables.  It is instructive to compare this design against the list of disadvantages identified for context.\n API clutter would be removed, as request-scoped data would now be part of the language without needing explicit passing. Dynamically scoped variables are statically type-safe. Every dyn declaration has an unambiguous type. It would still not be possible to express critical dependencies on dynamically scoped variables but they also couldn\u0026rsquo;t be absent. At worst they\u0026rsquo;ll have their zero value. Name collision is eliminated. Identifiers are, just like variable names, properly scoped. While a naive implementation would still use linked lists, they wouldn\u0026rsquo;t be inefficient. Every dyn declaration gets its own list and only the head-pointer ever needs to be operated on. The design is still \u0026ldquo;magic\u0026rdquo; to a degree. But that \u0026ldquo;magic\u0026rdquo; is problem-inherent (at least if I understand the criticism correctly). The magic is exactly the possibility to pass values transparently through API boundaries.  Lastly, I\u0026rsquo;d like to mention cancellation. While the author of above post dedicates most of it to cancellation, I have so far mostly ignored it. That\u0026rsquo;s because I believe cancellation to be trivially implementable on top of a good context.Value implementation. For example:\n// $GOROOT/src/done package done  // C is closed when the current execution context (e.g. request) should be // cancelled. dyn C \u0026lt;-chan struct{}  // CancelFunc returns a channel that gets closed, when C is closed or cancel is // called. func CancelFunc() (c \u0026lt;-chan struct, cancel func()) { \t// Note: We can\u0026#39;t modify C here, because it is dynamically scoped, which is \t// why we return a new channel that the caller should store. \tch := make(chan struct)  \tvar o sync.Once \tcancel = func() { o.Do(close(ch)) } \tif C != nil { \tgo func() { \t\u0026lt;-C \tcancel() \t}() \t} \treturn ch, cancel }  // $GOPATH/example.com/foo package foo  func Foo() { \tvar cancel func() \tdone.C, cancel = done.CancelFunc() \tdefer cancel() \t// Do things } This cancellation mechanism would now be usable from any library that wants it without needing any explicit support in its API. This would also make it easy to add cancellation capabilities retroactively.\n Whether you like this design or not, it demonstrates that we shouldn\u0026rsquo;t rush to calling for the removal of context. Removing it is only one possible solution to its downsides.\nIf the removal of context.Context actually comes up, the question we should ask is \u0026ldquo;do we want a canonical way to manage request-scoped values and at what cost\u0026rdquo;. Only then should we ask what the best implementation of this would be or whether to remove the current one.\n","permalink":"https://blog.merovius.de/posts/2017-08-14-why-context-value-matters-and-how-to-improve-it/","summary":"In light of recent discussions about its removal, I try to discuss what kinds of problems context.Value tries to solve. I then try to describe a design which would address most (but not all) of the criticism surrounding it.","title":"Why context.Value matters and how to improve it"},{"content":"This is intended as an Experience Report about logging in Go. There are many like it but this one is mine.\nI have been trying for a while now to find (or build) a logging API in Go that fills my needs. There are several things that make this hard to get \u0026ldquo;right\u0026rdquo; though. This is my attempt to describe them coherently in one place.\nWhen I say \u0026ldquo;logging\u0026rdquo;, I mean informational text messages for human consumption used to debug a specific problem. There is an idea currently gaining traction in the Go community called \u0026ldquo;structured logging\u0026rdquo;. logrus is a popular package that implements this idea. If you haven\u0026rsquo;t heard of it, you might want to skim its README. And while I definitely agree that log-messages should contain some structural information that is useful for later filtering (like the current time or a request ID), I believe the idea as often advocated is somewhat misguided and conflates different use cases that are better addressed otherwise. For example, if you are tempted to add a structured field to your log containing an HTTP response code to alert on too many errors, you probably want to use metrics and timeseries instead. If you want to follow a field through a variety of systems, you probably want to annotate a trace. If you want analytics like calculating daily active users or what used user-agents are used how often, you probably want what I like to call request annotations, as these are properties of a request, not of a log-line. If you exclude all these use cases, there isn\u0026rsquo;t a lot left for structured logging to address.\nThe logs I am talking about is to give a user or the operator of a software more insight into what is going on under the covers. The default assumption is, that they are not looked at until something goes wrong: Be it a test failing, an alerting system notifying of an issue or a bug report being investigated or a CLI not doing what the user expected. As such it is important that they are verbose to a certain degree. As an operator, I don\u0026rsquo;t want to find out that I can\u0026rsquo;t troubleshoot a problem because someone did not log a critical piece of information. An API that requires (or encourages) me to only log structured data will ultimately only discourage me from logging at all. In the end, some form of log.Debugf(\u0026quot;Error reading foo: %v\u0026quot;, err) is the perfect API for my use case. Any structured information needed to make this call practically useful should be part of the setup phase of whatever log is.\nThe next somewhat contentious question is whether or not the API should support log levels (and if so, which). My personal short answer is \u0026ldquo;yes and the log levels should be Error, Info and Debug\u0026rdquo;. I could try and justify these specific choices but I don\u0026rsquo;t think that really helps; chalk it up to personal preference if you like. I believe having some variation on the verbosity of logs is very important. A CLI should be quiet by default but be able to tell the user more specifically where things went wrong on request. A service should be debuggable in depth, but unconditionally logging verbosely would have in unacceptable latency impact in production and too heavy storage costs. There need to be some logs by default though, to get quick insights during an emergency or in retrospect. So, those three levels seem fine to me.\nLastly what I need from a logging API, is the possibility to set up verbosity and log sinks both horizontally and vertically. What I mean by that is that software is usually build in layers. They could be individual microservices, Go packages or types. Requests will then traverse these layers vertically, possibly branching out and interleaved to various degrees.\nDepending on what and how I am debugging, it makes sense to increase the log verbosity of a particular layer (say I narrowed down the problem to shared state in a particular handler and want to see what happens to that state during multiple requests) or for a particular request (say, I narrowed down a problem to \u0026ldquo;requests which have header FOO set to BAR\u0026rdquo; and want to follow one of them to get a detailed view of what it does). Same with logging sinks, for example, a request initiated by a test should get logged to its *testing.T with maximum verbosity, so that I get a detailed context about it if and only if the test fails to immediately start debugging. These settings should be possible during runtime without a restart. If I am debugging a production issue, I don\u0026rsquo;t want to change a command line flag and restart the service.\nLet\u0026rsquo;s try to implement such an API.\nWe can first narrow down the design space a bit, because we want to use testing.T as a logging sink. A T has several methods that would suit our needs well, most notably Logf. This suggest an interface for logging sinks that looks somewhat like this:\ntype Logger interface { \tLogf(format string, v ...interface{}) }  type simpleLogger struct { \tw io.Writer }  func (l simpleLogger) Logf(format string, v ...interface{}) { \tfmt.Fprintf(l.w, format, v...) }  func NewLogger(w io.Writer) Logger { \treturn simpleLogger{w} } This has the additional advantage, that we can add easily implement a Discard-sink, that has minimal overhead (not even the allocations of formatting the message):\ntype Discard struct{}  func (Discard) Logf(format string, v ...interface{}) {} The next step is to get leveled logging. The easiest way to achieve this is probably\ntype Logs struct { \tDebug Logger \tInfo Logger \tError Logger }  func DiscardAll() Logs { \treturn Logs{ \tDebug: Discard{}, \tInfo: Discard{}, \tError: Discard{}, \t} } By putting a struct like this (or its constituent fields) as members of a handler, type or package, we can get the horizontal configurability we are interested in.\nTo get vertical configurability we can use context.Value - as much as it\u0026rsquo;s frowned upon by some, it is the canonical way to get request-scoped behavior/data in Go. So, let\u0026rsquo;s add this to our API:\ntype ctxLogs struct{}  func WithLogs(ctx context.Context, l Logs) context.Context { \treturn context.WithValue(ctx, ctxLogs{}, l) }  func GetLogs(ctx context.Context, def Logs) Logs { \t// If no Logs are in the context, we default to its zero-value, \t// by using the ,ok version of a type-assertion and throwing away \t// the ok. \tl, _ := ctx.Value(ctxLogs{}).(Logs) \tif l.Debug == nil { \tl.Debug = def.Debug \t} \tif l.Info == nil { \tl.Info = def.Info \t} \tif l.Error == nil { \tl.Error = def.Error \t} \treturn l } So far, this is a sane, simple and easy to use logging API. For example:\ntype App struct { \tL log.Logs }  func (a *App) ServeHTTP(res http.ResponseWriter, req *http.Request) { \tl := log.GetLogs(req.Context(), a.L) \tl.Debug.Logf(\u0026#34;%s %s\u0026#34;, req.Method, req.URL.Path) \t// ... } The issue with this API, however, is that it is completely inflexible, if we want to preserve useful information like the file and line number of the caller. Say, I want to implement the equivalent of io.MultiWriter. For example, I want to write logs both to os.Stderr and to a file and to a network log service.\nI might try to implement that via\nfunc MultiLogger(ls ...Logger) Logger { \treturn multiLog{ls} }  type multiLog struct { \tloggers []Logger }  func (m *multiLog) Logf(format string, v ...interface{}) { \tfor _, l := range m.loggers { \tm.Logf(format, v...) \t} } However, now the caller of Logf of the individual loggers will be the line in (*multiLog).Logf, not the line of its caller. Thus, caller information will be useless. There are two APIs currently existing in the stdlib to work around this:\n (testing.T).Helper (from Go 1.9) lets you mark a frame as a test-helper. When the caller-information is then added to the log-output, all frames marked as a helper is skipped. So, theoretically, we could add a Helper method to our Logger interface and require that to be called in each wrapper. However, Helper itself uses the same caller-information. So all wrappers must call the Helper method of the underlying *testing.T, without any wrapping methods. Even embedding doesn\u0026rsquo;t help, as the Go compiler creates an implicit wrapper for that. (log.Logger).Output lets you specify a number of call-frames to skip. We could add a similar method to our log sink interface. And wrapping loggers would then need to increment the passed in number, when calling a wrapped sink. It\u0026rsquo;s possible to do this, but it wouldn\u0026rsquo;t help with test-logs.  This is a very similar problem to the ones I wrote about last week. For now, I am using the technique I described as Extraction Methods. That is, the modified API is now this:\n// Logger is a logging sink. type Logger interface { \t// Logf logs a text message with the given format and values to the sink. \tLogf(format string, v ...interface{})  \t// Helpers returns a list of Helpers to call into from all helper methods, \t// when wrapping this Logger. This is used to skip frames of logging \t// helpers when determining caller information. \tHelpers() []Helper }  type Helper interface { \t// Helper marks the current frame as a helper method. It is then skipped \t// when determining caller information during logging. \tHelper() }  // Callers can be used as a Helper for log sinks who want to log caller // information. An empty Callers is valid and ready for use. type Callers struct { \t// ... }  // Helper marks the calling method as a helper. When using Callers in a // Logger, you should also call this to mark your methods as helpers. func (*Callers) Helper() { \t// ... }  type Caller struct { \tName string \tFile string \tLine int }  // Caller can be used to determine the caller of Logf in a Logger, skipping all // frames marked via Helper. func (*Callers) Caller() Caller { \t// ... }  // TestingT is a subset of the methods of *testing.T, so that this package // doesn\u0026#39;t need to import testing. type TestingT interface { \tLogf(format string, v ...interface{}) \tHelper() }  // Testing returns a Logger that logs to t. Log lines are discarded, if the // test succeeds. func Testing(t TestingT) Logger { \treturn testLogger{t} }  type testLogger struct { \tt TestingT }  func (l testLogger) Logf(format string, v ...interface{}) { \tl.t.Helper() \tl.t.Logf(format, v...) }  func (l testLogger) Helpers() []Helper { \treturn []Helper{l.t} }  // New returns a logger writing to w, prepending caller-information. func New(w io.Writer) Logger { \treturn simple{w, new(Callers)} }  type simple struct { \tw io.Writer \tc *Callers }  func (l *simple) Logf(format string, v ...interface{}) { \tl.c.Helper() \tc := l.c.Caller() \tfmt.Fprintf(l.w, \u0026#34;%s:%d: \u0026#34; + format, append([]interface{}{c.File, c.Line}, v...)...) }  func (l *simple) Helpers() []Helper { \treturn []Helper{l.c} }  // Discard discards all logs. func Discard() Logger { \treturn discard{} }  type discard struct{}  func (Discard) Logf(format string, v ...interface{}) { }  func (Discard) Helpers() []Helper { \treturn nil }  // MultiLogger duplicates all Logf-calls to a list of loggers. func MultiLogger(ls ...Logger) Logger { \tvar m multiLogger \tfor _, l := range ls { \tm.helpers = append(m.helpers, l.Helpers()...) \t} \tm.loggers = ls \treturn m }  type multiLogger struct { \tloggers []Logger \thelpers []Helper }  func (m multiLogger) Logf(format string, v ...interface{}) { \tfor _, h := range m.helpers { \th.Helper() \t} \tfor _, l := range m.loggers { \tl.Logf(format, v...) \t} }  func (m multiLogger) Helpers() []Helper { \treturn m.helpers } It\u0026rsquo;s a kind of clunky API and I have no idea about the performance implications of all the Helper-code. But it does work, so it is, what I ended up with for now. Notably, it puts the implementation complexity into the implementers of Logger, in favor of making the actual consumers of them as simple as possible.\n","permalink":"https://blog.merovius.de/posts/2017-08-06-what-i-want-from-a-logging-api/","summary":"Logging in Go is a notoriously lacking topic in the standard library. There are 3rd-party libraries trying to work around this. I\u0026rsquo;m trying to explain, why I find them still lacking","title":"What I want from a logging API"},{"content":"tl;dr: I take a look at the pattern of optional interfaces in Go: what they are used for, why they are bad and what we can do about it.\nNote: I wrote most of this article on Wednesday, with the intention to finish and publish it on the weekend. While I was sleeping, Jack Lindamood published this post, which talks about much of the same problems. This was the exact moment I saw that post :) I decided, to publish this anyway; it contains, in my opinion, enough additional content, to be worth it. But I do encourage to (also?) read his post.\nWhat are optional interfaces? Optional interfaces are interfaces which can optionally be extended by implementing some other interface. A good example is http.Flusher (and similar), which is optionally implemented by an http.ResponseWriter. If a request comes in via HTTP/2, the ResponseWriter will implement this interface to support HTTP/2 Server Push. But as not all requests will be over HTTP/2, this isn\u0026rsquo;t part of the normal ResponseWriter interface and instead provided via an optional interface that needs to be type-asserted at runtime.\nIn general, whenever some piece of code is doing a type-assertion with an interface type (that is, use an expression v.(T), where T is an interface type), it is very likely offering an optional interface.\nA far from exhaustive list of where the optional interface pattern is used (to roughly illustrate the scope of the pattern):\n io net/http database/sql/driver go/types Dave Chaney\u0026rsquo;s errors package  What are people using them for? There are multiple reasons to use optional interfaces. Let\u0026rsquo;s find examples for them. Note that this list neither claims to be exhaustive (there are probably use cases I don\u0026rsquo;t know about) nor disjunct (in some cases, optional interfaces will carry more than one of these use cases). But I think it\u0026rsquo;s a good rough partition to discuss.\nPassing behavior through API boundaries This is the case of ResponseWriter and its optional interfaces. The API, in this case, is the http.Handler interface that users of the package implement and that the package accepts. As features like HTTP/2 Push or connection hijacking are not available to all connections, this interface needs to use the lowest common denominator between all possible behaviors. So, if more features need to be supported, we must somehow be able to pass this optional behavior through the http.Handler interface.\nEnabling optional optimizations/features io.Copy serves as a good example of this. The required interfaces for it to work are just io.Reader and io.Writer. But it can be made more efficient, if the passed values also implement io.WriterTo or io.ReaderFrom, respectively. For example, a bytes.Reader implements WriteTo. This means, you need less copying if the source of an io.Copy is a bytes.Reader. Compare these two (somewhat naive) implementations:\nfunc Copy(w io.Writer, r io.Reader) (n int64, err error) { \tbuf := make([]byte, 4096) \tfor { \trn, rerr := r.Read(buf) \twn, werr := w.Write(buf[:rn]) \tn += int64(wn) \tif rerr == io.EOF { \treturn n, nil \t} \tif rerr != nil { \treturn n, rerr \t} \tif werr != nil { \treturn n, werr \t} \t} }  func CopyTo(w io.Writer, r io.WriterTo) (n int64, err error) { \treturn r.WriteTo(w) }  type Reader []byte  func (r *Reader) Read(b []byte) (n int, err error) { \tn = copy(b, *r) \t*r = (*r)[n:] \tif n == 0 { \terr = io.EOF \t} \treturn n, err }  func (r *Reader) WriteTo(w io.Writer) (int64, error) { \tn, err := w.Write(*r) \t*r = (*r)[n:] \treturn int64(n), err } Copy needs to first allocate a buffer, then copy all the data from the *Reader to that buffer, then pass it to the Writer. CopyTo, on the other hand, can directly pass the byte-slice to the Writer, saving an allocation and a copy.\nSome of that cost can be amortized, but in general, its existence is a forced consequence of the API. By using optional interfaces, io.Copy can use the more efficient method, if supported, and fall back to the slow method, if not.\nBackwards compatible API changes When database/sql upgraded to use context, it needed help from the drivers to actually implement cancellation and the like. So it needed to add contexts to the methods of driver.Conn. But it can\u0026rsquo;t just do that change; it would be a backwards incompatible API change, violating the Go1 compatibility guarantee. It also can\u0026rsquo;t add a new method to the interface to be used, as there are third-party implementations for drivers, which would be broken as they don\u0026rsquo;t implement the new method.\nSo it instead resorted to deprecate the old methods and instead encourage driver implementers to add optional methods including the context.\nWhy are they bad? There are several problems with using optional interfaces. Some of them have workarounds (see below), but all of them have drawbacks on their own.\nThey violate static type safety In a lot of cases, the consumer of an optional interface can\u0026rsquo;t really treat it as optional. For example, http.Hijacker is usually used to support WebSockets. A handler for WebSockets will, in general, not be able to do anything useful, when called with a ResponseWriter that does not implement Hijacker. Even when it correctly does a comma-ok type assertion to check for it, it can\u0026rsquo;t do anything but serve an error in that case.\nThe http.Hijacker type conveys the necessity of hijacking a connection, but since it is provided as an optional interface, there is no possibility to require this type statically. In that way, optional interfaces hide static type information.\nThey remove a lot of the power of interfaces Go\u0026rsquo;s interfaces are really powerful by being very small; in general, the advice is to only add one method, maybe a small handful. This advice enables easy and powerful composition. io.Reader and io.Writer have a myriad of implementations inside and outside of the standard library. This makes it really easy to, say, read uncompressed data from a compressed network connection, while streaming it to a file and hashing it at the same time to write to some content-addressed blob storage.\nNow, this composition will, in general, destroy any optional interfaces of those values. Say, we have an HTTP middleware to log requests. It wants to wrap an http.Handler and log the requests method, path, response code and duration (or, equivalently, collect them as metrics to export). This is, in principle, easy to do:\ntype logResponder struct { \thttp.ResponseWriter \tcode int \tset bool }  func (rw *logResponder) WriteHeader(code int) { \trw.code = code \trw.set = bool \trw.ResponseWriter.WriteHeader(code) }  func LogRequests(h http.Handler) http.Handler { \treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { \tlr := \u0026amp;logResponder{ResponseWriter: w} \tm, p, start := r.Method, r.Path, time.Now() \tdefer func() { \tlog.Printf(\u0026#34;%s %s -\u0026gt; %d (%v)\u0026#34;, m, p, lr.code, time.Now().Sub(start)) \t}() \th(lr, r) \t}) } But *logResponder will now only support the methods declared by http.ResponseWriter, even if the wrapped ResponseWriter also supports some of the optional interfaces. That is because method sets of a type are determined at compile time.\nThus, by using this middleware, the wrapped handler is suddenly unable to use websockets, or HTTP/2 server push or any of the other use cases of optional interfaces. Even worse: this deficiency will only be discovered at runtime.\nOptimistically adding the optional interface\u0026rsquo;s methods and type-asserting the underlying ResponseWriter at runtime doesn\u0026rsquo;t work either: handlers would incorrectly conclude the optional interface is always present. If the underlying ResponseWriter does not support adding at the underlying connection there just is no useful way to implement http.Hijacker.\nThere is one way around this, which is to dynamically check the wrapped interface and create a type with the correct method set, e.g.:\nfunc Wrap(wrap, with http.ResponseWriter) http.ResponseWriter { \tvar ( \tflusher http.Flusher \tpusher http.Pusher \t// ... \t) \tflusher, _ = wrap.(http.Flusher) \tpusher, _ = wrap.(http.Pusher) \t// ... \tif flusher == nil \u0026amp;\u0026amp; pusher == nil { \treturn with \t} \tif flusher == nil \u0026amp;\u0026amp; pusher != nil { \treturn struct{ \thttp.ResponseWriter \thttp.Pusher \t}{with, pusher} \t} \tif flusher != nil \u0026amp;\u0026amp; pusher == nil { \treturn struct{ \thttp.ResponseWriter \thttp.Flusher \t}{with, flusher} \t} \treturn struct{ \thttp.ResponseWriter \thttp.Flusher \thttp.Pusher \t}{with, flusher, pusher} } This has two major drawbacks:\n Both code-size and running time of this will increase exponentially with the number of optional interfaces you have to support (even if you generate the code). You need to know every single optional interface that might be used. While supporting everything in net/http is certainly tenable, there might be other optional interfaces, defined by some framework unbeknownst to you. If you don\u0026rsquo;t know about it, you can\u0026rsquo;t wrap it.  What can we use instead? My general advice is, to avoid optional interfaces as much as possible. There are alternatives, though they also are not entirely satisfying.\nContext.Value context was added after most of the optional interfaces where already defined, but its Value method was meant exactly for this kind of thing: to pass optional behavior past API boundaries. This will still not solve the static type safety issue of optional interfaces, but it does mean you can easily wrap them.\nFor example, net/http could instead do\nvar ctxFlusher = ctxKey(\u0026#34;flusher\u0026#34;)  func GetFlusher(ctx context.Context) (f Flusher, ok bool) { \tf, ok = ctx.Value(ctxFlusher).(Flusher) \treturn f, ok } This would enable you to do\nfunc ServeHTTP(w http.ResponseWriter, r *http.Request) { \tf, ok := http.GetFlusher(r.Context()) \tif ok { \tf.Flush() \t} } If now a middleware wants to wrap ResponseWriter, that\u0026rsquo;s not a problem, as it will not touch the Context. If a middleware wants to add some other optional behavior, it can do so easily:\ntype Frobnicator interface{ \tFrobnicate() }  var ctxFrobnicator = ctxKey(\u0026#34;frobnicator\u0026#34;)  func GetFrobnicator(ctx context.Context) (f Frobnicator, ok bool) { \tf, ok = ctx.Value(ctxFrobnicator).(Frobnicator) \treturn f, ok } As contexts form a linked list of key-value-pairs, this will interact nicely with whatever optional behavior is already defined.\nThere are good reasons to frown upon the usage of Context.Value; but they apply just as much to optional interfaces.\nExtraction methods If you know an interface type that is probable to be wrapped and also has optional interfaces associated it is possible to enforce the possibility of dynamic extension in the optional type. So, e.g.:\npackage http  type ResponseWriter interface { \t// Methods… }  type ResponseWriterWrapper interface { \tResponseWriter  \tWrappedResponseWriter() ResponseWriter }  // GetFlusher returns an http.Flusher, if res wraps one. // Otherwise, it returns nil. func GetFlusher(res ResponseWriter) Flusher { \tif f, ok := res.(Flusher); ok { \treturn f \t} \tif w, ok := res.(ResponseWriterWrapper); ok { \treturn GetFlusher(w.WrappedResponseWriter()) \t} \treturn nil }  package main  type logger struct { \tres ResponseWriter \treq *http.Request \tlog *log.Logger \tstart time.Time }  func (l *logger) WriteHeader(code int) { \td := time.Now().Since(l.start) \tl.log.Write(\u0026#34;%s %s -\u0026gt; %d (%v)\u0026#34;,\tl.req.Method, l.req.Path, code, d) \tl.res.WriteHeader(code) }  func (l *logger) WrappedResponseWriter() http.ResponseWriter { \treturn l.res }  func LogRequests(h http.Handler, l *log.Logger) http.Hander { \treturn http.HandlerFunc(res http.ResponseWriter, req *http.Request) { \tres = \u0026amp;logger{ \tres: res, \treq: req, \tlog: l, \tstart: time.Now(), \t} \th.ServeHTTP(res, req) \t} }  func ServeHTTP(res http.ResponseWriter, req *http.Request) { \tif f := http.GetFlusher(res); f != nil { \tf.Flush() \t} } This still doesn\u0026rsquo;t address the static typing issue and explicit dependencies, but at least it enables you to wrap the interface conveniently.\nNote, that this is conceptually similar to the errors package, which calls the wrapper-method \u0026ldquo;Cause\u0026rdquo;. This package also shows an issue with this pattern; it only works if all wrappers use it. That\u0026rsquo;s why I think it\u0026rsquo;s important for the wrapping interface to live in the same package as the wrapped interface; it provides an authoritative way to do that wrapping, preventing fragmentation.\nProvide statically typed APIs net/http could provide alternative APIs for optional interfaces that explicitly include them. For example:\ntype Hijacker interface { \tResponseWriter \tHijack() (net.Conn, *bufio.ReadWriter, error) }  type HijackHandler interface{ \tServeHijacker(w Hijacker, r *http.Request) }  func HandleHijacker(pattern string, h HijackHandler) { \t// ... } For some use cases, this provides a good way to side-step the issue of unsafe types. Especially if you can come up with a limited set of scenarios that would rely on the optional behavior, putting them into their own type would be viable.\nThe net/http package could, for example, provide separate ResponseWriter types for different connection types (for example HTTP2Response). It could then provide a func(HTTP2Handler) http.Handler, that serves an error if it is asked to serve an unsuitable connection and otherwise delegates to the passed Handler. Now, the programmer needs to explicitly wire a handler that requires HTTP/2 up accordingly. They can rely on the additional features, while also making clear what paths must be used over HTTP/2.\nGradual repair I think the use of optional interfaces as in database/sql/driver is perfectly fine - if you plan to eventually remove the original interface. Otherwise, users will have to continue to implement both interfaces to be usable with your API, which is especially painful when wrapping interfaces. For example, I recently wanted to wrap importer.Default to add behavior and logging. I also needed ImporterFrom, which required separate implementations, depending on whether the importer returned by Default implements it or not. Most modern code, however, shouldn\u0026rsquo;t need that.\nSo, for third party packages (the stdlib can\u0026rsquo;t do that, because of compatibility guarantees), you should consider using the methodology described in Russ Cox\u0026rsquo; excellent Codebase Refactoring article and actually deprecate and eventually remove the old interface. Use optional interfaces as a transition mechanism, not a fix.\nHow could Go improve the situation? Make it possible for reflect to create methods There are currently at least two GitHub issues which would make it possible to do extend interfaces dynamically: reflect: NamedOf, reflect: MakeInterface. I believe this would be the easiest solution - it is backwards compatible and doesn\u0026rsquo;t require any language changes.\nProvide a language mechanism for extension The language could provide a native mechanism to express extension, either by adding a keyword for it or, for Go2, by considering to make extension the default behavior for interface-\u0026gt;struct embedding. I\u0026rsquo;m not sure either is a good idea, though. I would probably prefer the latter, because of my distaste for keywords. Note, that it would still be possible to then compose an interface into a struct, just not via embedding but by adding a field and delegation-methods. Personally, I\u0026rsquo;m not a huge fan of embedding interfaces in structs anyway except when I\u0026rsquo;m explicitly trying to extend them with additional behavior. Their zero-value is not usable, so it requires additional hoops to jump through.\nConclusion I recommend:\n If at all possible, avoid optional interfaces in APIs you provide. They are just too inconvenient and un-Go-ish. Be careful when wrapping interfaces, in particular when there are known optional interfaces for them.  Using optional interfaces correctly is inconvenient and cumbersome. That should signal that you are fighting the language. The workarounds needed all try to circumvent one or more design decision of Go: to value composition over inheritance, to prefer static typing and to make computation and behavior obvious from code. To me, that signifies that optional interfaces are fundamentally not a good fit for the language.\n","permalink":"https://blog.merovius.de/posts/2017-07-30-the-trouble-with-optional-interfaces/","summary":"I take a look at the pattern of optional interfaces in Go: what they are used for, why they are bad and what we can do about it.","title":"The trouble with optional interfaces"},{"content":"tl;dr: I used Hilbert Curves to make it quicker to walk through a list of locations on a map, so I could could fully complete a video game.\nAs you probably know recently the question of what the best Zelda Game is was finally settled by Breath Of The Wild. Like most people I know I ended up playing. And to keep me engaged I early on decided that I would get as close as possible to 100% of the game before finishing it. That is I wanted to finish all shrines, find all Korok Seeds, max out all armor and do all quests before killing Ganon. I recently finished that and finally killed Ganon. Predictably, I was in for a disappointment:\n98.59 percent! I did expect that though. The reason is that only certain things count into the percentage as displayed; Korok Seeds are one of them, Shrines are another. But it also counts landmarks and locations as shown on the map. Each contributes 1/12% to the total.\nSo I started on the onerous task of finding the last 17 locations. I\u0026rsquo;m not above using help for that so I carefully scrolled through an online map of the BotW universe, maticulously comparing the locations on it with the ones already on my in-game map. Anything I haven\u0026rsquo;t visited was marked and visited. But that only put me to 99.58%; I was still missing 5 locations. apparently I didn\u0026rsquo;t compare carefully enough.\nI needed a more systematic approach. I started to instead go through an alphabetical list of locations, looking up each on the map and see if I already had it mapped. But that got old really quickly. Alphabetical just wasn\u0026rsquo;t a great way to organize these; I wanted a list that I could systematically check. But I didn\u0026rsquo;t want it alphabetically but geographically. I didn\u0026rsquo;t want to have to jump around the map to try and find the next one. Which is when I realized that this would be the perfect application for a Hilbert curve.\nIf you don\u0026rsquo;t know (though you should really just read the Wikipedia Article), the Hilbert curve is a space filling fractal curve, that is a continuous bijective map from the real number line to the plane. It is iteratively defined as the limit of finite curves that get denser and denser. One of the most interesting properties of the curve and its finite approximations is that points that are close on the real number line get mapped to points that are close in the plane. So if we could extract all locations from the online map, figure out for each what real number gets mapped to that point and order the locations by those numbers, we\u0026rsquo;d get a list of locations where neighbors in the list are close to each other on the map. Presumably, that would make for easy checking of the list: The next location should be pretty much neighbouring the previous one and if I can\u0026rsquo;t find a location nearby, chances are that I didn\u0026rsquo;t visit it yet (and I can then look it up specifically).\n[edit] Commentors on reddit and Hacker news have pointed out correctly, that all curves satisfy the property that near point on the line map to near points on the plane. What makes the Hilbert Curve special, is that we work on finite approximations and with the Hilbert Curve, we don\u0026rsquo;t have to worry about the \u0026ldquo;correct\u0026rdquo; level of discrete approximation.\nTo see what that means, we can look at a zig-zag curve. Say, we split our map into a 100000x100000 grid and move in a zig-zag, left-to-right, top-to-bottom. Given how sparse our point-set is, this would mean that most of the rows are empty and some of them would have only one point on them. So we wozuld have to constantly move along the entire width of the map. On the other hand, if we split it into a 2x2 grid, it wouldn\u0026rsquo;t be very helpful; a lot of points would end up in the same quadrant, which would be very large, so we wouldn\u0026rsquo;t have won anything. So there would have to be some fineness of the grid that\u0026rsquo;s \u0026ldquo;just right\u0026rdquo; somewhere in the middle, which we\u0026rsquo;d need to find.\nOn the other hand with Hilbert Curves, this isn\u0026rsquo;t a problem. That\u0026rsquo;s because the limit of the finite approximations is continuous (which isn\u0026rsquo;t the case with the limit of zig-zag curves). What that means, in essence, is that where a point falls on the curve won\u0026rsquo;t jump around a lot when we make our grid finer, it will \u0026ldquo;home in\u0026rdquo; to its final location on the continuous curve. A first order Hilbert Curve is just a zig-zag curve, so it has the same problem as the 2x2-grid zig-zag line. But as we increase it\u0026rsquo;s order, the points will just become more and more local, instead of requiring scanning empty space. That is the interesting consequence of the Hilbert curve being space-filling.\nReally, this video explains it much better than I ever could (even though I find the example given there slightly ridiculous). In the end, I mostly agree with the commentors; it probably wouldn\u0026rsquo;t have been too hard to find a good approximation that would make a zig-zag curve work well. But I had Hilbert Curves ready anyway and appreciated the opportunity to usue them.[/edit]\nThe first step for this was to get a list of locations and their corresponding positions. I was pretty sure that the online map should have that available somehow, as it uses some Google Maps framework to draw the map. So I looked at the network tab of the Chrome developer tools, found the URL that loaded the landmark data, copied the request as curl and saved the output for further massaging.\nThe returned file turns out to not actually be JSON (that\u0026rsquo;d be too easy, I guess) but some kind of javascript-code which is then probably eventually eval\u0026rsquo;d to get the data (edit: It has been pointed out, that this is just JSONP. I was aware that this is probably the case, but didn\u0026rsquo;t feel comfortable using the term, as I don\u0026rsquo;t know enough about it. I also didn\u0026rsquo;t consider it very important) :\n/**/jQuery31106443585752152035_1500757689075(/* json-data */) I just removed everything but the actual JSON with my editor and ran it through a pretty-printer, to get at it\u0026rsquo;s actual structure. I spare you the details; it turns out the list of locations isn\u0026rsquo;t even simply contained in that it\u0026rsquo;s embedded as another string, with HTML tags, as a property (twice!).\nSo I quickly hacked together some go code to dissect the data and voilà: Got a list of location names with the corresponding positions:\nfunc main() { \tvar data struct { \tParse struct { \tProperties []struct { \tName string `json:\u0026#34;name\u0026#34;` \tContent string `json:\u0026#34;*\u0026#34;` \t} `json:\u0026#34;properties\u0026#34;` \t} `json:\u0026#34;parse\u0026#34;` \t}  \tif err := json.NewDecoder(os.Stdin).Decode(\u0026amp;data); err != nil { \tpanic(err) \t}  \tvar content string  \tfor _, p := range data.Parse.Properties { \tif p.Name == \u0026#34;description\u0026#34; { \tcontent = p.Content \t} \t}  \tif content == \u0026#34;\u0026#34; { \tpanic(\u0026#34;no content\u0026#34;) \t}  \tvar landmarks []struct { \tType string \tGeometry struct { \tType string \tCoordinates []float64 \t} \tProperties struct { \tType string \tId string \tName string \tLink string \tSrc string \t} \t}  \tif err := json.NewDecoder(arrayReader(content)).Decode(\u0026amp;landmarks); err != nil { \tpanic(err) \t}  \tfor _, m := range landmarks { \tfmt.Printf(\u0026#34;%s: %v\\n\u0026#34;, m.Properties.Name, m.Geometry.Coordinates) \t} }  func arrayReader(s string) io.Reader { \ts = strings.TrimSuffix(strings.TrimSpace(s), \u0026#34;,\u0026#34;) \treturn io.MultiReader(strings.NewReader(\u0026#34;[\u0026#34;), strings.NewReader(s), strings.NewReader(\u0026#34;]\u0026#34;)) } This bode well. Now all I needed to do was to calculate the Hilbert Curve coordinate for each of them and I\u0026rsquo;d have what I need. The Wikipedia Article helpfully contains an implementation of the corresponding algorithm in C. xy2d assumes a discrete grid of n² cells and returns an integer preimage of the given coordinates. The coordinates we have are all floating point numbers between 0 and 2 (ish) with 5 significant digits. I figured that 65536 should be able to represent the granularity of points well enough, so I chose that as an n, ported the code to go, sorted the locations accordingly and it actually worked!\nfunc main() { \t// Same stuff as before  \tsort.Slice(landmarks, func(i, j int) bool { \txi := f2d(landmarks[i].Geometry.Coordinates[0]) \tyi := f2d(landmarks[i].Geometry.Coordinates[1]) \txj := f2d(landmarks[j].Geometry.Coordinates[0]) \tyj := f2d(landmarks[j].Geometry.Coordinates[1]) \tdi := xy2d(1\u0026lt;\u0026lt;16, xi, yi) \tdj := xy2d(1\u0026lt;\u0026lt;16, xj, yj) \treturn di \u0026lt; dj \t})  \tfor _, m := range landmarks { \tfmt.Printf(\u0026#34;%s: %v\\n\u0026#34;, m.Properties.Name, m.Geometry.Coordinates) \t} }  func xy2d(n, x, y int) int { \tvar d int \tfor s := n / 2; s \u0026gt; 0; s = s / 2 { \tvar rx, ry int \tif (x \u0026amp; s) \u0026gt; 0 { \trx = 1 \t} \tif (y \u0026amp; s) \u0026gt; 0 { \try = 1 \t} \td += s * s * ((3 * rx) ^ ry) \tx, y = rot(s, x, y, rx, ry) \t} \treturn d }  func rot(n, x, y, rx, ry int) (int, int) { \tif ry == 0 { \tif ry == 1 { \tx = n - 1 - x \ty = n - 1 - y \t} \tx, y = y, x \t} \treturn x, y }  func f2d(f float64) int { \treturn int((1 \u0026lt;\u0026lt; 15) * f) } In the end, there was still a surprising amount of jumping around involved. I don\u0026rsquo;t know whether that\u0026rsquo;s accidental (i.e. due to my code being wrong) or inherent (that is the Hilbert curve just can\u0026rsquo;t map this perfectly well). I assume it\u0026rsquo;s a bit of both. The list also contains the same landmark multiple times. This is because things like big lakes or plains where marked multiple times. It would be trivial to filter duplicates but I actually found them reasonably helpfull when having to jump around.\nThere might also be better approaches than Hilbert Curves. For example, we could view it as an instance of the Traveling Salesman Problem with a couple of hundred points; it should be possible to have a good heuristic solution for that. On the other hand, a TSP solution doesn\u0026rsquo;t necessarily only have short jumps, so it might not be that good?\nIn any case, this approach was definitely good enough for me and it\u0026rsquo;s probably the nerdiest thing I ever did :)\n","permalink":"https://blog.merovius.de/posts/2017-07-22-using-hilbert-curves-to-100-zelda/","summary":"Using math, I made it a lot easier to find the last undiscovered parts of my Zelda - Breath Of The Wild map.","title":"Using Hilbert Curves to 100% Zelda"},{"content":"If you don\u0026rsquo;t write web-thingies in go you can stop reading now. Also, I am somewhat snarky in this article. I intend that to be humorous but am probably failing. Sorry for that\nAs everyone™ knows, people need to stop writing routers/muxs in go. Some people attribute the abundance of routers to the fact that the net/http package fails to provide a sufficiently powerful router, so people roll their own. This is also reflected in this post, in which a gopher complains about how complex and hard to maintain it would be to route requests using net/http alone.\nI disagree with both of these. I don\u0026rsquo;t believe the problem is a lack of a powerful enough router in the stdlib. I also disagree that routing based purely on net/http has to be complicated or hard to maintain.\nHowever, I do believe that the community currently lacks good guidance on how to properly route requests using net/http. The default result seems to be that people assume they are supposed to use http.ServeMux and get frustrated by it. In this post I want to explain why routers in general - including http.ServeMux - should be avoided and what I consider simple, maintainable and scalable routing using nothing but the stdlib.\nBut why? Why do I believe that routers should not be used? I have three arguments for that: They need to be very complex to be useful, they introduce strong coupling and they make it hard to understand how requests are flowing.\nThe basic idea of a router/mux is, that you have a single component which looks at a request and decides what handler to dispatch it to. In your func main() you then create your router, you define all your routes with all your handlers and then you call Serve(l, router) and everything\u0026rsquo;s peachy.\nBut since URLs can encode a lot of important information to base your routing decisions on, doing it this way requires a lot of extra features. The stdlib ServeMux is an incredibly simple router but even that contains a certain amount of magic in its routing decisions; depending on whether a pattern contains a trailing slash or not it might either be matched as a prefix or as a complete URL and longer patterns take precedence over shorter ones and oh my. But the stdlib router isn\u0026rsquo;t even powerful enough. Many people need to match URLs like \u0026quot;/articles/{category}/{id:[0-9]+}\u0026quot; in their router and while we\u0026rsquo;re at it also extract those nifty arguments. So they\u0026rsquo;re using gorilla/mux instead. An awful lot of code to route requests.\nNow, without cheating (and actually knowing that package counts as cheating), tell me for each of these requests:\n GET /foo GET /foo/bar GET /foo/baz POST /foo PUT /foo PUT /foo/bar POST /foo/123  What handler they map to and what status code do they return (\u0026ldquo;OK\u0026rdquo;? \u0026ldquo;Bad Request\u0026rdquo;? \u0026ldquo;Not Found\u0026rdquo;? \u0026ldquo;Method not allowed\u0026rdquo;?) in this routing setup?\nr := mux.NewRouter() r.PathPrefix(\u0026#34;/foo\u0026#34;).Methods(\u0026#34;GET\u0026#34;).HandlerFunc(Foo) r.PathPrefix(\u0026#34;/foo/bar\u0026#34;).Methods(\u0026#34;GET\u0026#34;).HandlerFunc(FooBar) r.PathPrefix(\u0026#34;/foo/{user:[a-z]+}\u0026#34;).Methods(\u0026#34;GET\u0026#34;).HandlerFunc(FooUser) r.PathPrefix(\u0026#34;/foo\u0026#34;).Methods(\u0026#34;POST\u0026#34;).HandlerFunc(PostFoo) What if you permute the lines in the routing-setup?\nYou might guess correctly. You might not. There are multiple sane routing strategies that you could base your guess on. The routes might be tried in source order. The routes might be tried in order of specificity. Or a complicated mixture of all of them. The router might realize that it could match a Route if the method were different and return a 405. Or it might not not. Or that /foo/123 is, technically, an illegal argument, not a missing page. I couldn\u0026rsquo;t really find a good answer to any of these questions in the documentation of gorilla/mux for what it\u0026rsquo;s worth. Which meant that when my web app suddenly didn\u0026rsquo;t route requests correctly, I was stumped and needed to dive into code.\nYou could say that people just have to learn how gorilla/mux decides it\u0026rsquo;s routing (I believe it\u0026rsquo;s \u0026ldquo;as defined in source order\u0026rdquo;, by the way). But there are at least fifteen thousand routers for go and no newcomer to your application will ever know all of them. When a request does the wrong thing, I don\u0026rsquo;t want to have to debug your router first to find out what handler it is actually going to and then debug that handler. I want to be able to follow the request through your code, even if I have next to zero familiarity with it.\nLastly, this kind of setup requires that all the routing decisions for your application are done in a central place. That introduces edit-contention, it introduces strong coupling (the router needs to be aware of all the paths and packages needed in the whole application) and it becomes unmaintainable after a while. You can alleviate that by delegating to subrouters though; which really is the basis of how I prefer to do all of this these days.\nHow to use the stdlib to route Let\u0026rsquo;s build the toy example from this medium post. It\u0026rsquo;s not terribly complicated but it serves nicely to illustrate the general idea. The author intended to show that using the stdlib for routing would be too complicated and wouldn\u0026rsquo;t scale. But my thesis is that the issue is that they are effectively trying to write a router. They are trying to encapsulate all the routing decisions into one single component. Instead, separate concerns and make small, easily understandable routing decisions locally.\nRemember how I told you that we\u0026rsquo;re going to use only the stdlib for routing?\nWe are going to use this one helper function:\n// ShiftPath splits off the first component of p, which will be cleaned of // relative components before processing. head will never contain a slash and // tail will always be a rooted path without trailing slash. func ShiftPath(p string) (head, tail string) { \tp = path.Clean(\u0026#34;/\u0026#34; + p) \ti := strings.Index(p[1:], \u0026#34;/\u0026#34;) + 1 \tif i \u0026lt;= 0 { \treturn p[1:], \u0026#34;/\u0026#34; \t} \treturn p[1:i], p[i:] } Let\u0026rsquo;s build our app. We start by defining a handler type. The premise of this approach is that handlers are strictly separated in their concerns. They either correctly handle a request with the correct status code or they delegate to another handler which will do that. They only need to know about the immediate handlers they delegate to and they only need to know about the sub-path they are rooted at:\ntype App struct { \t// We could use http.Handler as a type here; using the specific type has \t// the advantage that static analysis tools can link directly from \t// h.UserHandler.ServeHTTP to the correct definition. The disadvantage is \t// that we have slightly stronger coupling. Do the tradeoff yourself. \tUserHandler *UserHandler }  func (h *App) ServeHTTP(res http.ResponseWriter, req *http.Request) { \tvar head string \thead, req.URL.Path = ShiftPath(req.URL.Path) \tif head == \u0026#34;user\u0026#34; { \th.UserHandler.ServeHTTP(res, req) \treturn \t} \thttp.Error(res, \u0026#34;Not Found\u0026#34;, http.StatusNotFound) }  type UserHandler struct { }  func (h *UserHandler) ServeHTTP(res http.ResponseWriter, req *http.Request) { \tvar head string \thead, req.URL.Path = ShiftPath(req.URL.Path) \tid, err := strconv.Atoi(head) \tif err != nil { \thttp.Error(res, fmt.Sprintf(\u0026#34;Invalid user id %q\u0026#34;, head), http.StatusBadRequest) \treturn \t} \tswitch req.Method { \tcase \u0026#34;GET\u0026#34;: \th.handleGet(id) \tcase \u0026#34;PUT\u0026#34;: \th.handlePut(id) \tdefault: \thttp.Error(res, \u0026#34;Only GET and PUT are allowed\u0026#34;, http.StatusMethodNotAllowed) \t} }  func main() { \ta := \u0026amp;App{ \tUserHandler: new(UserHandler), \t} \thttp.ListenAndServe(\u0026#34;:8000\u0026#34;, a) } This seems very simple to me (not necessarily in \u0026ldquo;lines of code\u0026rdquo; but definitely in \u0026ldquo;understandability\u0026rdquo;). You don\u0026rsquo;t need to know anything about any routers. If you want to understand how the request is routed you start by looking at main. You see that (*App).ServeHTTP is used to serve any request so you :GoDef to its definition. You see that it decides to dispatch to UserHandler, you go to its ServeHTTP method and you see directly how it parses the URL and what the decisions are that it made on its base.\nWe still need to add some patterns to our application. Let\u0026rsquo;s add a profile handler:\ntype UserHandler struct{ \tProfileHandler *ProfileHandler }  func (h *UserHandler) ServeHTTP(res http.ResponseWriter, req *http.Request) { \tvar head string \thead, req.URL.Path = ShiftPath(req.URL.Path) \tid, err := strconv.Atoi(head) \tif err != nil { \thttp.Error(res, fmt.Sprintf(\u0026#34;Invalid user id %q\u0026#34;, head), http.StatusBadRequest) \treturn \t}  \tif req.URL.Path != \u0026#34;/\u0026#34; { \thead, tail := ShiftPath(req.URL.Path) \tswitch head { \tcase \u0026#34;profile\u0026#34;: \t// We can\u0026#39;t just make ProfileHandler an http.Handler; it needs the \t// user id. Let\u0026#39;s instead… \th.ProfileHandler.Handler(id).ServeHTTP(res, req) \tcase \u0026#34;account\u0026#34;: \t// Left as an exercise to the reader. \tdefault: \thttp.Error(res, \u0026#34;Not Found\u0026#34;, http.StatusNotFound) \t} \treturn \t} \t// As before \t... }  type ProfileHandler struct { }  func (h *ProfileHandler) Handler(id int) http.Handler { \treturn http.HandlerFunc(func(res http.ResponseWriter, req *http.Request) { \t// Do whatever \t}) } This may, again, seem complicated but it has the cool advantage that the dependencies of ProfileHandler are clear at compile time. It needs a user id which needs to come from somewhere. Providing it via this kind of method ensures this is the case. When you refactor your code, you won\u0026rsquo;t accidentally forget to provide it; it\u0026rsquo;s impossible to miss!\nThere are two potential alternatives to this if you prefer them: You could put the user-id into req.Context() or you could be super-hackish and add them to req.Form. But I prefer it this way.\nYou might argue that App still needs to know all the transitive dependencies (because they are members, transitively) so we haven\u0026rsquo;t actually reduced coupling. But that\u0026rsquo;s not true. Its UserHandler could be created by a NewUserHandler function which gets passed its dependencies via the mechanism of your choice (flags, dependency injection,…) and gets wired up in main. All App needs to know is the API of the handlers it\u0026rsquo;s directly invoking.\nConclusion I hope I convinced you that routers in and of itself are harmful. Pulling the routing into one component means that that component needs to encapsulate an awful lot of complexity, making it hard to debug. And as no single existing router will contain all the complicated cleverness you want to base your routing decisions on, you are tempted to write your own. Which everyone does.\nInstead, split your routing decisions into small, independent chunks and express them in their own handlers. And wire the dependencies up at compile time, using the type system of go, and reduce coupling.\n","permalink":"https://blog.merovius.de/posts/2017-06-18-how-not-to-use-an-http-router/","summary":"Trying to provide some advice on how to do easy, readable, scalable routing in go, without relying on any muxers/routers (or writing your own).","title":"How to not use an http-router in go"},{"content":"tl;dr: I\u0026rsquo;ve been diagnosed with ADHD. I ramble incoherently for a while and I might do some less rambling posts about it in the future.\nAs the title says, I\u0026rsquo;ve been recently diagnosed with ADHD and I thought I\u0026rsquo;d try to be as open about it as possible and share my personal experiences with mental illness. That being said, I am also adding the disclaimer, that I have no training or special knowledge about it and that the fact that I have been diagnosed with ADHD does not mean I am an authoritative source on its effects, that this diagnoses is going to stick or that my experiences in any way generalize to other people who got the same diagnosis.\nThis will hopefully turn into a bit of a series of blog posts and I\u0026rsquo;d like to start it off with a general description of what lead me to look for a diagnosis and treatment in the first place. Some of the things I am only touching on here I might write more about in the future (see below for a non-exhaustive list). Or not. I have not yet decided :)\n It is no secret (it\u0026rsquo;s actually kind of a running gag) that I am a serious procrastinator. I always had trouble starting on something and staying with it; my graveyard of unfinished projects is huge. For most of my life, however, this hasn\u0026rsquo;t been a huge problem to me. I was reasonably successful in compensating for it with a decent amount of intelligence (arrogant as that sounds). I never needed any homework and never needed to learn for tests in school and even at university I only spent very little time on both. The homework we got was short-term enough that procrastination was not a real danger, I finished it quickly and whenever there was a longer-term project to finish (such as a seminar-talk or learning for exams) I could cram for a night and get enough of an understanding of things to do a decent job.\nHowever, that strategy did not work for either my bachelor, nor my master thesis, which predictably lead to both turning out a lot worse than I would\u0026rsquo;ve wished for (I am not going to go into too much detail here). Self-organized long-term work seemed next to impossible. This problem got much worse when I started working full-time. Now almost all my work was self-organized and long-term. Goals are set on a quarterly basis, the decision when and how and how much to work is completely up to you. Other employers might frown at their employees slacking off at work; where I work, it\u0026rsquo;s almost expected. I was good at being oncall, which is mainly reactive, short-term problem solving. But I was (and am) completely dissatisfied with my project work. I felt that I did not get nearly as much done as I should or as I would want. My projects in my first quarter had very clear deadlines and I finished on time (I still procrastinated, but at least at some point I sat down until I got it done. It still meant staying at work until 2am the day before the deadline) but after that it went downhill fast, with projects that needed to be done ASAP, but not with a deadline. So I started slipping. I didn\u0026rsquo;t finish my projects (in fact, the ones that I didn\u0026rsquo;t drop I am still not done with), I spent weeks doing effectively nothing (I am not exaggerating here. I spent whole weeks not writing a single line of code, closing a single bug or running a single command, doing nothing but browse reddit, twitter and wasting my time in similar ways. Yes, you can waste a week doing nothing, while sitting at your desk), not being able to get myself to start working on anything and hating myself for it.\nAnd while I am mostly talking about work, this affected my personal life too. Mail remains unopened, important errands don\u0026rsquo;t get done, I am having trouble keeping in contact with friends, because I can always write or visit them some other time…\nI tried (and had tried over the past years) several systems to organize myself better, to motivate myself and to remove distractions. I was initially determined to try to solve my problems on my own, that I did not really need professional help. However, at some point, I realized that I won\u0026rsquo;t be able to fix this just by willing myself to it. I realized it in the final months of my master thesis, but convinced myself that I don\u0026rsquo;t have time to fix it properly, after all, I have to write my thesis. I then kind of forgot about it (or rather: I procrastinated it) in the beginning of starting work, because things where going reasonably well. But it came back to me around the start of this year. After not finishing any of my projects in the first quarter. And after telling my coworkers and friends of my problems and them telling me that it\u0026rsquo;s just impostor syndrome and a distorted view of myself (I\u0026rsquo;ll go into why they where wrong some more later, possibly).\nI couldn\u0026rsquo;t help myself and I couldn\u0026rsquo;t get effective help from my coworkers. So, in April, I finally decided to see a Psychologist. Previously the fear of the potential cost (or the stress of dealing with how that works with my insurance), the perceived complexity of finding one that is both accepting patients that are only publicly insured and is specialized on my particular kinds of issues and the perceived lack of time prevented me from doing so. Apart from a general doubt about its effectiveness and fear of the implications for my self-perception and world-view, of course.\nLuckily one of the employee-benefits at my company is free and uncomplicated access to a Mental Health (or \u0026ldquo;Emotional well-being\u0026rdquo;, what a fun euphemism) provider. It only took a single E-Mail and the meetings happen around 10 meters away from my desk. So I started seeing a psychologist on a regular basis (on average probably every one and a half weeks or so) and talking about my issues. I explained and described my problems and it went about as good as I feared; they tried to convince me that the real issue isn\u0026rsquo;t my performance, but my perception of it (and I concede that I still have trouble coming up with hard, empirical evidence to present to people. Though it\u0026rsquo;s performance review season right now. As I haven\u0026rsquo;t done anything of substance in the past 6 months, maybe I will finally get that evidence…) and they tried to get me to adopt more systems to organize myself and remove distractions. All the while, I got worse and worse. My inability to get even the most basic things done or to concentrate even for an hour, even for five minutes, on anything of substance, combined with the inherent social isolation of moving to a new city and country, lead me into deep depressive episodes.\nFinally, when my Psychologist in a session tried to get me to write down what was essentially an Unschedule (a system I knew about from reading \u0026ldquo;The Now Habit\u0026rdquo; myself when working at my bachelor thesis and that I even had moderate success with; for about two weeks), I broke down. I told them that I do not consider this a valuable use of these sessions, that I tried systems before, that I tried this particular system before and that I can find these kind of lifestyle advise on my own, in my free time. That the reason I was coming to these sessions was to get systematic, medical, professional help of the kind that I can\u0026rsquo;t get from books. So we agreed, at that point, to pursue a diagnosis, as a precondition for treatment.\nWhich, basically, is where we are at now. The diagnostic process consisted of several sessions of questions about my symptoms, my childhood and my life in general, of filling out a couple of diagnostic surveys and having my siblings fill them out too (in the hope that they can fill in some of the blanks in my own memory about my childhood) and of several sessions of answering more questions from more surveys. And two weeks ago, I officially got the diagnosis ADHD. And the plan to attack it by a combination of therapy and medication (the latter, in particular, is really hard to get from books, for some reason :) ).\nI just finished my first day on Methylphenidate (the active substance in Ritalin), specifically Concerta. And though this is definitely much too early to actually make definitive judgments on its effects and effectiveness, at least for this one day I was feeling really great and actively happy. Which, coincidentally, helped me to finally start on this post, to talk about mental health issues; a topic I\u0026rsquo;ve been wanting to talk about ever since I started this blog (again), but so far didn\u0026rsquo;t really felt I could.\n As I said, this is hopefully the first post in a small ongoing series. I am aware that it is long, rambling and probably contentious. It definitely won\u0026rsquo;t get all my points across and will change the perception some people have of me (I can hear you thinking how all of this doesn\u0026rsquo;t really speak \u0026ldquo;mental illness\u0026rdquo;, how it seems implausible that someone with my CV would actually, objectively, get nothing done and how I am a drama queen and shouldn\u0026rsquo;t try to solve my problems with dangerous medication). It\u0026rsquo;s an intentionally broad overview of my process and its main purpose is to \u0026ldquo;out\u0026rdquo; myself publicly and create starting points for multiple, more specific, concise, interesting and convincing posts in the future. Things I might, or might not talk about are\n My specific symptoms and how this has and still is influencing my life (and how, yes, this is actual an illness, not just a continuous label). In particular, there are things I wasn\u0026rsquo;t associating with ADHD, which turn out to be relatively tightly linked. How my medication is specifically affecting me and what it does to those symptoms. I can not overstate how fascinated I am with today\u0026rsquo;s experience. I was wearing a visibly puzzled expression all day because I couldn\u0026rsquo;t figure out what was happening. And then I couldn\u0026rsquo;t stop smiling. :) Possibly things about my therapy? I really don\u0026rsquo;t know what to expect about that, though. Therapy is kind off the long play, so it\u0026rsquo;s much harder to evaluate and talk about its effectiveness. Why I consider us currently to be in the Middle Ages of mental health and why I think that in a hundred years or so people will laugh at how we currently deal with it. And possibly be horrified. My over ten years (I\u0026rsquo;m still young, mkey‽) of thinking about my own mental health and mental health in general and my thoughts of how mental illness interacts with identity and self-definition. How much I loathe the term \u0026ldquo;impostor syndrome\u0026rdquo; and why I am (still) convinced that I don\u0026rsquo;t get enough done, even though I can\u0026rsquo;t produce empirical evidence for that and people try to convince me otherwise. And what it does to you, to need to take the \u0026ldquo;I suck\u0026rdquo; side of an argument and still don\u0026rsquo;t have people believe you.  Let me know, what you think :)\n","permalink":"https://blog.merovius.de/posts/2016-08-31-ive-been-diagnosed-with-adhd/","summary":"I\u0026rsquo;ve been diagnosed with ADHD. I ramble incoherently for a while and I might do some less rambling posts about it in the future.","title":"I've been diagnosed with ADHD"},{"content":"tl;dr: There are next to no \u0026ldquo;backwards compatible API changes\u0026rdquo; in go. You should explicitely name your compatibility-guarantees.\nI really love go, I really hate vendoring and up until now I didn\u0026rsquo;t really get, why anyone would think go should need something like that. After all, go seems to be predestined to be used with automatically checked semantic versioning. You can enumerate all possible changes to an API in go and the list is quite short. By looking at vcs-tags giving semantic versions and diffing the API, you can automatically check that you never break compatibility (the go compiler and stdlib actually do something like that). Heck, in theory you could even write a package manager that automatically (without any further annotations) determines the latest version of a package that still builds all your stuff or gives you the minimum set of packages that need changes to reconcile conflicts.\nThis thought lead me to contemplate what makes an API change a breaking change. After a bit of thought, my conclusion is that almost every API change is a breaking change, which might surprise you.\nFor this discussion we first need to make some assumptions about what constitutes breakage. We will use the go1 compatibility promise. The main gist is: Stuff that builds before is guaranteed to build after. Notable exceptions (apart from necessary breakages due to security or other bugs) are unkeyed struct literals and dot-imports.\n[edit] I should clarify, that whenever I talk about an API-change, I mean your exported API as defined by Code (as opposed to comments/documentation). This includes the public identifiers exported by your package, including type information. It excludes API-requirements specified in documentation, like on io.Writer. These are just too complex to talk about in a meaningful way and must be dealt with separately anyway. [/edit]\nSo, given this definition of breakage, we can start enumerating all the possible changes you could do to an API and check whether they are breaking under the definition of the go1 compatibility promise:\nAdding func/type/var/const at package scope This is the only thing that seems to be fine under the stability guarantee. It turns out the go authors thought about this one and put the exception of dot-imports into the compatibility promise, which is great.\ndot-imports are imports of the form . import \u0026quot;foo\u0026quot;. They import every package-level identifier of package foo into the scope of the current file.\nAbsence of dot-imports means, every identifier at your package scope must be referenced with a selector-expression (i.e. foo.Bar) which can\u0026rsquo;t be redeclared by downstream. It also means that you should never use dot-imports in your packages (which is a bad idea for other reasons too). Treat dot-imports as a historic artifact which is completely deprecated. An exception is the need to use a separate foo_test package for your tests to break dependency cycles. In that case it is widely deemed acceptable to . import \u0026quot;foo\u0026quot; to save typing and add clarity.\nRemoving func/type/var/const at package scope Downstream might use the removed function/type/variable/constant, so this is obviously a breaking change.\nAdding a method to an interface Downstream might want to create an implementation of your interface and try to pass it. After you add a method, this type doesn\u0026rsquo;t implement your interface anymore and downstreams code will break.\nRemoving a method from an interface Downstream might want to call this method on a value of your interface type, so this is obviously a breaking change.\nAdding a field to a struct This is perhaps surprising, but adding a field to a struct is a breaking change. The reason is, that downstream might embed two types into a struct. If one of them has a field or method Bar and the other is a struct you added the Field Bar to, downstream will fail to build (because of an ambiguous selector expression).\nSo, e.g.:\n// foo/foo.go package foo  type Foo struct { \tFoo string \tBar int // Added after the fact }  // bar/bar.go package bar  type Baz struct { \tBar int }  type Spam struct { \tfoo.Foo \tBaz }  func Eggs() { \tvar s Spam \ts.Bar = 42 // ambiguous selector s.Bar } This is what the compatibility might refer to with the following quote:\n Code that uses unkeyed struct literals (such as pkg.T{3, \u0026ldquo;x\u0026rdquo;}) to create values of these types would fail to compile after such a change. However, code that uses keyed literals (pkg.T{A: 3, B: \u0026ldquo;x\u0026rdquo;}) will continue to compile after such a change. We will update such data structures in a way that allows keyed struct literals to remain compatible, although unkeyed literals may fail to compile. (There are also more intricate cases involving nested data structures or interfaces, but they have the same resolution.)\n (emphasis is mine). By \u0026ldquo;the same resolution\u0026rdquo; they might refer to only accessing embedded Fields via a keyed selector (so e.g. s.Baz.Bar in above example). If so, that is pretty obscure and it makes struct-embedding pretty much useless. Every usage of a field or method of an embedded type must be explicitly Keyed, which means you can just not embed it after all. You need to write the selector and wrap every embedded method anyway.\nI hope we all agree that type embedding is awesome and shouldn\u0026rsquo;t need to be avoided :)\nRemoving a field from a struct Downstream might use the now removed field, so this is obviously a breaking change.\nAdding a method to a type The argument is pretty much the same as adding a field to a struct: Downstream might embed your type and suddenly get ambiguities.\nRemoving a method from a type Downstream might call the now removed method, so this is obviously a breaking change.\nChanging a function/method signature Most changes are obviously breaking. But as it turns out you can\u0026rsquo;t do any change to a function or method signature. This includes adding a variadic argument which looks backwards compatible on the surface. After all, every call site will still be correct, right?\nThe reason is, that downstream might save your function or method in a variable of the old type, which will break because of nonassignable types.\nConclusion It looks to me like anything that isn\u0026rsquo;t just adding a new Identifier to the package-scope will potentially break some downstream. This severely limits the kind of changes you can do to your API if you want to claim backwards compatibility.\nThis of course doesn\u0026rsquo;t mean that you should never ever make any changes to your API ever. But you should think about it and you should clearly document, what kind of compatibility guarantees you make. When you do any changes named in this document, you should check your downstreams, whether they are affected by it. If you claim a similar level of compatibility as the go standard library, you should definitely be aware of the implications and what you can and can\u0026rsquo;t do.\nWe, the go community, should probably come up with some coherent definition of what changes we deem backwards compatible and which we don\u0026rsquo;t. A tool to automatically looks up all your (public) importerts on godoc.org, downloads the latest version and tries to build them with your changes should be fairly simple to write in go (and may even already exist). We should make it a standard check (like go vet and golint) for upstream package authors to do that kind of thing before push to prevent frustrated downstreams.\nOf course there is still the possibility, that my reading of the go1 compatibility promise is wrong or inaccurate. I would welcome comments on that, just like on everything else in this post :)\n","permalink":"https://blog.merovius.de/posts/2015-07-29-backwards-compatibility-in-go/","summary":"There are next to no \u0026ldquo;backwards compatible API changes\u0026rdquo; in go. You should explicitely name your compatibility-guarantees.","title":"Backwards compatibility in go"},{"content":"tl;dr: I did lazy evaluation in go\nA small pattern that is usefull for some algorithms is lazy evaluation. Haskell is famous for making extensive use of it. One way to emulate goroutine-safe lazy evaluation is using closures and the sync-package:\ntype LazyInt func() int  func Make(f func() int) LazyInt { \tvar v int \tvar once sync.Once \treturn func() int { \tonce.Do(func() { \tv = f() \tf = nil // so that f can now be GC\u0026#39;ed \t}) \treturn v \t} }  func main() { \tn := Make(func() { return 23 }) // Or something more expensive… \tfmt.Println(n()) // Calculates the 23 \tfmt.Println(n() + 42) // Reuses the calculated value } This is not the fastest possible code, but it already has less overhead than one would think (and it is pretty simple to deduce a faster implementation from this). I have implemented a simple command, that generates these implementations (or rather, more optimized ones based on the same idea) for different types.\nThis is of course just the simplest use-case for lazynes. In practice, you might also want Implementations of Expressions\nfunc LazyAdd(a, b LazyInt) LazyInt { \treturn Make(func() { return a() + b() }) } or lazy slices (slightly more complicated to implement, but possible) but I left that for a later improvement of the package (plus, it makes the already quite big API even bigger) :)\n","permalink":"https://blog.merovius.de/posts/2015-07-17-lazy-evaluation-in-go/","summary":"I did lazy evaluation in go.","title":"Lazy evaluation in go"},{"content":"I\u0026rsquo;ve been thinking about how to do an authentication scheme, that uses some kind of relational database (it doesn\u0026rsquo;t matter specifically, that the database is relational, the concerns should pretty much apply to all databases) as a backing store, in a way that is resilient against timing side-channel attacks and doesn\u0026rsquo;t leak any data about which usernames exist in the system and which don\u0026rsquo;t.\nThe first obvious thing is, that you need to do a constant time comparison of password-hashes. Luckily, most modern crypto libraries should include something like that (at least go\u0026rsquo;s bcrypt implementation comes with that).\nBut now the question is, how you prevent enumerating users (or checking for existence). A naive query will return an empty result set if the user does not exists, so again, obviously, you need to compare against some password, even if the user isn\u0026rsquo;t found. But just doing, for example\nif result.Empty { \t// Compare against a prepared hash of an empty password, to have constant \t// time check. \tbcrypt.CompareHashAndPassword(HashOfEmptyPassword, enteredPassword) } else { \tbcrypt.CompareHashAndPassword(result.PasswordHash, enteredPassword) } won\u0026rsquo;t get you very far. Because (for example) the CPU will predict either of the two branches (and the compiler might or might not decide to \u0026ldquo;help\u0026rdquo; with that), so again an attacker might be able to distinguish between the two cases. The best way, to achieve resilience against timing side-channels is to make sure, that your control flow does not depend on input data at all. Meaning no branch or loop should ever take in any way into account, what is actually input into your code (including the username and the result of the database query).\nSo my next thought was to modify the query to return the hash of an empty password as a default, if no user is found. That way, your code is guaranteed to always get a well-defined bcrypt-hash from the database and your control flow does not depend on whether or not the user exists (and an empty password can be safely excluded in advance, as returning early for that does not give any new data to the attacker).\nWhich sounds well, but now the question is, if maybe the timing of your database query tells the attacker something. And this is where I hit a roadblock: If the attacker knows enough about your code (i.e. what database engine you are using, what machine you are running on and what kind of indices your database uses) they can potentially enumerate users by timing your database queries. To illustrate: If you would use a simple linear list as an index, a failed search has to traverse the whole list, whereas a successfull search will abort early. The same issue exists with balanced trees. An attacker could potentially hammer your application with unlikely usernames and measure the mean time to answer. They can then test individual usernames and measure if the time to answer is significantly below the mean for failures, thus enumerating usernames.\nNow, I haven\u0026rsquo;t tested this for practicality yet (might be fun) and it is pretty likely that this can\u0026rsquo;t be exploited in reality. Also, the possibility of enumerating users isn\u0026rsquo;t particularly desirable, but it is also far from a security meltdown of your authentication-system. Nevertheless, the idea that this theoretical problem exists makes me uneasy.\nAn obvious fix would be to make sure, that every query always has to search the complete table on every lookup. I don\u0026rsquo;t know if that is possible, it might be just trivial by not giving a limit and not marking the username column as unique, but it might also be hard and database-dependent because there will still be an index over this username column which might still create the same kind of issues. There will also likely still be a variance, because we basically just shifted the condition from our own code into the DBMS. I have simply no idea.\nSo there you have it. I am happy to be corrected and pointed to some trivial design. I will likely accept the possibity of being vulnerable here, as the systems I am currently building aren\u0026rsquo;t that critical. But I will probably still have a look at how other projects are handling this. And maybe if there really is a problem in practice.\n","permalink":"https://blog.merovius.de/posts/2015-04-13-difficulties-making-sql-based-au/","summary":"It is surprisingly hard to make SQL-based authentication immune to timing side-channels, if you want to prevent enumeration of users.","title":"SQL authentication timing side-channels"},{"content":"As people who know me know, my current favourite language is go. One of the best features of go is the lack of features. This is actually the reason I preferred C over most scripting languages for a long time – it does not overburden you with language-features that you first have to wrap your head around. You don\u0026rsquo;t have to think for a while about what classes or modules or whatever you want to have, you just write your code down and the (more or less) entire language can easily fit inside your head. One of the best writeups of this (contrasting it with python) was done by Gustavo Niemeyer in a blogpost a few years back.\nSo when I say, there are a few things popping up I miss about go, this does not mean I wish them to be included. I subjectively miss them and it would definitely make me happy, if they existed. But I still very much like the go devs for prioritizing simplicity over making me happy.\nSo let\u0026rsquo;s dig in.\n Generics Weak references Dynamic loading of go code Garbage-collected goroutines  Generics So let\u0026rsquo;s get this elephant out of the room first. I think this is the most named feature lacking from go. They are asked so often, they have their own entry in the go FAQ. The usual answers are anything from \u0026ldquo;maybe they will get in\u0026rdquo; to \u0026ldquo;I don\u0026rsquo;t understand why people want generics, go has generic programming using interfaces\u0026rdquo;. To illustrate one shortcoming of the (current) interface approach, consider writing a (simple) graph-algorithm:\ntype Graph [][]int  func DFS(g Graph, start int, visitor func(int)) { \tvisited := make([]bool, len(g))  \tvar dfs func(int) \tdfs = func(i int) { \tif visited[i] { \treturn \t} \tvisitor(i) \tvisited[i] = true \tfor _, j := range g[i] { \tdfs(j) \t} \t}  \tdfs(start) } This uses an adjacency list to represent the graph and does a recursive depth-first-search on it. Now imagine, you want to implement this algorithm generically (given, a DFS is not really hard enough to justify this, but you could just as easily have a more complex algorithm). This could be done like this:\ntype Node interface{}  type Graph interface { \tNeighbors(Node) []Node }  func DFS(g Graph, start Node, visitor func(Node)) { \tvisited := make(map[Node]bool)  \tvar dfs func(Node) \tdfs = func(n Node) { \tif visited[n] { \treturn \t} \tvisitor(n) \tvisited[n] = true \tfor _, n2 := range g.Neighbors(n) { \tdfs(n2) \t} \t}  \tdfs(start) } This seems simple enough, but it has a lot of problems. For example, we loose type-safety: Even if we write Neighbors(Node) []Node there is no way to tell the compiler, that these instances of Node will actually always be the same. So an implementation of the graph interface would have to do type-assertions all over the place. Another problem is:\ntype AdjacencyList [][]int  func (l AdjacencyList) Neighbors(n Node) []Node { \ti := n.(int) \tvar result []Node \tfor _, j := range l[i] { \tresult = append(result, j) \t} \treturn result } An implementation of this interface as an adjacency-list actually performs pretty badly, because it can not return an []int, but must return a []Node, and even though int satisfies Node, []int is not assignable to []Node (for good reasons that lie in the implementation of interfaces, but still).\nThe way to solve this, is to always map your nodes to integers. This is what the standard library does in the sort-package. It is exactly the same problem. But it might not always be possible, let alone straightforward, to do this for Graphs, for example if they do not fit into memory (e.g. a web-crawler). The answer is to have the caller maintain this mapping via a map[Node]int or something similar, but… meh.\nWeak references I have to admit, that I am not sure, my use case here is really an important or even very nice one, but let\u0026rsquo;s assume I want to have a database abstraction that transparently handles pointer-indirection. So let\u0026rsquo;s say I have two tables T1 and T2 and T2 has a foreign key referencing T1. I think it would be pretty neat, if a database abstraction could automatically deserialize this into a pointer to a T1-value A. But to do this. we would a) need to be able to recognize A a later Put (so if the user changes A and later stores it, the database knows what row in T1 to update) and b) hand out the same pointer, if another row in T2 references the same id.\nThe only way I can think how to do this is to maintain a map[Id]*T1 (or similar), but this would prevent the handed out values to ever be garbage-collected. Even though there a hacks that would allow some use cases for weak references to be emulated, I don\u0026rsquo;t see how they would work here.\nSo, as in the case of generics, this mainly means that some elegant APIs are not possible in go for library authors (and as I said, in this specific case it probably isn\u0026rsquo;t a very good idea. For example you would have to think about what happens, if the user gets the same value in two different goroutines from the database).\nDynamic loading of go code It would be useful to be able to dynamically load go code at runtime, to build plugins for go software. Specifically I want a good go replacement for jekyll because I went through some ruby-version-hell with it lately (for example jekyll serve -w still does not work for me with the version packaged in debian) and I think a statically linked go-binary would take a lot of possible pain-points out here. But plugins are a really important feature of jekyll for me, so I still want to be able to customize a page with plugins (how to avoid introducing the same version hell with this is another topic).\nThe currently recommended ways to do plugins are a) as go-packages and recompiling the whole binary for every change of a plugin and b) using sub-processes and net/rpc.\nI don\u0026rsquo;t feel a) being a good fit here, because it means maintaining a separate binary for every jekyll-site you have which just sounds like a smallish nightmare for binary distributions (plus I have use cases for plugins where even the relatively small compilation times of go would result in an intolerable increase in startup-time).\nb) on the other hand results in a lot of runtime-penalty: For example I can not really pass interfaces between plugins, let alone use channels or something and every function call has to have its parameters and results serialized and deserialized. Where in the same process I can just define a transformation between different formats as a func(r io.Reader) io.Reader or something, in the RPC-context I first have to transmit the entire file over a socket, or have the plugin-author implement a net/rpc server himself and somehow pass a reference to it over the wire. This increases the burden on the plugin-authors too much, I think.\nLuckily, it seems there seems to be some thought put forward recently on how to implement this, so maybe we see this in the nearish future.\nGarbage-collected goroutines Now, this is the only thing I really don\u0026rsquo;t understand why it is not part of the language. Concurrency in go is a first-class citizen and garbage-collection is a feature emphasized all the time by the go-authors as an advantage. Yet, they both seem to not play entirely well together, making concurrency worse than it has to be.\nSomething like the standard example of how goroutines and channels work goes a little bit like this:\nfunc Foo() { \tch := make(chan int) \tgo func() { \ti := 0 \tfor { \tch \u0026lt;- i \ti++ \t} \t}()  \tfor { \tfmt.Println(\u0026lt;-ch) \t} } Now, this is all well, but what if we want to exit the loop prematurely? We have to do something like this:\nfunc Foo() { \tch := make(chan int) \tdone := make(chan bool) \tgo func() { \ti := 0 \tfor { \tselect { \tcase ch \u0026lt;- i: \ti++ \tcase \u0026lt;-done: \treturn \t} \t} \t}() \tfor { \ti := \u0026lt;-ch \tif i \u0026gt; 1000 { \tbreak \t} \tfmt.Println(i) \t} } Because otherwise the goroutine would just stay around for all eternity, effectively being leaked memory. There are entire talks build around this and similar problems, where I don\u0026rsquo;t really understand why. If we add a break to our first version, Foo returns and suddenly, all other references to ch, except the one the goroutine is blocking on writing to are gone and can be garbage-collected. The runtime can already detect if all goroutines are sleeping and we have a deadlock, the garbage-collector can accurately see what references there are to a given channel, why can we not combine the two to just see \u0026ldquo;there is absolutely no way, this channel-write can ever succeed, so let\u0026rsquo;s just kill it and gc all it\u0026rsquo;s memory\u0026rdquo;? This would have zero impact on existing programs (because as you can not get any references to goroutines, a deadlocked one can have no side-effect on the rest of the program), but it would make channels so much more fun to work with. It would make channels as iterators a truly elegant pattern, it would simplify pipelines and it would possibly allow a myriad other use cases for channels I can not think of right now. Heck, you could even think about (not sure if this is possible or desirable) running any deferred statements, when a goroutine is garbage-collected, so all other resources held by it will be correctly released.\nThis is the one thing I really wish to be added to the language. Really diving into channels and concurrency right now is very much spoiled for me because I always have to think about draining every channel, always think about what goroutine closes what channels, passing cancellation-channels…\n","permalink":"https://blog.merovius.de/posts/2014-09-12-the-four-things-i-miss-from-go/","summary":"A short list of four things that I might want to add to go (but probably wouldn\u0026rsquo;t).","title":"The four things I miss about go"},{"content":"I stumbled upon a mildly interesting problem yesterday: Given an Array a and a permutation p, apply the permutation (in place) to the Array, using only O(1) extra space. So, if b is the array after the algorithm, we want that a[i] == b[p[i]].\nNaively, we would solve our problem by doing something like this (I\u0026rsquo;m using go here):\nfunc Naive(vals, perm []int) { \tn := len(vals) \tres := make([]int, n) \tfor i := range vals { \tres[perm[i]] = vals[i] \t} \tcopy(vals, res) } This solves the problem in O(n) time, but it uses of course O(n) extra space for the result array. Note also, that it does not really work in place, we have to copy the result back.\nThe simplest iteration of this, would be to simply use a sorting-algorithm of our choice, but use as a sorting key not the value of the elements, but the position of the corresponding field in the permutation array:\nimport \u0026#34;sort\u0026#34;  type PermSorter struct { \tvals []int \tperm []int }  func (p PermSorter) Len() int { \treturn len(p.vals) }  func (p PermSorter) Less(i, j int) bool { \treturn p.perm[i] \u0026lt; p.perm[j] }  func (p PermSorter) Swap(i, j int) { \tp.vals[i], p.vals[j] = p.vals[j], p.vals[i] \tp.perm[i], p.perm[j] = p.perm[j], p.perm[i] }  func Sort(vals, perm []int) { \tsort.Sort(PermSorter{vals, perm}) } This appears a promising idea at first, but as it turns out, this doesn\u0026rsquo;t really use constant space after all (at least not generally). The go sort package uses introsort internally, which is a combination of quick- and heapsort, the latter being chosen if the recursion-depth of quicksort exceeds a limit in O(log(n)). Thus it uses actually O(log(n)) auxiliary space. Also, the running time of sorting is O(n log(n)) and while time complexity wasn\u0026rsquo;t part of the initially posed problem, it would actually nice to have linear running time, if possible.\nNote also another point: The above implementation sorts perm, thus destroying the permutation array. Also not part of the original problem, this might pose problems if we want to apply the same permutation to multiple arrays. We can rectify that in this case by doing the following:\ntype NDPermSorter struct { \tvals []int \tperm []int }  func (p NDPermSorter) Len() int { \treturn len(p.vals) }  func (p NDPermSorter) Less(i, j int) bool { \treturn p.perm[p.vals[i]] \u0026lt; p.perm[p.vals[j]] }  func (p NDPermSorter) Swap(i, j int) { \tp.vals[i], p.vals[j] = p.vals[j], p.vals[i] }  func NDSort(vals, perm []int) { \tsort.Sort(NDPermSorter{vals, perm}) } But note, that this only works, because we want to sort an array of consecutive integers. In general, we don\u0026rsquo;t want to do that. And I am unaware of a solution that doesn\u0026rsquo;t have this problem (though I also didn\u0026rsquo;t think about it a lot).\nThe solution of solving this problem in linear time lies in a simple observation: If we start at any index and iteratively jump to the target index of the current one, we will trace out a cycle. If any index is not in the cycle, it will create another cycle and both cycles will be disjoint. For example the permutation\ni 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 p[i] 2 13 1 5 3 15 14 12 8 10 4 19 16 11 9 7 18 6 17 0 will create the following set of cycles: \nSo the idea is to resolve every cycle separately, by iterating over the indices and moving every element to the place it belongs:\nfunc Cycles(vals, perm []int) { \tfor i := 0; i \u0026lt; len(vals); i++ { \tv, j := vals[i], perm[i] \tfor j != i { \tvals[j], v = v, vals[j] \tperm[j], j = j, perm[j] \t} \tvals[i], perm[i] = v, i \t} } This obviously only needs O(1) space. The secret, why it also only uses O(n) time lies in the fact, that the inner loop will not be entered for elements, that are already at the correct position. Thus this is (from a complexity standpoint at least) the optimal solution to the problem, as it is impossible to use less than linear time for applying a permutation.\nThere is still one small problem with this solution: It also sorts the permutation array. We need this, to know when a position is already occupied by it\u0026rsquo;s final element. In our algorithm this is represented by the fact, that the permutation is equal to it\u0026rsquo;s index at that point. But really, it would be nice if we could mark the index without losing the order of the permutation. But that is not hard either - because every index is non-negative, we can simply negate every index we are done with. This will make a negative index out of it and we can check for that if we encounter it later and skip it in this case. After we are done, we only need to take care to flip everything back and all should be fine:\nfunc NDCycles(vals, perm []int) { \tfor i := 0; i \u0026lt; len(vals); i++ { \tif perm[i] \u0026lt; 0 { \t// already correct - unmark and go on \t// (note that ^a is the bitwise negation \tperm[i] = ^perm[i] \tcontinue \t}  \tv, j := vals[i], perm[i] \tfor j != i { \tvals[j], v = v, vals[j] \t// When we find this element in the future, we must not swap it any \t// further, so we mark it here \tperm[j], j = ^perm[j], perm[j] \t} \tvals[i] = v \t} } Here we only mark the elements we will again encounter in the future. The current index will always be unmarked, once we are done with the outer loop.\nI am aware, that this is technically cheating; This solution relies on the fact, that the upper-most bit of the permutation elements won\u0026rsquo;t ever be set. Thus, we actually do have O(n) auxiliary space (as in n bit), because these bits are not necessary for the algorithm. However, since it is pretty unlikely, that we will find an architecture where this is not possible (and go guarantees us that it actually is, because len(vals) is always signed, so we cant have arrays that are big enough for the msb being set anyway), I think I am okay with it ;)\nI ran sum Benchmarks on this an these are the figures I came up with:\n  n Naive NDCycle NDSort   10 332 ns 130 ns 1499 ns   100 883 ns 1019 ns 27187 ns   1000 15046 ns 17978 ns 473078 ns   10000 81800 ns 242121 ns 4659433 ns   I did not measure space-use. The time of NDCycle for 10000 elements seems suspicious - while it is not surprising, that in general it takes more time than the naive approach, due to it\u0026rsquo;s complexity, this jump is unexpected. Maybe if I have the time I will investigate this and also measure memory use. In the meantime, I uploaded all the code used here, so you can try it out yourself. You can run it with go run perm.go and run the benchmarks with go test -bench Benchmark.*.\n","permalink":"https://blog.merovius.de/posts/2014-08-12-applying-permutation-in-constant/","summary":"A mildly interesting algorithmic problem and my solution to it.","title":"Applying permutation in constant space (and linear time)"},{"content":"tl:dr: We made a gamejam-game\nAt the GPN14 we (meaning me and Lea, with a bit of help by sECuRE) participated in the gamejam. It was the first time for us both, I did all the coding and Lea provided awesome graphics.\nThe result is a small minigame “crazy cat lady”, where you throw cats at peoples faces and - if you hit - scratch the shit out of them (by hammering the spacebar). The game mechanics are kind of limited, but the graphics are just epic, in my opinion:\nBecause sounds make every game 342% more awesome, we added a creative commons licensed background-music. We also wanted some cat-meowing and very angry pissed of hissing, which was more of a problem to come by. Our solution was to just wander about the GPN and asking random nerds to make cat sounds and recording them. That gave a pretty cool result, if you ask me.\nOn the technical side we used LÖVE, an open 2D game engine for lua, widely used in gamejams. I am pretty happy with this engine, it took about 3 hours to get most of the game-mechanics going, the rest was pretty much doing the detailed animations. It is definitely not the nicest or most efficient code, but for a gamejam it is a well suited language and engine.\nIf you want to try it (I don\u0026rsquo;t think it is interesting for more than a few minutes, but definitely worth checking out), you should install LÖVE (most linux-distributions should have a package for that) and just download it, or check out the sourcecode.\nWe did not make first place, but that is okay, the game that won is a nice game and a deserved winner. We had a lot of fun and we are all pretty happy with the result as first-timers.\n","permalink":"https://blog.merovius.de/posts/2014-06-22-gpn14-gamejam--crazy-cat-lady/","summary":"We made a gamejam-game.","title":"GPN14 GameJam - Crazy cat lady"},{"content":"This is an argument of belief, but I think this is highly irregular and unexpected behaviour of python:\na = [1, 2, 3, 4] b = a a = a + [5] print(b) a = [1, 2, 3, 4] b = a a += [5] print(b) Output:\n[1, 2, 3, 4] [1, 2, 3, 4, 5] Call me crazy, but in my world, x += y should behave exactly the same as x = x + y and this is another example, why operator overloading can be abused in absolutely horrible ways.\nNever mind, that there is actually python teaching material out there that teaches wrong things. That is, there are actually people out there who think they know python well enough to teach it, but don\u0026rsquo;t know this. Though credit where credit is due, the official documentation mentions this behaviour.\n","permalink":"https://blog.merovius.de/posts/2014-05-06-python-fnord-of-the-day/","summary":"An unexpected behavior of python that surprised me today.","title":"Python-fnord of the day"},{"content":"Due to the Heartbleed vulnerability I had to recreate all TLS-keys of my server. Since CACert appears to be mostly dead (or dying at least), I am currently on the lookout for a new CA. In the meantime I switched to self-signed certificates for all my services.\nThe new fingerprints are:\n  Service SHA1-Fingerprint   merovius.de 8C:85:B1:9E:37:92:FE:C9:71:F6:0E:C6:9B:25:9C:CD:30:2B:D5:35   blog.merovius.de 1B:DB:45:11:F3:EE:66:8D:3B:DF:63:B9:7C:D9:FC:26:A4:D1:E1:B8   git.merovius.de 65:51:16:25:1A:9E:50:B2:F7:D7:8A:2B:77:DE:DE:0C:02:3C:6C:ED   smtp (mail.merovius.de) 1F:E5:3F:9D:EE:B4:47:AE:2E:02:D8:2C:1E:2A:6C:FC:D6:62:99:F4   jabber (merovius.de) 15:64:29:49:82:0E:8B:76:47:1A:19:5B:98:6F:E4:56:24:D9:69:07   This is of course useless in the general case, but if you already trust my gpg-key, you can use\ncurl http://blog.merovius.de/2014/04/10/heartbleed-new-certificates.html | gpg to get this post signed and verified.\n","permalink":"https://blog.merovius.de/posts/2014-04-10-heartbleed-new-certificates/","summary":"Updating my TLS-certificates due to Heartbleed.","title":"Heartbleed: New certificates"},{"content":"Let\u0026rsquo;s say you write a library in go and want an easy way to get debugging information from your users. Sure, you return errors from everything, but it is sometimes hard to pinpoint where a particular error occured and what caused it. If your package panics, that will give you a stacktrace, but as you probably know you shouldn\u0026rsquo;t panic in case of an error, but just gracefull recover and return the error to your caller.\nI recently discovered a pattern which I am quite happy with (for now). You can include a stacktrace when returning an error. If you disable this behaviour by default you should have as good as no impact for normal users, while making it much easier to debug problems. Neat.\npackage awesomelib import ( \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; ) type tracedError struct { err error trace string } var ( stacktrace bool traceSize = 16*1024 ) func init() { if os.Getenv(\u0026#34;AWESOMELIB_ENABLE_STACKTRACE\u0026#34;) == \u0026#34;true\u0026#34; { stacktrace = true } } func wrapErr(err error) error { // If stacktraces are disabled, we return the error as is if !stacktrace { return err } // This is a convenience, so that we can just throw a wrapErr at every // point we return an error and don\u0026#39;t get layered useless wrappers if Err, ok := err.(*tracedError); ok { return Err } buf := make([]byte, traceSize) n := runtime.Stack(buf, false) return \u0026amp;tracedError{ err: err, trace: string(buf[:n]) } } func (err *tracedError) Error() string { return fmt.Sprintf(\u0026#34;%v\\n%s\u0026#34;, err.err, err.trace) } func DoFancyStuff(path string) error { file, err := os.Open(path) if err != nil { return wrapErr(err) } // fancy stuff } ","permalink":"https://blog.merovius.de/posts/2014-02-19-go-stacktraces/","summary":"A small pattern to add stacktraces to errors.","title":"go stacktraces"},{"content":"tl;dr: I sign my blog posts. curl http://blog.merovius.de/2014/01/23/signed-blog-posts.html | gpg\nUpdate: I no longer do this. I haven\u0026rsquo;t for a while\nI might have to update my TLS server certificate soon, because the last change seems to have broken the verification of https://merovius.de/. This is nothing too exciting, but it occured to me that I should actually provide some warning or notice in that case, so that people can be sure, that there is nothing wrong. The easiest way to accomplish this would be a blogpost and the easiest way to verify that the statements in that blogpost are correct would be, to provide a signed version. So because of this (and, well, because I can) I decided to sign all my blogposts with my gpg-key. People who know me should have my gpg key so they can verify that I really have written everything I claim.\nI could have used jekyll-gpg_clearsign, but it does not really do the right thing in my opinion. It wraps all the HTML in a GPG SIGNED MESSAGE block and attaches a signature. This has the advantage of minimum overhead - you only add the signature itself plus some constant comments of overhead. However, it makes really verifying the contents of a blogpost pretty tedious: You would have to either manually parse the HTML in your mind, or you would have to save it to disk and view it in your browser, because you cannot be sure, that the HTML you get when verifying it via curl on the commandline is the same you get in your browser. You could write a browser-extension or something similar that looks for these blocks, but still, the content could be tempered with (for example: Add the correctly signed page as a comment in a tampered with page. Or try to somehow include some javascript that changes the text after verifying…). Also, the generated HTML is not really what I want to sign; after all I can not really attest that the HTML-generation is really solid and trustworthy, I never read the jekyll source-code and I don\u0026rsquo;t want to, at every update. What I really want to sign is the stuff I wrote myself, the markdown (or whatever) I put into the post. This has the additional advantage, that most markdown is easily parseable by humans, so you can actually have your gpg output the signed text and immediately read everything I wrote.\nSo this is, what happens now. In every blogpost there is a HTML-comment embedded, containing the original markdown I wrote for this post in compressed, signed and ASCII-armored form. You can try it via\ncurl http://blog.merovius.de/2014/01/23/signed-blog-posts.html | gpg  This should output some markdown to stdout and a synopsis of gpg about a valid (possibly untrusted, if you don\u0026rsquo;t have my gpg-key) signature on stderr. Neat!\nThe changes needed in the blog-code itself where pretty minimal. I had however (since I don\u0026rsquo;t want my gpg secret key to be on the server) to change the deployment a little bit. Where before a git push would trigger a hook on the remote repository on my server that ran jekyll, now I have a local script, that wraps a jekyll build, an rsync to the webserver-directory and a git push. gpg-agent ensures, that I am not asked for a passphrase too often.\nSo, yeah. Crypto is cool. And the procrastinator prevailed again!\n","permalink":"https://blog.merovius.de/posts/2014-01-23-signed-blog-posts/","summary":"I sign my blog posts.","title":"Signed blog posts"},{"content":"tl;dr: I hate software-engineering as it is teached in Heidelberg. Really\nI often recited the story of how I got to choose computer science over physics as a minor in my mathematics bachelor:\nAfter sitting through almost one semester of the introductory course to theoretical physics in my 3rd semester — which is incredibly unsatisfactory and boring, once you are past your first year of mathematics — I suddenly realized that my reward for suffering through yet another problem sheet of calculating yet another set of differential operators is, that I have to suffer through four or five more of these type of courses. This really seemed like a poor incentive, when I was just discovering hacking and that I was really good at computer science. So I decided to pass on the opportunity, did not work all night on that last sheet (and later found out that I would have gotten credit for that course without even taking the written exam if I just handed in this additional problem sheet) and instead decided to minor in computer science.\nThree years after that I decided to get a second bachelor degree in computer science (I finished my bachelor of mathematics earlier that year and was pursuing my master degree at that point), because it seemed a really easy thing to do at that point: I only needed two semesters more of studies and a bachelor thesis. That is not a lot of work for a degree. We are now one year and some change after that point, and there really is not a lot I need anymore. Basically I only need to finish the introduction to software engineering and then write my thesis. Yay for me.\nThe reason I write this (and the reason I started with the anecdote of physics) is that once again I am questioning the incentives versus the cost. Since I am pretty sure that it would actually be fun to write my thesis, it all boils down to the question, whether I want to finish this course (which again, I\u0026rsquo;m more than halfway done with, it is not a lot work to go) to get a bachelor degree in computer science. And don\u0026rsquo;t get me wrong — I\u0026rsquo;m sure that software engineering is a topic, that can be interesting and captivating, or at the minimum bearable. But the way it is done here in Heidelberg is just hell. It is incredibly boringly presented and consists of a flood of uninteresting repetitive tasks and the project-work, to show how important teamwork and quality-assurance and drawing a lot of diagrams is, is a catastrophically bad, unusable and ugly piece of crapware, that can\u0026rsquo;t even decently perform the very simple task it was designed to (managing a private movie collection. I mean, come on, it is not exactly rocket science to do this in at least a barely usable way).\nAnd even though it is a hell that I would only have to endure for about two or three problem sheets and one written exam, I watch myself putting off the work on it (for example by writing this stupid blogpost) and I seriously question whether this second bachelor is really incentive enough to suffer through it.\nIf it was my first degree, that would of course be a clear ”yes“. But a second one? Not sure. Ironically the main way I\u0026rsquo;m putting of work on this problem sheet — I got up today at 10am, to immediately and energetically start to work on it — is watching I lot of TED talks on youtube. That\u0026rsquo;s right, I practically spent 14 hours more or less non-stop watching TED talks. This one applies to some extend — extrinsic incentives can only go this far in making us do some work; at some point, without at least some intrinsic motivation, I at least will not perform very well (or at all):\n  ","permalink":"https://blog.merovius.de/posts/2013-12-16-incentives-in-education/","summary":"I hate software-engineering as it is teached in Heidelberg. Really.","title":"Incentives in education"},{"content":"tl;dr: ext4 has a feature called dir_index enabled by default, which is quite susceptible to hash-collisions\nI am currently restructuring my mail-setup. Currently, I use offlineimap to sync my separate accounts to a series of maildirs on my server. I then use sup on the server as a MUA. I want to switch to a local setup with notmuch, so I set up an dovecot imapd on my server and have all my accounts forward to my primary address. I then want to use offlineimap to have my mails in a local maildir, which I browse with notmuch.\nI then stumbled about a curious problem: When trying to copy my mails from my server to my local harddisk, it would fail after about 50K E-mails with the message “could not create xyz: no space left on device” (actually, offlineimap would just hog all my CPUs and freeze my whole machine in the process, but that\u0026rsquo;s a different story). But there actually was plenty of space left.\nIt took me and a few friends a whole while to discover the problem. So if you ever get this error message (using ext4) you should probably check these four things (my issue was the last one):\nDo you actually have enough space? Use df -h. There is actually a very common pitfall with ext4. Let\u0026rsquo;s have a look:\nmero@rincewind ~$ df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/sda2_crypt 235G 164G 69G 71% / ... If you add 164G and 69G, you get 233G, which is 2G short of the actual size. This is about 1%, but on your system it will likely be more of 5% difference. The reason is the distinction between \u0026ldquo;free\u0026rdquo; and \u0026ldquo;available\u0026rdquo; space. Per default on ext4, there are about 5% of \u0026ldquo;reserved\u0026rdquo; blocks. This has two reasons: First ext4\u0026rsquo;s performance seems to take a small hit, when almost full. Secondly, it leaves a little space for root to login and troubleshoot problems or delete some files, when users filled their home-directory. If there was no space left, it might well be, that no login is possible anymore (because of the creation of temporary files, logfiles, history-files…). So use tune2fs \u0026lt;path_to_your_disk\u0026gt; to see, if you have reserved blocks, and how many of them:\nmero@rincewind ~$ sudo tune2fs -l /dev/mapper/sda2_crypt | grep \u0026#34;Reserved block\u0026#34; Reserved block count: 2499541 Reserved blocks uid: 0 (user root) Reserved blocks gid: 0 (group root) Do you have too many files? Even though you might have enough space left, it might well be, that you have too many files. ext4 allows an enormous amount of files on any file system, but it is limited. Checking this is easy: Just use df -i:\nFilesystem Inodes IUsed IFree IUse% Mounted on /dev/mapper/sda2_crypt 15622144 925993 14696151 6% / ... So as you see, that wasn\u0026rsquo;t the problem with me. But if you ever have the IUse% column near 100, you probably want to delete some old files (and you should definitely question, how so many files could be created to begin with).\nDo a file system check At least some people on the internet say, that something like this has happened to them after a crash (coincidentally my system crashed before the problem arose. See above comments about offlineimap) and that a file system check got rid of it. So you probably want to run fsck -f \u0026lt;path_to_your_disk\u0026gt; to run such a check. You probably also want to do that from a live-system, if you cannot unmount it (for example if it\u0026rsquo;s mounted at the root-dir).\nDo you have dir_index enabled? So this is the punch line: ext4 has the possibility to hash the filenames of its contents. This enhances performance, but has a “small” problem: ext4 does not grow its hashtable, when it starts to fill up. Instead it returns -ENOSPC or “no space left on device”.\next4 uses half_md4 as a default hashing-mechanism. If I interpret my google-results correctly, this uses the md4-hash algorithm, but strips it to 32 bits. This is a classical example of the birthday-paradox: A 32 bit hash means, that there are 4294967296 different hash values available, so if we are fair and assume a uniform distribution of hash values, that makes it highly unlikely to hit one specific hash. But the probability of hitting two identical hashes, given enough filenames, is much much higher. Using the formula from Wikipedia we get (with about 50K files) a probability of about 25% that a newly added file has the same hash. This is a huge probability of failure. If on the other hand we take a 64bit hash-function the probability becomes much smaller, about 0.00000000007%.\nSo if you have a lot of files in the same directory, you probably want to switch off dir_index, or at least change to a different hash-algorithm. You can check if you have dir_index enabled and change the hash, like this:\nmero@rincewind ~$ sudo tune2fs -l /dev/mapper/sda2_crypt | grep -o dir_index dir_index ## Change the hash-algo to a bigger one mero@rincewind ~$ sudo tune2fs -E \u0026#34;hash_alg=tea\u0026#34; /dev/mapper/sda2_crypt ## Disable it completely mero@rincewind ~$ sudo tune2fs -O \u0026#34;^dir_index\u0026#34; Note however, that dir_index and half_md4 where choices made for performance reasons. So you might experience a performance-hit after this.\nUPDATE: After trying it out, I realized, that the problem actually also persists with the tea-hash. I then had a look at the ext4-documentation about the topic and it seems, that the hash is only stored as 32 bits, so it actually does not matter what hash we choose, regarding this particular problem. So if half_md4 is chosen because of its better performance and collision-resistance it actually makes sense to leave it as the default. You can by the way easily test and reproduce the issue by using the following on an ext4 file system:\nfor a in `seq 100000` do file=`head -c 51 /dev/urandom | base64 | tr \u0026#39;/\u0026#39; \u0026#39;_\u0026#39;` touch $file done Curiously, this only gives me about 160 collisions on 100K files (instead of about 10K collisions on 60K files), which would suggest, that my original sample (meaning my mailbox) exhibits some properties that make collisions more likely both on half_md4 and tea.\n","permalink":"https://blog.merovius.de/posts/2013-10-20-ext4-mysterious-no-space-left-on/","summary":"ext4 has a feature called dir_index enabled by default, which is quite susceptible to hash-collisions","title":"ext4: Mysterious “No space left on device”-errors"},{"content":"tl;dr: Put a terminal with a vim-instance in an i3-scratchpad, combine it with autosave-when-idle and you got the perfect note keeping workflow\nThere are often occasions where I want to write something down, while not wanting to disturb my thought-process too much or taking too much of an effort. An example for the former would be a short TODO I suddenly remember while doing something more important. As an example for the latter, I keep an \u0026ldquo;account\u0026rdquo; for drinks at our local computer club, so that I don\u0026rsquo;t always have to put single coins into the register, but can just put 20€ or something in and don\u0026rsquo;t have to worry about it for a while. Combining the scratchpad-window feature of i3 with a little vim-magic makes this effortless enough to be actually preferable to just paying.\nFirst of, you should map a key to scratchpad show in i3, for example I have the following in my config:\nbind Mod4+Shift+21 move scratchpad bind Mod4+21 scratchpad show I can then just use Mod4+\u0026lt;backtic\u0026gt; to access the scratchpad.\nNow, just put a terminal in scratchpad-mode and open .notes in vim in this terminal. By pressing the scratchpad show binding repeatedly, you can send it to the background and bring it to the foreground again.\nI have my current \u0026ldquo;balance\u0026rdquo; in this notes-file and during the meetings of the computer club leave the cursor on this balance. If I take a drink, I press ^X decreasing my balance by one (every drink is one Euro). If I pay, say 10 Euros into the register, I press 10^A increasing my balance by 10.\nThis is already much better, but it still has one problem: I better save that file every time I change my balance, else a crash would screw up my accounting. Luckily, vim provides autocommands and has an event for \u0026ldquo;the user did not type for a while\u0026rdquo;. This means, that we can automatically save the file if we idled for a few seconds, for example if we send the scratchpad window away. For this, we put the following in our .vimrc:\n\u0026#34; Automatically save the file notes when idle autocmd CursorHold .notes :write Now adjusting my balance is just a matter of a very short key sequence: \u0026lt;mod4\u0026gt;`\u0026lt;c-x\u0026gt;\u0026lt;mod4\u0026gt;` \n","permalink":"https://blog.merovius.de/posts/2013-10-20-using-i3-and-vim-to-keep-a-set-o/","summary":"Put a terminal with a vim-instance in an i3-scratchpad, combine it with autosave-when-idle and you got the perfect note keeping workflow.\u0026quot;","title":"Using i3 and vim to keep a set of notes at hand"},{"content":"tl;dr: I wrote a simple Tic Tac Toe AI as an exercise. You can get it on github\nI am currently considering writing a basic chess AI as an exercise in AI development and to help me analyze my own games (and hopefully get a better chess-player just by thinking about how a machine would do it). As a small exercise and to get some familiarity with the algorithms involved, I started with Tic Tac Toe. Because of the limited number of games (only 255168) all positions can be bruteforced very fast, which makes it an excellent exercise, because even with a very simple Minimax-Algorithm perfect play is possible.\nMy AI uses exactly this algorithm (if coded a little crude). It comes with a little TUI and a small testsuite, you can try it like this:\n$ git clone git://github.com/Merovius/tictactoe.git $ cd tictactoe $ make $ make test $ ./tictactoe You will notice, that there already is no noticable delay (at least not on a relatively modern machine), even though the AI is unoptimized and bruteforces the whole tree of possible moves on every move.\nNext I will first refactor the basic algorithm in use now, then I will probably implement better techniques, such as limited search-depth, αβ-Pruning or machine learning. I will then think about moving on to a little more complex games (for example Connect 4, Mill or Hex seem good choices). Then I will decide how big the effort would be for chess and if it\u0026rsquo;s worth a try.\n","permalink":"https://blog.merovius.de/posts/2013-10-19-tic-tac-toe-ai/","summary":"I wrote a simple Tic Tac Toe AI as an exercise. You can get it on github.","title":"Tic Tac Toe AI"},{"content":"tl;dr: Using gdb to manipulate a running process is fun and just the right amount of danger to be exiting\nJust to document this (a friend asked me): If you ever wanted for example to globally change your $PATH, or add a global $LD_PRELOAD (for example to use insulterr ;) ), without restarting your session, gdb is your friend.\nYou can call arbitrary functions in the context of any process (that you are priviledged to attach a debugger, it has to run under your uid or you have to be root, see ptrace(2) for specifics), as long as they are linked. Almost everything is linked to libld, so with enough effort this actually means every function.\nFor example, suppose you are running i3wm and want to add /home/user/insulterr/insulterr.so to your $LD_PRELOAD in every process started by i3:\n$ gdb -p `pidof i3` `which i3` \u0026lt;lots of output of gdb\u0026gt; gdb $ call setenv(\u0026#34;LD_PRELOAD\u0026#34;, \u0026#34;/home/user/insulterr/insulterr.so\u0026#34;) gdb $ quit A debugging session is active. Inferior 1 [process 2] will be detached. Quit anyway? (y or n) y Detaching from program: /usr/bin/i3, process 2 This is of course a terrible hack, by high standards. Things to look out for are (off the top of my head):\n You call a function that manipulates errno or does some other non-reentrent things. If you are attaching the debugger right in the middle of a library call (or immediately after) this might make the program unhappy because it does not detect an error (or falsely thinks there is an error). You call a function that does not work in a multithreaded context and another thread modifies it at the same time. Bad. You interrupt a badly checked read(2)/write(2)/whatever(…) call and a badly written program doesn\u0026rsquo;t realize it got less data then expected (and/or crashes). Shouldn\u0026rsquo;t happen in practice, if it does, file a bug. You try to use symbols that are not available. This is actually not very bad and can be worked around (a friend of mine had the problem of needing false and just substituted 0). You use a daemon (like urxvtd(1)) for your terminals and the environment does not get passed correctly. This is also not very bad, just confusing. Attach your debugger to the daemon and change the environment there too. You attach the debugger to some process vital to the interaction with your debugger. Your window manager is a mild example. The terminal daemon is slightly worse (because, well, you can\u0026rsquo;t actually type in the terminal window that your debugger is running in, ergo you can\u0026rsquo;t stop it…), but you can change to a virtual terminal. Something like getty or init might be killing it.  Have fun!\n","permalink":"https://blog.merovius.de/posts/2013-10-11-inject-environment-variables-int/","summary":"Using gdb to manipulate a running process is fun and just the right amount of danger to be exiting","title":"Inject Environment variables into running processes"},{"content":"tl;dr: We had a codegolf challenge recently. My C-solution was 246 byte, the perl-winner was 191. I decided to give notes for C-golf beginners\nNote: Most of this blog post is incredibly boring. A better way than to read through it is to just skip through the git-repository and refer to the explanations here everytime you don\u0026rsquo;t know why a change works or what it does. To make this easier, I added ankers to every paragraph, named by the commit of the change it explains. So if you want to know, how a specific change works, you can add #commitid to the url of this post, with commitid being the full id of that change. If you want to read the more interesting stuff, from about here it starts to get non-obvious I think.\nAt the rgb2rv10 we again had a codegolf event. C is generally not a preferred language for golf, but I use it, because I know it best and my experiences with it are not that bad. My C solutions are most of the times longer then the shortest solutions in perl or similar languages, but they are competitive. That\u0026rsquo;s why I decided to make a blogpost explaining this years C solution as an example to show some basic C-golfing techniques.\nThis years challenge was to implement an interpreter for esocalc, a two-dimensional language for arithmetic expressions. Follow the link for a more detailed specification. This challenge is primed to be solved in C. This is also reflected by the length of the different solutions: The shortest C solution is 246 bytes, the shortest Python solution is 227 and the shortest Perl solution is 191 bytes. For a codegolf-challenge this is an impressively small gap between C and scripting languages.\nYou can follow this post by checking out the code on github. The oldest commit is the one we are starting with and we will refine it until we reach the 246 byte solution in master. You can test it by compiling it (gcc -o golf golf.c should suffice in most cases, the shortest needs a longer commandline, which is put in the Makefile, so you should make it). You can run it through the testsuite used in the contest by running GOLF_BIN=\u0026quot;./golf prove -l\u0026quot;.\n The first step is to implement an easily readable, working version. This is done in the first commit. Though you yourself might have come up with a different implementation, this is pretty straightforward I think. We just read the whole esocalc-sourcecode and walk through it, executing every instruction as we go. The stack is statically allocated and of a fixed size, but that\u0026rsquo;s no problem because we only have a limited testsuite anyway.\n The next step is obvious: We remove comments and move to one-letter variable names, thus reducing readability, but also size considerably. We will leave most of the whitespace for now, because else it is hard to follow the changes.\n An important lesson for C-golfers is the following: for is never longer then while and most of the times shorter. An endless loop with while takes one character more then a for-loop. We will later see more instances when for will be considerably shorter. Also, we see the if/else-constructs in the control-flow instructions. It is considerably shorter to use a ternary operator in most cases, because in C, most statements are also expressions, so we can write them as cases in ?: - or use the short-circuiting \u0026amp;\u0026amp; if there is no else-part. We will see more of that later. Lastly we collapse multiple variable declarations into one to save int-keywords. These three changes are what happend in the next version.\n We continue in our path and notice, that we every char-literal takes three bytes, while the number it represents often only takes two in decimal. Let\u0026rsquo;s fix that.\n We also have two temporary variables a and b, that we shouldn\u0026rsquo;t need. We can get rid of them, by thinking up a single statement for arithmetic operations.\n The next step uses a real detail of C: If you don\u0026rsquo;t give a type for a global variable, a parameter or the return type of a function, int is assumed. If a function is not defined, a prototype of int foo() is assumed, meaning we can pass arbitrary arguments and get an int. The libc is linked in by default. All these facts means, we can drop all includes and put our variables in the global scope to remove all int-keywords. This is a very basic, but very usefull technique. It has one important caveat, you should look out for: If you need the return value of a libc-function and it is not int, you should think about wether it can be safely converted. For example on amd64 an int has 32 bits, but a pointer has 64 bits, therefore pointers as return values get truncated (even if you assign them to a pointer).\n We can save more by using a parameter to main. This is also a very basic and often seen trick in C-golfing. You get up to 3 local variables for free this way. In our case there is an additional benefit: The first parameter to main is the number of arguments, which is 1 for a normal call (the first argument is the name with which the programm was called). This means, we get the initialization to 1 for free.\n A trivial optimization is using gets instead of read. gets always adds a terminating zero-byte, so we need to grow our buffer a little bit.\n If we now look at our code, all the case-keywords might annoy us. If we see a lot of repititions in our code, the obvious tool to use in C are defines. So lets define the structure of the cases and replace every case by a short 1-letter identifier.\n The same goes for the arithmetic operations: Four times the same long code cries for a define. A define is not always a good solution. You have to weigh the additional overhead of the keyword and the needed newline against the savings and number of repititions.\n Next we eliminate the variable i. Skilled C-coders use pointer-arithmetic quite often (no matter how bad the reputation is). In this case it would be a bad idea, if we were not explicitely allowed to assume that all programs are correct and stay in the bounds given (because bound-checks are a lot harder without indexing).\n Another example of savings by for-loops is the next change. Here we moved two statements into the for-loop, thus using the semicolons we need there anyway and saving two bytes.\n So the next big thing that catches our eyes are the switch, case and break-keywords. Everytime you see long identifiers or keywords you should think about wether a different program-structure or a different libc-builtin may help you save it. switch-construct can almost always be replaced by an if-else if construct (which is why we learned to use switch anyway). This is often shorter, but as we learned, the ternary operator is even shorter. So in the next step we use a giant ternary expression instead of a switch-structure. This brings one major problem: return is one of the few things that\u0026rsquo;s a statement, but not an expression. So we can\u0026rsquo;t use it in ?:-expressions (because the branches have to be expressions). We use exit() instead, which is an expression, but a void-expression, so again we run into problems using it in ?:. We work around that for now by using (exit(0),1) instead. If you connect expressions by , they are evaluated in succession (contrary to using boolean operators for example) and the value of the last one is becoming the value of the whole expression - so our exit-expression evaluates to 1 in this case.\n exit is still pretty long (especially with the added parens and comma-expression), so we want to avoid it too. Here comes a notable quote of the organisator of the competition into action: “The return value isn\u0026rsquo;t important, as long as the output is correct. So it doesn\u0026rsquo;t matter if you segfault or anything”. This is the key to the next change: Instead of exiting orderly we just create the conditions for a segfault by assigning zero to p, which is dereferenced shortly thereafter, thus creating a segfault when we want to exit. This is one of my favourite optimizations.\n There still is some repitition in our code. We still assign to d more often then not. But our big nested ternary operator doesn\u0026rsquo;t return anything yet. So our next step is to return the new value for d in all subexpressions (if need be by using a comma). This does not save a lot, but still a few bytes.\n Now the sources of bytes to save are getting scarcer. What still is a pain is the explicit stack of a fixed size. Here another deep mistery of C (or more specifically the way modern computers work) comes into play: The call stack. We can actually use this as our stack. The way this works is, that we use a pointer to an address in the memory area, the operating system reserved for our call stack and grow down (contrary to the illustration on wikipedia, the stack grows downwards. But this is a minor detail). By writing to this pointer and decrementing, we can push to the stack. By incrementing it and reading we can pop something from the top of the stack. To get a valid stack-address we could use the address of a local variable (for example s itself). Local variables are at the bottom of the stackframe, so we do not overwrite anything important if growing down. There is however a problem: We call gets and printf which push a few stackframes to the callstack. Our stack would get smashed by these calls. Therefore we just subtract a sufficiently high number from it to reserve space for the stackframes of the function calls. 760 is the minimum amount needed in my setup, everything up to 99999 should save at least one byte.\n This still is unsatisfactory, so we will hack a little more and use the fact, that the testsuite only uses quite small programms and a quite small stack is needed. So we just use s unitialized, which is absolutely crazy. I discovered (by accident), that you will always end up with a pointer to your program-array, using around 200 bytes of the end (most probably some earlier deeply nested call in the startup of the binary will write an appropriate address here by accident). This of course is borderline cheating, but it saves 6 bytes, so who cares. From now on it\u0026rsquo;s absolutely forbidden to compile with optimizations, because this will destroy this coincidence. Oh well.\n So, if we are already doing unreliable horrific voodoo which will curl up the fingernails of every honest C developer, we can also save two bytes by not setting p to zero, but instead just doubling it. You will then end up with some address, that is hard to predict, but in all cases I tried leads to crashing just as reliable. This means, we exit our program in just one byte. Neato!\n There is not a lot we can save left now. What might still annoy us and is a very good tip in general are all this numbers. Even if most characters have only 2 bytes as a decimal, they still only have one byte as a character (not a char-literal!). We can fix this by passing a verbatim character as the first argument to the c-makro. To interpret it as a char, we stringify it (with #a) and dereference it (with *#a), getting the first char. This opens a problem: A space is a significant character in the interpreted source code, so we need to use it as an argument. But a space is not significant at that point in the C source code, so we simply can not pass it to our makro. The solution to this is to move the whole ASCII-table. So instead of comparing *p we compare *p+n with n to be choosen. Thus we don\u0026rsquo;t need to pass a space, but some other char, that is n positions away and everyone is happy. Kind of. We also need to avoid single quotes, double quotes (though we can avoid this by using emtpy string (think about why this works), but too many bytes!!!), parenthesis and chars outside of ASCII (because this will break our C-file). These constrictions make n=3 pretty much the only choice. This means, we have to include a DEL-character in our source-code, but the compiler is quite happy about that (the wiki isn\u0026rsquo;t, github isn\u0026rsquo;t, the editor isn\u0026rsquo;t, but who cares). This is my second most favourite hack.\n Now there is not much left to do. We remove the last char-literal left and remove all non-essential whitespace.\n This leaves us with 253 bytes. To get below 250, we use buildflags instead of defines. Usually such flags are counted by the difference they add to a minimal compiler call needed. In this case, we have a 186 byte C-file (after removing the trailing newline added by vim) and 60 bytes of compiler-flags, totalling 246 bytes.\nI think there still is potential to remove some more characters. Other tools not used here include dispatch tables (which are kind of hard in C, because it lacks an eval, but some variations of the concept still apply) and magic formulas. If the testcases are very limited, some people resort to hardcoding the wanted results and just golf a minimal way to differentiate between what output is wanted. This might be surprising, but in many cases (this included) this will end up being shorter (though I consider it cheating and try to avoid it). We also didn\u0026rsquo;t do a lot of bit banging. For example using ^ instead of == reverses the check but saves a byte. But I think it is a usefull intro for people who are just learning C and want to dive deeper into the language by golfing.\n","permalink":"https://blog.merovius.de/posts/2013-10-11-how-to-cgolf/","summary":"We had a codegolf challenge recently. My C-solution was 246 byte, the perl-winner was 191. I decided to give notes for C-golf beginners.","title":"How to C-Golf"},{"content":"Because I recently applied for the position of a Debian maintainer, I finally had to upgrade my PGP Key to a more secure 4096 bit RSA. The new fingerprint is\nAF03 1CB8 DFFB 7DC5 E1EE EB04 A7C9 FF06 3F3D 2E03  I signed the new key with my old key and uploaded it to the keyservers. My old key will be valid for a little longer, so you can still send me encrypted Mails using my old key, but I would ask you to transition as fast as possible to the new one. If the signature with my old key is enough to earn your trust, you can delete the old key and set the new one to trusted (and maybe even sign it again and mail me the signed key or upload it), else you can check the fingerprint in person when we meet next time.\n","permalink":"https://blog.merovius.de/posts/2013-10-03-new-pgp-key/","summary":"Because I recently applied for the position of a Debian maintainer, I finally had to upgrade my PGP Key to a more secure 4096 bit RSA. The new fingerprint is\nAF03 1CB8 DFFB 7DC5 E1EE EB04 A7C9 FF06 3F3D 2E03  I signed the new key with my old key and uploaded it to the keyservers. My old key will be valid for a little longer, so you can still send me encrypted Mails using my old key, but I would ask you to transition as fast as possible to the new one.","title":"New PGP Key"},{"content":"tl;dr: I put up a small script to automate creating blog-posts in jekyll\nIf you think about setting up your own blog, jekyll seems to be an appropriate choice. This short guide should put you through the process of having an easy setup for writing and deploying your blog via your favourite editor (vim) and your favourite version control system (git) to a publicly available server via ssh.\nFirst thing you will need, is to have jekyll installed on both machines (the ones where you will write your posts and the one where you will deploy them to). Because the debian-version appears to be horribly outdated, I installed it via gem. As far as I understood, this has the advantage of making an installation without root-privileges possible. You should also have git available on both machines.\nInitializing your blog is pretty easy, jekyll new mynewblog (on your local machine) should suffice. You still want to do some configuration and customization, most of which should be straight-forward. Edit the index.html, the _config.yml and the _layouts/default.html. You might also want to have an Atom-template, so people can subscribe to your blog in their favourite RSS-reader. My good friend Stefan helped with that just put that file into the root of your blog-directory, edit your blogtitle and everything into it and add the line \u0026lt;link rel=\u0026#34;alternate\u0026#34; type=\u0026#34;application/atom+xml\u0026#34; href=\u0026#34;/atom.xml\u0026#34; title=\u0026#34;Atom feed\u0026#34;\u0026gt; in the \u0026lt;head\u0026gt; section of _layouts/default.html.\nNext thing is setting up deployment. Just git init a blog, git add every configuration file, page, template and whatnot and git commit it. ssh onto your deployment-machine and do a git init --bare blog.git. Save the following file to blog.git/hooks/post-update and change the path to point to a directory, that is served by your http-server: {% gist 6736709 post-update %} Everytime you push into blog.git you will then have jekyll automatically rebuild your blog. You now only have to do the following on your local machine to deploy your blog:\ngit remote add origin username@example.com:blog.git git push --set-upstream origin master Now to the really fancy stuff. Jekyll expects your blogposts to live under the _posts-directory under a special filename-format and to have a YAML-preamble, containing some configuration. It can be quite cumbersome to manage this yourself, so I wrote a shellscript to ease the process. Put it anywhere in your path (i chose the name newpost) and make it executable.\nWhen you run the script, it will look into the current directory for a jekyll-blog and create a draft from a small template given in the script. It will then optionally run a jekyll-development server, so that you can preview your blog-post in your browser (by saving the draft) and open the draft in your favourite editor. After you close your editor, the jekyll server will be stopped and the draft will be saved under _posts/YYYY-MM-DD-abbrev-title.fmt, where YYYY-MM-DD is the current date (date and time will also be automatically added to the YAML-preamble), fmt is a configurable format (markdown is default) and abbrev-title is a short string derived from the title you put in.\nThere will also (optionally) be a git-commit created with a default commit-message. You can edit the message in an editor and abort the commit, by deleting everything and saving an empty commit-message. If you really want (though I would not advise it) you can also automatically push it, after you\u0026rsquo;re done.\nAfter this setup, to create a new blogpost, you just have to cd to your blog-repository, run newpost, type your blogpost (and add a title), preview it in your browsers, exit your editor and you have everything ready to push. It can\u0026rsquo;t get much easier.\n","permalink":"https://blog.merovius.de/posts/2013-09-28-lazy-blogging-with-jekyll/","summary":"I put up a small script to automate creating blog-posts in jekyll","title":"Lazy blogging with jekyll"},{"content":"tl;dr: I gave a very introductory programming course and saw once again how the basic ideas underlying the modernization of teaching just work when implemented right.\nThis last week I organized a (very basic) introductory course on programming for our first-year students. I was set on C++ because it is the language used in the introductory lecture and we wanted to give people with absolutely no background in programming or proper use of a computer the necessary tools to start in this lecture on mostly equal grounds to people who already took a basic computer science course in school. We had five days, with 3 hours a day to try to reach that goal, which is a very limited amount of time for such a course and we had 50 participants.\nThe whole concept of the course was very modern (at least for our universities standards) - instead of just giving lectures, telling people about syntax and stuff we divided up the whole course into 19 lessons, each of which was worked at mostly independent. That had two big advantages (and was very positively perceived): First, the amount of time, we needed to spend lecturing and doing frontal presentations was minimized to about half an hour over all the course. The saved time could be invested in individual tutoring. This enabled us to react to every student needing help in a few seconds, using only about 3-4 senior students (with mostly pretty minimal background themselves actually) to teach.\nSecond the students where able to just work in their own speed without external pressure or a limit on the time spent on any lesson. Missing deadlines for lessons meant more experimentation, less competition amongst the students, less stress and less pressure to finish with all lessons in time. The course was not designed to be finished, so even though many students didn\u0026rsquo;t reach the last lesson, I think the additional experimentation (combined with a less content-driven curriculum) added much more value for the students.\nThe content also was rather different from what you usually read in tutorials or get in lectures at the university. Instead of systematically developing syntax and different language constructs, we used the language less then the object to learn, but the mean to learn basic skills needed, when tackling a programming lecture (basically: „How do I start“ and „what can I do, if it doesn\u0026rsquo;t work?“). We introduced every lesson with about a page of text, describing the key constructs underlying the object of that lesson, gave some basic code-examples and (without explaining the details of the syntax) then presented some basic exercises, which could be mastered without much understanding of what was happening, but which ensured the reproduction needed, to properly learn the syntactic device or the idea. We then added some playfull, very open exercises, where through experimentation and through their own mistakes the students where supposed to discover themselves the more intricate details of the subject matter. Thematically we restricted the syntax to the absolute minimum to get some basic, but fun and usefull programms to work (for example, we introduced only one kind of loop, and we introduced only the datatypes int, bool, std::string and double, as well as arrays thereof)\nThough this all might sound fairly „new-agey“, it worked remarkably well. We saw a fair amount of experimentation, we saw very creative solutions to seemingly easy and straightforward, we got very positive feedback and though we introduced many special subjects (for example debuggers, online references and detailed lectures and exercises on how to read manpages or error output of the compiler), I think it is fair to say, that we reached at least the level of proficiency and confidence as the more traditional courses we held the last years had.\nSo, the bottomline is: We took a very huge bite out of the ideas and thoughts underlying the ongoing effort in europe to modernize teaching at universities (The „Bologna Process“, as it\u0026rsquo;s known at least here in germany) and though I totally agree, that the implementation of these guidelines at the universities is currently pretty misguided and plain bad, I once again feel confirmed in my view, that if you put some effort into it and really use what the underlying ideas of bologna are (instead of just picking up, what you hear from the media about it), you can create a really kick-ass curriculum, that is both more fun and more informative at the same time.\nAll used content is on github, if you are interested in what exactly we used in the course.\n","permalink":"https://blog.merovius.de/posts/2013-09-28-firstyear-introductory-course-fo/","summary":"I gave a very introductory programming course and saw once again how the basic ideas underlying the modernization of teaching just work when implemented right.","title":"First-year introductory course for programming"},{"content":"Years ago I took down my blog because I was so unsatisfied with wordpress. In these years I started about 5 times to write my own git-based blog-engine and about 5 times I stopped after making considerable progress (mostly because it was too hard to integrate comments). This time the urge to restart my blog is finally overpowering my chronic nii-syndrome and I decided to use jekyll as a blog-engine and add some git-hook-magic myself.\nThe layout is still pretty shitty, it\u0026rsquo;s the default of jekyll, but I wanted to put it online before seing my efforts die again on such minor details.\nSo this is finally the relaunch of my blog. Yay for me.\n","permalink":"https://blog.merovius.de/posts/2013-09-28-relaunch/","summary":"The (n+1)th launch of my blog","title":"Relaunch"}]